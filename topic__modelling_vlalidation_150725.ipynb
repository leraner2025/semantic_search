{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ugZjCzA3Qvaa",
        "outputId": "8cc3f65f-d13b-4faf-df6f-ed792aaebdf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting scann\n",
            "  Downloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.9.post2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.43.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n",
            "Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m838.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scann, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bertopic\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bertopic-0.17.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 scann-1.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
            "Requirement already satisfied: scann in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.43.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.7.9)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scann 1.4.0 requires numpy~=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "d1b299cda7184186b0a7914aae23bf7f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install numpy sentence-transformers bertopic hdbscan nltk scann\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "!pip install sentence-transformers bertopic hdbscan umap-learn scann nltk datasets\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTS & SETUP ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "import re\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from sklearn.metrics import silhouette_score, precision_recall_fscore_support\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "nltk.download(\"punkt\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# === CLEANING & CONTEXT EXTRACTION ===\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):\n",
        "    entity_context_pairs = []\n",
        "    for idx, ents in enumerate(entities_per_chunk):\n",
        "        chunk = clean_text(chunks[idx])\n",
        "        sentences = sent_tokenize(chunk)\n",
        "        for ent in ents:\n",
        "            ent_lower = ent.lower()\n",
        "            matched = False\n",
        "            for i, sent in enumerate(sentences):\n",
        "                if ent_lower in sent:\n",
        "                    context = (\n",
        "                        \" \".join(sentences[max(0, i - 1): i + 2])\n",
        "                        if use_multi_sentence else sent.strip()\n",
        "                    )\n",
        "                    entity_context_pairs.append((ent_lower, context.strip()))\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                entity_context_pairs.append((ent_lower, chunk))\n",
        "    return entity_context_pairs\n",
        "\n",
        "# === TOPIC SEARCHER CLASS ===\n",
        "class AllergyTopicSearcher:\n",
        "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params,\n",
        "                 model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"):\n",
        "        self.chunks = chunks\n",
        "        self.entities_per_chunk = entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "        self.umap_params = umap_params\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        entity_context_pairs = extract_entity_contexts(\n",
        "            self.chunks,\n",
        "            self.entities_per_chunk,\n",
        "            use_multi_sentence=True\n",
        "        )\n",
        "\n",
        "        if not entity_context_pairs:\n",
        "            raise ValueError(\"No entity-context pairs extracted!\")\n",
        "\n",
        "        contextual_texts = [context for _, context in entity_context_pairs]\n",
        "        # UPDATED: use full 768-dim embeddings for search\n",
        "        contextual_embeddings = self.embedding_model.encode(\n",
        "            contextual_texts, normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "        umap_model = UMAP(**self.umap_params)\n",
        "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
        "\n",
        "        self.topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
        "\n",
        "        topic_to_contexts = defaultdict(list)\n",
        "        topic_to_entities = defaultdict(set)\n",
        "        topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "        for i, topic in enumerate(topics):\n",
        "            ent, context = entity_context_pairs[i]\n",
        "            topic_to_contexts[topic].append(context)\n",
        "            topic_to_entities[topic].add(ent)\n",
        "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id in topic_to_contexts:\n",
        "            embeddings = topic_to_embeddings[topic_id]\n",
        "            mean_emb = np.mean(embeddings, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": list(topic_to_entities[topic_id]),\n",
        "                \"sentences\": topic_to_contexts[topic_id],\n",
        "                \"sentence_embeddings\": np.array(embeddings)\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "        if len(self.topic_embeddings) < 1:\n",
        "            raise RuntimeError(\"No topic embeddings to index.\")\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 3)\n",
        "\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=1):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        results = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            seen = set()\n",
        "            unique_sentences = []\n",
        "            unique_embeddings = []\n",
        "\n",
        "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "                if sent not in seen:\n",
        "                    seen.add(sent)\n",
        "                    unique_sentences.append(sent)\n",
        "                    unique_embeddings.append(emb)\n",
        "\n",
        "            sent_embs = np.array(unique_embeddings)\n",
        "            if len(sent_embs) == 0:\n",
        "                continue\n",
        "\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            top_sents = [(unique_sentences[i], float(sims[i])) for i in top_indices]\n",
        "\n",
        "            results.append({\n",
        "                \"topic_id\": meta[\"topic_id\"],\n",
        "                \"entities\": list(meta[\"entities\"]),  # <- ensure it's a list in case it's a set\n",
        "                \"sentences\": top_sents,\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# === EVALUATION METRICS ===\n",
        "def compute_bertopic_coherence(topic_model, topic_metadata, topk=15):\n",
        "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
        "    topic_word_lists = [[word for word, _ in topic] for topic in topics]\n",
        "\n",
        "    texts = []\n",
        "    for meta in topic_metadata:\n",
        "        for sent in meta[\"sentences\"]:\n",
        "            tokens = clean_text(sent).split()\n",
        "            texts.append(tokens)\n",
        "\n",
        "    dictionary = Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    coherence_model = CoherenceModel(\n",
        "        topics=topic_word_lists,\n",
        "        texts=texts,\n",
        "        dictionary=dictionary,\n",
        "        coherence=\"c_v\"\n",
        "    )\n",
        "    return coherence_model.get_coherence()\n",
        "\n",
        "def compute_topic_diversity(topic_model, topic_metadata, topk=10):\n",
        "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
        "    all_words = [word for topic in topics for word, _ in topic]\n",
        "    return len(set(all_words)) / (len(topics) * topk)\n",
        "\n",
        "def compute_silhouette_score_custom(topic_metadata):\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    for meta in topic_metadata:\n",
        "        embeddings = meta[\"sentence_embeddings\"]\n",
        "        labels = [meta[\"topic_id\"]] * len(embeddings)\n",
        "        all_embeddings.append(embeddings)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    if len(all_embeddings) == 0:\n",
        "        return None\n",
        "\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "    n_samples = all_embeddings.shape[0]\n",
        "    n_labels = len(set(all_labels))\n",
        "\n",
        "    if n_labels < 2 or n_labels > n_samples - 1:\n",
        "        return None\n",
        "\n",
        "    return silhouette_score(all_embeddings, all_labels, metric=\"cosine\")\n",
        "\n",
        "# === DATASET & INITIALIZATION ===\n",
        "allergy_dataset = {\n",
        "   \"chunks\": [\n",
        "  \"A 75-year-old male with ischemic cardiomyopathy (LVEF 28%) and persistent atrial fibrillation on warfarin presented with worsening dyspnea and orthopnea over 2 weeks.\",\n",
        "  \"He has a history of COPD, stage 4 chronic kidney disease, type 2 diabetes mellitus (HbA1c 9.1%), and prior CABG in 2008.\",\n",
        "  \"On admission, his vitals were BP 155/95, HR 112 irregular, RR 28, and SpO₂ 86% on room air, improving with oxygen via nasal cannula.\",\n",
        "  \"Physical exam revealed elevated JVP, bibasilar crackles, 3+ pitting edema, and an S3 gallop.\",\n",
        "  \"Labs showed Cr 2.8, eGFR 25, BNP 2,400 pg/mL, and INR 2.3. ABG showed pH 7.36, PaCO₂ 50, PaO₂ 70. CXR indicated cardiomegaly and pleural effusions.\",\n",
        "  \"ECG confirmed atrial fibrillation with QTc 480 ms. Pacemaker function was intact.\",\n",
        "  \"He was treated with IV furosemide, oxygen therapy, and continued on carvedilol and lisinopril. Metformin was held due to renal impairment.\",\n",
        "  \"Diabetes was managed with basal-bolus insulin and sliding scale. A low-sodium renal diet was started.\",\n",
        "  \"Education included fluid restriction, daily weight tracking, and signs of heart failure. Warfarin was continued with INR monitoring.\",\n",
        "  \"He showed improvement with a 3 kg net negative fluid balance and SpO₂ rising to 94%.\",\n",
        "  \"An echocardiogram showed LVEF 28%, moderate mitral regurgitation, and left atrial enlargement.\",\n",
        "  \"Discharge medications included furosemide 80 mg, carvedilol 12.5 mg BID, lisinopril 5 mg, spironolactone 25 mg, basal insulin, and warfarin.\",\n",
        "  \"Follow-up appointments were scheduled with cardiology, nephrology, and the diabetes clinic. Anticoagulation clinic and home health visits arranged.\",\n",
        "  \"Patient is a widower living alone with limited mobility and uses a walker. Transportation issues were noted.\",\n",
        "  \"Social work coordinated support services including meal delivery, medication management, and transport assistance.\"\n",
        "]\n",
        ",\n",
        "  \"entities\": [\n",
        "  [\n",
        "    \"ischemic cardiomyopathy\",\n",
        "    \"LVEF 28%\",\n",
        "    \"atrial fibrillation\",\n",
        "    \"warfarin\",\n",
        "    \"dyspnea\",\n",
        "    \"orthopnea\",\n",
        "    \"2 weeks\"\n",
        "  ],\n",
        "  [\n",
        "    \"chronic obstructive pulmonary disease\",\n",
        "    \"stage 4 chronic kidney disease\",\n",
        "    \"type 2 diabetes mellitus\",\n",
        "    \"HbA1c 9.1%\",\n",
        "    \"CABG 2008\"\n",
        "  ],\n",
        "  [\n",
        "    \"BP 155/95\",\n",
        "    \"HR 112\",\n",
        "    \"RR 28\",\n",
        "    \"SpO₂ 86%\",\n",
        "    \"room air\",\n",
        "    \"oxygen therapy\",\n",
        "    \"nasal cannula\"\n",
        "  ],\n",
        "  [\n",
        "    \"jugular venous pressure\",\n",
        "    \"bibasilar crackles\",\n",
        "    \"3+ pitting edema\",\n",
        "    \"S3 gallop\"\n",
        "  ],\n",
        "  [\n",
        "    \"Cr 2.8\",\n",
        "    \"eGFR 25\",\n",
        "    \"BNP 2,400 pg/mL\",\n",
        "    \"INR 2.3\",\n",
        "    \"ABG pH 7.36\",\n",
        "    \"PaCO₂ 50\",\n",
        "    \"PaO₂ 70\",\n",
        "    \"CXR\",\n",
        "    \"cardiomegaly\",\n",
        "    \"pleural effusions\"\n",
        "  ],\n",
        "  [\n",
        "    \"ECG\",\n",
        "    \"atrial fibrillation\",\n",
        "    \"QTc 480 ms\",\n",
        "    \"pacemaker function\"\n",
        "  ],\n",
        "  [\n",
        "    \"IV furosemide\",\n",
        "    \"oxygen therapy\",\n",
        "    \"carvedilol\",\n",
        "    \"lisinopril\",\n",
        "    \"metformin\",\n",
        "    \"renal impairment\"\n",
        "  ],\n",
        "  [\n",
        "    \"basal-bolus insulin\",\n",
        "    \"sliding scale insulin\",\n",
        "    \"low-sodium renal diet\"\n",
        "  ],\n",
        "  [\n",
        "    \"fluid restriction\",\n",
        "    \"daily weight tracking\",\n",
        "    \"heart failure education\",\n",
        "    \"warfarin\",\n",
        "    \"INR monitoring\"\n",
        "  ],\n",
        "  [\n",
        "    \"3 kg fluid loss\",\n",
        "    \"SpO₂ 94%\"\n",
        "  ],\n",
        "  [\n",
        "    \"echocardiogram\",\n",
        "    \"LVEF 28%\",\n",
        "    \"moderate mitral regurgitation\",\n",
        "    \"left atrial enlargement\"\n",
        "  ],\n",
        "  [\n",
        "    \"furosemide 80 mg\",\n",
        "    \"carvedilol 12.5 mg BID\",\n",
        "    \"lisinopril 5 mg\",\n",
        "    \"spironolactone 25 mg\",\n",
        "    \"basal insulin\",\n",
        "    \"warfarin\"\n",
        "  ],\n",
        "  [\n",
        "    \"cardiology follow-up\",\n",
        "    \"nephrology follow-up\",\n",
        "    \"diabetes clinic follow-up\",\n",
        "    \"anticoagulation clinic\",\n",
        "    \"home health nursing\"\n",
        "  ],\n",
        "  [\n",
        "    \"widowed\",\n",
        "    \"lives alone\",\n",
        "    \"limited mobility\",\n",
        "    \"walker use\",\n",
        "    \"transportation issues\"\n",
        "  ],\n",
        "  [\n",
        "    \"social work\",\n",
        "    \"meal delivery\",\n",
        "    \"medication management\",\n",
        "    \"transport assistance\"\n",
        "  ]\n",
        "]\n",
        "\n",
        "}\n",
        "\n",
        "best_umap = {\"n_neighbors\": 5, \"n_components\": 5, \"min_dist\": 0.1, \"metric\": \"cosine\"}\n",
        "best_hdbscan = {\"min_cluster_size\": 2, \"min_samples\": 1, \"metric\": \"euclidean\"}\n",
        "\n",
        "print(\"Preparing Allergy Topic Searcher...\")\n",
        "searcher = AllergyTopicSearcher(\n",
        "    chunks=allergy_dataset[\"chunks\"],\n",
        "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
        "    umap_params=best_umap,\n",
        "    hdbscan_params=best_hdbscan,\n",
        "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
        ")\n",
        "print(\"✅ Model ready for querying.\")\n",
        "\n",
        "# === METRICS ===\n",
        "coherence = compute_bertopic_coherence(searcher.topic_model, searcher.topic_metadata, topk=15)\n",
        "diversity = compute_topic_diversity(searcher.topic_model, searcher.topic_metadata, topk=10)\n",
        "sil_score = compute_silhouette_score_custom(searcher.topic_metadata)\n",
        "\n",
        "print(\"\\n=== Topic Quality Metrics ===\")\n",
        "print(f\"🧪 Coherence Score (c_v): {coherence:.4f}\")\n",
        "print(f\"🌈 Topic Diversity: {diversity:.4f}\")\n",
        "if sil_score is not None:\n",
        "    print(f\"📐 Silhouette Score: {sil_score:.4f}\")\n",
        "else:\n",
        "    print(\"📐 Silhouette Score: Not applicable.\")\n",
        "\n",
        "# === GROUND TRUTH TOPICS ===\n",
        "ground_truth_topics = [\n",
        "  { \"topic_id\": \"T1\", \"entities\": [\"ischemic cardiomyopathy\", \"LVEF 28%\", \"moderate mitral regurgitation\", \"mild tricuspid regurgitation\", \"echocardiogram\"] },\n",
        "  { \"topic_id\": \"T2\", \"entities\": [\"atrial fibrillation\", \"warfarin\", \"INR 2.3\", \"pacemaker function\", \"ECG\", \"QTc 480 ms\"] },\n",
        "  { \"topic_id\": \"T3\", \"entities\": [\"chronic obstructive pulmonary disease\", \"dyspnea\", \"orthopnea\", \"two-pillow PND\", \"SpO₂ 86%\", \"nasal cannula\"] },\n",
        "  { \"topic_id\": \"T4\", \"entities\": [\"ischemic stroke 2012\", \"CABG 2008\"] },\n",
        "  { \"topic_id\": \"T5\", \"entities\": [\"type 2 diabetes mellitus\", \"HbA1c 9.1%\", \"metformin\", \"basal-bolus insulin\", \"sliding scale insulin\", \"diabetes clinic follow-up\"] },\n",
        "  { \"topic_id\": \"T6\", \"entities\": [\"stage 4 chronic kidney disease\", \"Cr 2.8\", \"eGFR 25\", \"nephrology follow-up\"] },\n",
        "  { \"topic_id\": \"T7\", \"entities\": [\"hypertension\", \"hyperlipidemia\", \"atorvastatin\", \"lisinopril\"] },\n",
        "  { \"topic_id\": \"T8\", \"entities\": [\"osteoarthritis\"] },\n",
        "  { \"topic_id\": \"T9\", \"entities\": [\"BP 155/95\", \"HR 112\", \"RR 28\", \"temperature 37.3 °C\", \"SpO₂ 86%\", \"SpO₂ 93%\"] },\n",
        "  { \"topic_id\": \"T10\", \"entities\": [\"jugular venous pressure\", \"bibasilar crackles\", \"scattered wheezes\", \"3+ pitting edema\", \"S3 gallop\", \"displaced PMI\", \"dullness to percussion\"] },\n",
        "  { \"topic_id\": \"T11\", \"entities\": [\"WBC 10.2\", \"Hb 12.0\", \"platelets 210\", \"Na 138\", \"K 4.5\", \"BUN 42\", \"BNP 2,400 pg/mL\", \"INR 2.3\", \"TSH normal\", \"troponin I negative\"] },\n",
        "  { \"topic_id\": \"T12\", \"entities\": [\"ABG pH 7.36\", \"PaCO₂ 50\", \"PaO₂ 70\"] },\n",
        "  { \"topic_id\": \"T13\", \"entities\": [\"CXR\", \"cardiomegaly\", \"interstitial edema\", \"small bilateral pleural effusions\"] },\n",
        "  { \"topic_id\": \"T14\", \"entities\": [\"IV furosemide\", \"oxygen via nasal cannula\"] },\n",
        "  { \"topic_id\": \"T15\", \"entities\": [\"carvedilol\", \"lisinopril\", \"spironolactone\", \"furosemide 80 mg\"] },\n",
        "  { \"topic_id\": \"T16\", \"entities\": [\"warfarin\", \"INR monitoring\", \"anticoagulation clinic\"] },\n",
        "  { \"topic_id\": \"T17\", \"entities\": [\"oxygen therapy\", \"nasal cannula\", \"SpO₂ 94%\"] },\n",
        "  { \"topic_id\": \"T18\", \"entities\": [\"3 kg net negative fluid balance\", \"daily weight tracking\", \"fluid restriction\"] },\n",
        "  { \"topic_id\": \"T19\", \"entities\": [\"pulmonary rehab evaluation\", \"physical therapy evaluation\", \"6-minute walk test\"] },\n",
        "  { \"topic_id\": \"T20\", \"entities\": [\"heart failure red flags\", \"dietician visit\", \"home health nursing\"] },\n",
        "  { \"topic_id\": \"T21\", \"entities\": [\"influenza vaccination\", \"pneumococcal vaccination\", \"herpes zoster vaccination\"] },\n",
        "  { \"topic_id\": \"T22\", \"entities\": [\"cardiology follow-up\", \"nephrology follow-up\", \"diabetes clinic follow-up\"] },\n",
        "  { \"topic_id\": \"T23\", \"entities\": [\"widowed\", \"lives alone\", \"former smoker\", \"limited mobility\", \"walker use\", \"transportation issues\"] },\n",
        "  { \"topic_id\": \"T24\", \"entities\": [\"social work\", \"meal delivery\", \"medication management\", \"appointment coordination\"] }\n",
        "]\n",
        "\n",
        "\n",
        "# === EVALUATION CODE ===\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def normalize(entities):\n",
        "    return [e.lower().strip() for e in entities]\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    set1, set2 = set(set1), set(set2)\n",
        "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0.0\n",
        "\n",
        "# Prepare model topics\n",
        "model_topics = [\n",
        "    {\"topic_id\": meta[\"topic_id\"], \"entities\": normalize(meta[\"entities\"])}\n",
        "    for meta in searcher.topic_metadata\n",
        "]\n",
        "\n",
        "# Matching model topics to ground truth\n",
        "matched_gt_ids = set()\n",
        "matches = []\n",
        "all_model_entities = []\n",
        "all_gt_entities = []\n",
        "\n",
        "for mt in model_topics:\n",
        "    best_score = 0\n",
        "    best_gt = None\n",
        "    for gt in ground_truth_topics:\n",
        "        score = jaccard_similarity(mt[\"entities\"], normalize(gt[\"entities\"]))\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_gt = gt\n",
        "    if best_gt:\n",
        "        matches.append((mt[\"topic_id\"], best_gt[\"topic_id\"], best_score))\n",
        "        matched_gt_ids.add(best_gt[\"topic_id\"])\n",
        "\n",
        "        # Collect entities for entity-level precision/recall\n",
        "        all_model_entities.extend(mt[\"entities\"])\n",
        "        all_gt_entities.extend(normalize(best_gt[\"entities\"]))\n",
        "\n",
        "# Entity-level metrics\n",
        "model_entity_counter = Counter(all_model_entities)\n",
        "gt_entity_counter = Counter(all_gt_entities)\n",
        "\n",
        "unique_entities = list(set(list(model_entity_counter.keys()) + list(gt_entity_counter.keys())))\n",
        "y_true = [gt_entity_counter[e] > 0 for e in unique_entities]\n",
        "y_pred = [model_entity_counter[e] > 0 for e in unique_entities]\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "\n",
        "# Print evaluation\n",
        "print(\"\\n=== 📊 Topic Matching Summary ===\")\n",
        "for model_id, gt_id, score in matches:\n",
        "    print(f\"🔗 Model Topic {model_id} ↔ Ground Truth {gt_id} — Jaccard: {score:.2f}\")\n",
        "\n",
        "print(f\"\\n🧮 Average Jaccard Similarity: {sum(score for _, _, score in matches) / len(matches):.4f}\")\n",
        "print(f\"📈 Ground Truth Coverage: {len(matched_gt_ids)}/{len(ground_truth_topics)} \"\n",
        "      f\"({(len(matched_gt_ids)/len(ground_truth_topics))*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n=== 🧠 Entity-Level Evaluation ===\")\n",
        "print(f\"🎯 Precision: {precision:.4f}\")\n",
        "print(f\"🧲 Recall:    {recall:.4f}\")\n",
        "print(f\"🏅 F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# === QUERY LOOP ===\n",
        "print(\"\\n=== Allergy Topic Search ===\")\n",
        "while True:\n",
        "    query = input(\"\\nEnter a query (or type 'exit' to quit): \").strip()\n",
        "    if query.lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    results = searcher.search(query, top_k_topics=1, top_k_sents=3)\n",
        "    print(f\"\\n🔎 Top results for: '{query}'\")\n",
        "    for res in results:\n",
        "        print(f\"🧠 Topic ID: {res['topic_id']}\")\n",
        "        print(f\"🔗 Related Entities: {', '.join(res['entities'])}\")\n",
        "        for sent, _ in res[\"sentences\"]:\n",
        "            print(f\"✓ {sent}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j41HbDOKVPz9",
        "outputId": "ed8c8e2b-47b2-44db-8fea-b211999d255b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing Allergy Topic Searcher...\n",
            "✅ Model ready for querying.\n",
            "\n",
            "=== Topic Quality Metrics ===\n",
            "🧪 Coherence Score (c_v): 0.7842\n",
            "🌈 Topic Diversity: 0.5619\n",
            "📐 Silhouette Score: 0.3062\n",
            "\n",
            "=== 📊 Topic Matching Summary ===\n",
            "🔗 Model Topic -1 ↔ Ground Truth T14 — Jaccard: 0.20\n",
            "🔗 Model Topic 12 ↔ Ground Truth T1 — Jaccard: 0.17\n",
            "🔗 Model Topic 17 ↔ Ground Truth T16 — Jaccard: 0.25\n",
            "🔗 Model Topic 5 ↔ Ground Truth T1 — Jaccard: 0.29\n",
            "🔗 Model Topic 11 ↔ Ground Truth T4 — Jaccard: 0.25\n",
            "🔗 Model Topic 15 ↔ Ground Truth T6 — Jaccard: 0.20\n",
            "🔗 Model Topic 1 ↔ Ground Truth T9 — Jaccard: 0.36\n",
            "🔗 Model Topic 3 ↔ Ground Truth T10 — Jaccard: 0.50\n",
            "🔗 Model Topic 0 ↔ Ground Truth T12 — Jaccard: 0.30\n",
            "🔗 Model Topic 13 ↔ Ground Truth T2 — Jaccard: 0.33\n",
            "🔗 Model Topic 19 ↔ Ground Truth T2 — Jaccard: 0.33\n",
            "🔗 Model Topic 7 ↔ Ground Truth T17 — Jaccard: 0.20\n",
            "🔗 Model Topic 4 ↔ Ground Truth T15 — Jaccard: 0.29\n",
            "🔗 Model Topic 9 ↔ Ground Truth T16 — Jaccard: 0.20\n",
            "🔗 Model Topic 14 ↔ Ground Truth T16 — Jaccard: 0.25\n",
            "🔗 Model Topic 6 ↔ Ground Truth T22 — Jaccard: 0.75\n",
            "🔗 Model Topic 2 ↔ Ground Truth T24 — Jaccard: 0.50\n",
            "🔗 Model Topic 18 ↔ Ground Truth T23 — Jaccard: 0.33\n",
            "🔗 Model Topic 10 ↔ Ground Truth T23 — Jaccard: 0.50\n",
            "\n",
            "🧮 Average Jaccard Similarity: 0.3264\n",
            "📈 Ground Truth Coverage: 14/24 (58.3%)\n",
            "\n",
            "=== 🧠 Entity-Level Evaluation ===\n",
            "🎯 Precision: 0.6567\n",
            "🧲 Recall:    0.7857\n",
            "🏅 F1 Score:  0.7154\n",
            "\n",
            "=== Allergy Topic Search ===\n",
            "\n",
            "Enter a query (or type 'exit' to quit): patient history\n",
            "\n",
            "🔎 Top results for: 'patient history'\n",
            "🧠 Topic ID: 6\n",
            "🔗 Related Entities: cardiology follow-up, nephrology follow-up, diabetes clinic follow-up, anticoagulation clinic\n",
            "✓ followup appointments were scheduled with cardiology nephrology and the diabetes clinic anticoagulation clinic and home health visits arranged\n",
            "\n",
            "Enter a query (or type 'exit' to quit): treatment\n",
            "\n",
            "🔎 Top results for: 'treatment'\n",
            "🧠 Topic ID: 2\n",
            "🔗 Related Entities: home health nursing, meal delivery, social work, medication management, transport assistance\n",
            "✓ social work coordinated support services including meal delivery medication management and transport assistance\n",
            "✓ followup appointments were scheduled with cardiology nephrology and the diabetes clinic anticoagulation clinic and home health visits arranged\n",
            "\n",
            "Enter a query (or type 'exit' to quit): medication\n",
            "\n",
            "🔎 Top results for: 'medication'\n",
            "🧠 Topic ID: 8\n",
            "🔗 Related Entities: basal insulin, spironolactone 25 mg, furosemide 80 mg\n",
            "✓ discharge medications included furosemide 80 mg carvedilol 125 mg bid lisinopril 5 mg spironolactone 25 mg basal insulin and warfarin\n",
            "\n",
            "Enter a query (or type 'exit' to quit): exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTS & SETUP ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "import re\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from sklearn.metrics import silhouette_score, precision_recall_fscore_support\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "nltk.download(\"punkt\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# === CLEANING & CONTEXT EXTRACTION ===\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):\n",
        "    entity_context_pairs = []\n",
        "    for idx, ents in enumerate(entities_per_chunk):\n",
        "        chunk = clean_text(chunks[idx])\n",
        "        sentences = sent_tokenize(chunk)\n",
        "        for ent in ents:\n",
        "            ent_lower = ent.lower()\n",
        "            matched = False\n",
        "            for i, sent in enumerate(sentences):\n",
        "                if ent_lower in sent:\n",
        "                    context = (\n",
        "                        \" \".join(sentences[max(0, i - 1): i + 2])\n",
        "                        if use_multi_sentence else sent.strip()\n",
        "                    )\n",
        "                    entity_context_pairs.append((ent_lower, context.strip()))\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                entity_context_pairs.append((ent_lower, chunk))\n",
        "    return entity_context_pairs\n",
        "\n",
        "# === TOPIC SEARCHER CLASS ===\n",
        "class AllergyTopicSearcher:\n",
        "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params,\n",
        "                 model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"):\n",
        "        self.chunks = chunks\n",
        "        self.entities_per_chunk = entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "        self.umap_params = umap_params\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        entity_context_pairs = extract_entity_contexts(\n",
        "            self.chunks,\n",
        "            self.entities_per_chunk,\n",
        "            use_multi_sentence=True\n",
        "        )\n",
        "\n",
        "        if not entity_context_pairs:\n",
        "            raise ValueError(\"No entity-context pairs extracted!\")\n",
        "\n",
        "        contextual_texts = [context for _, context in entity_context_pairs]\n",
        "        # UPDATED: use full 768-dim embeddings for search\n",
        "        contextual_embeddings = self.embedding_model.encode(\n",
        "            contextual_texts, normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "        umap_model = UMAP(**self.umap_params)\n",
        "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
        "\n",
        "        self.topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
        "\n",
        "        topic_to_contexts = defaultdict(list)\n",
        "        topic_to_entities = defaultdict(set)\n",
        "        topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "        for i, topic in enumerate(topics):\n",
        "            ent, context = entity_context_pairs[i]\n",
        "            topic_to_contexts[topic].append(context)\n",
        "            topic_to_entities[topic].add(ent)\n",
        "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id in topic_to_contexts:\n",
        "            embeddings = topic_to_embeddings[topic_id]\n",
        "            mean_emb = np.mean(embeddings, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": list(topic_to_entities[topic_id]),\n",
        "                \"sentences\": topic_to_contexts[topic_id],\n",
        "                \"sentence_embeddings\": np.array(embeddings)\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "        if len(self.topic_embeddings) < 1:\n",
        "            raise RuntimeError(\"No topic embeddings to index.\")\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 3)\n",
        "\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=1):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        results = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            seen = set()\n",
        "            unique_sentences = []\n",
        "            unique_embeddings = []\n",
        "\n",
        "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "                if sent not in seen:\n",
        "                    seen.add(sent)\n",
        "                    unique_sentences.append(sent)\n",
        "                    unique_embeddings.append(emb)\n",
        "\n",
        "            sent_embs = np.array(unique_embeddings)\n",
        "            if len(sent_embs) == 0:\n",
        "                continue\n",
        "\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            top_sents = [(unique_sentences[i], float(sims[i])) for i in top_indices]\n",
        "\n",
        "            results.append({\n",
        "                \"topic_id\": meta[\"topic_id\"],\n",
        "                \"entities\": list(meta[\"entities\"]),  # <- ensure it's a list in case it's a set\n",
        "                \"sentences\": top_sents,\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# === EVALUATION METRICS ===\n",
        "def compute_bertopic_coherence(topic_model, topic_metadata, topk=15):\n",
        "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
        "    topic_word_lists = [[word for word, _ in topic] for topic in topics]\n",
        "\n",
        "    texts = []\n",
        "    for meta in topic_metadata:\n",
        "        for sent in meta[\"sentences\"]:\n",
        "            tokens = clean_text(sent).split()\n",
        "            texts.append(tokens)\n",
        "\n",
        "    dictionary = Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    coherence_model = CoherenceModel(\n",
        "        topics=topic_word_lists,\n",
        "        texts=texts,\n",
        "        dictionary=dictionary,\n",
        "        coherence=\"c_v\"\n",
        "    )\n",
        "    return coherence_model.get_coherence()\n",
        "\n",
        "def compute_topic_diversity(topic_model, topic_metadata, topk=10):\n",
        "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
        "    all_words = [word for topic in topics for word, _ in topic]\n",
        "    return len(set(all_words)) / (len(topics) * topk)\n",
        "\n",
        "def compute_silhouette_score_custom(topic_metadata):\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    for meta in topic_metadata:\n",
        "        embeddings = meta[\"sentence_embeddings\"]\n",
        "        labels = [meta[\"topic_id\"]] * len(embeddings)\n",
        "        all_embeddings.append(embeddings)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    if len(all_embeddings) == 0:\n",
        "        return None\n",
        "\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "    n_samples = all_embeddings.shape[0]\n",
        "    n_labels = len(set(all_labels))\n",
        "\n",
        "    if n_labels < 2 or n_labels > n_samples - 1:\n",
        "        return None\n",
        "\n",
        "    return silhouette_score(all_embeddings, all_labels, metric=\"cosine\")\n",
        "\n",
        "# === DATASET & INITIALIZATION ===\n",
        "allergy_dataset = {\n",
        "   \"chunks\": [\n",
        "  \"A 68-year-old female with a history of chronic obstructive pulmonary disease (COPD) presented with worsening shortness of breath, increased sputum production, and wheezing over the past week.\",\n",
        "  \"She reports using her rescue inhaler more frequently and having difficulty climbing stairs due to breathlessness.\",\n",
        "  \"Home medications include tiotropium inhaler, albuterol as needed, and a prednisone taper started 3 days ago by her primary care provider.\",\n",
        "  \"She denies chest pain or fever but has noticed fatigue and reduced exercise tolerance.\",\n",
        "  \"She was started on nebulized bronchodilators and continued her inhaled therapies during the visit.\",\n",
        "  \"Pulmonary rehabilitation referral was placed, and smoking cessation counseling was reinforced.\",\n",
        "  \"Discharge plan included follow-up with her pulmonologist in one week and continuation of home medications.\",\n",
        "  \"She lives with her daughter, is a former smoker, and uses oxygen at home as needed.\"\n",
        "]\n",
        "\n",
        ",\n",
        "  \"entities\": [\n",
        "  [\"chronic obstructive pulmonary disease\", \"shortness of breath\", \"sputum production\", \"wheezing\", \"past week\"],\n",
        "  [\"rescue inhaler\", \"breathlessness\", \"difficulty climbing stairs\"],\n",
        "  [\"tiotropium inhaler\", \"albuterol\", \"prednisone taper\", \"primary care provider\"],\n",
        "  [\"chest pain (denied)\", \"fever (denied)\", \"fatigue\", \"reduced exercise tolerance\"],\n",
        "  [\"nebulized bronchodilators\", \"inhaled therapies\"],\n",
        "  [\"pulmonary rehabilitation\", \"smoking cessation counseling\"],\n",
        "  [\"pulmonologist follow-up\", \"home medications\"],\n",
        "  [\"lives with daughter\", \"former smoker\", \"home oxygen\"]\n",
        "]\n",
        "\n",
        "}\n",
        "\n",
        "best_umap = {\"n_neighbors\": 5, \"n_components\": 5, \"min_dist\": 0.1, \"metric\": \"cosine\"}\n",
        "best_hdbscan = {\"min_cluster_size\": 2, \"min_samples\": 1, \"metric\": \"euclidean\"}\n",
        "\n",
        "print(\"Preparing Allergy Topic Searcher...\")\n",
        "searcher = AllergyTopicSearcher(\n",
        "    chunks=allergy_dataset[\"chunks\"],\n",
        "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
        "    umap_params=best_umap,\n",
        "    hdbscan_params=best_hdbscan,\n",
        "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
        ")\n",
        "print(\"✅ Model ready for querying.\")\n",
        "\n",
        "# === METRICS ===\n",
        "coherence = compute_bertopic_coherence(searcher.topic_model, searcher.topic_metadata, topk=15)\n",
        "diversity = compute_topic_diversity(searcher.topic_model, searcher.topic_metadata, topk=10)\n",
        "sil_score = compute_silhouette_score_custom(searcher.topic_metadata)\n",
        "\n",
        "print(\"\\n=== Topic Quality Metrics ===\")\n",
        "print(f\"🧪 Coherence Score (c_v): {coherence:.4f}\")\n",
        "print(f\"🌈 Topic Diversity: {diversity:.4f}\")\n",
        "if sil_score is not None:\n",
        "    print(f\"📐 Silhouette Score: {sil_score:.4f}\")\n",
        "else:\n",
        "    print(\"📐 Silhouette Score: Not applicable.\")\n",
        "\n",
        "# === GROUND TRUTH TOPICS ===\n",
        "ground_truth_topics=[\n",
        "  { \"topic_id\": \"T1\", \"entities\": [\"chronic obstructive pulmonary disease\", \"shortness of breath\", \"wheezing\", \"sputum production\", \"breathlessness\"] },\n",
        "  { \"topic_id\": \"T2\", \"entities\": [\"tiotropium inhaler\", \"albuterol\", \"prednisone taper\", \"nebulized bronchodilators\", \"inhaled therapies\", \"rescue inhaler\"] },\n",
        "  { \"topic_id\": \"T3\", \"entities\": [\"fatigue\", \"reduced exercise tolerance\", \"difficulty climbing stairs\"] },\n",
        "  { \"topic_id\": \"T4\", \"entities\": [\"pulmonary rehabilitation\", \"smoking cessation counseling\"] },\n",
        "  { \"topic_id\": \"T5\", \"entities\": [\"pulmonologist follow-up\", \"home medications\", \"primary care provider\"] },\n",
        "  { \"topic_id\": \"T6\", \"entities\": [\"former smoker\", \"home oxygen\", \"lives with daughter\"] }\n",
        "]\n",
        "\n",
        "# === EVALUATION CODE ===\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def normalize(entities):\n",
        "    return [e.lower().strip() for e in entities]\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    set1, set2 = set(set1), set(set2)\n",
        "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0.0\n",
        "\n",
        "# Prepare model topics\n",
        "model_topics = [\n",
        "    {\"topic_id\": meta[\"topic_id\"], \"entities\": normalize(meta[\"entities\"])}\n",
        "    for meta in searcher.topic_metadata\n",
        "]\n",
        "\n",
        "# Matching model topics to ground truth\n",
        "matched_gt_ids = set()\n",
        "matches = []\n",
        "all_model_entities = []\n",
        "all_gt_entities = []\n",
        "\n",
        "for mt in model_topics:\n",
        "    best_score = 0\n",
        "    best_gt = None\n",
        "    for gt in ground_truth_topics:\n",
        "        score = jaccard_similarity(mt[\"entities\"], normalize(gt[\"entities\"]))\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_gt = gt\n",
        "    if best_gt:\n",
        "        matches.append((mt[\"topic_id\"], best_gt[\"topic_id\"], best_score))\n",
        "        matched_gt_ids.add(best_gt[\"topic_id\"])\n",
        "\n",
        "        # Collect entities for entity-level precision/recall\n",
        "        all_model_entities.extend(mt[\"entities\"])\n",
        "        all_gt_entities.extend(normalize(best_gt[\"entities\"]))\n",
        "\n",
        "# Entity-level metrics\n",
        "model_entity_counter = Counter(all_model_entities)\n",
        "gt_entity_counter = Counter(all_gt_entities)\n",
        "\n",
        "unique_entities = list(set(list(model_entity_counter.keys()) + list(gt_entity_counter.keys())))\n",
        "y_true = [gt_entity_counter[e] > 0 for e in unique_entities]\n",
        "y_pred = [model_entity_counter[e] > 0 for e in unique_entities]\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "\n",
        "# Print evaluation\n",
        "print(\"\\n=== 📊 Topic Matching Summary ===\")\n",
        "for model_id, gt_id, score in matches:\n",
        "    print(f\"🔗 Model Topic {model_id} ↔ Ground Truth {gt_id} — Jaccard: {score:.2f}\")\n",
        "\n",
        "print(f\"\\n🧮 Average Jaccard Similarity: {sum(score for _, _, score in matches) / len(matches):.4f}\")\n",
        "print(f\"📈 Ground Truth Coverage: {len(matched_gt_ids)}/{len(ground_truth_topics)} \"\n",
        "      f\"({(len(matched_gt_ids)/len(ground_truth_topics))*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n=== 🧠 Entity-Level Evaluation ===\")\n",
        "print(f\"🎯 Precision: {precision:.4f}\")\n",
        "print(f\"🧲 Recall:    {recall:.4f}\")\n",
        "print(f\"🏅 F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# === QUERY LOOP ===\n",
        "print(\"\\n=== Allergy Topic Search ===\")\n",
        "while True:\n",
        "    query = input(\"\\nEnter a query (or type 'exit' to quit): \").strip()\n",
        "    if query.lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    results = searcher.search(query, top_k_topics=1, top_k_sents=3)\n",
        "    print(f\"\\n🔎 Top results for: '{query}'\")\n",
        "    for res in results:\n",
        "        print(f\"🧠 Topic ID: {res['topic_id']}\")\n",
        "        print(f\"🔗 Related Entities: {', '.join(res['entities'])}\")\n",
        "        for sent, _ in res[\"sentences\"]:\n",
        "            print(f\"✓ {sent}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bVFUZBhXY2s",
        "outputId": "bf3bfe37-b2f7-4e10-9ef8-595ecec24100"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing Allergy Topic Searcher...\n",
            "✅ Model ready for querying.\n",
            "\n",
            "=== Topic Quality Metrics ===\n",
            "🧪 Coherence Score (c_v): 0.5139\n",
            "🌈 Topic Diversity: 0.8500\n",
            "📐 Silhouette Score: 0.4215\n",
            "\n",
            "=== 📊 Topic Matching Summary ===\n",
            "🔗 Model Topic 1 ↔ Ground Truth T1 — Jaccard: 0.42\n",
            "🔗 Model Topic 0 ↔ Ground Truth T2 — Jaccard: 0.36\n",
            "\n",
            "🧮 Average Jaccard Similarity: 0.3869\n",
            "📈 Ground Truth Coverage: 2/6 (33.3%)\n",
            "\n",
            "=== 🧠 Entity-Level Evaluation ===\n",
            "🎯 Precision: 0.4400\n",
            "🧲 Recall:    1.0000\n",
            "🏅 F1 Score:  0.6111\n",
            "\n",
            "=== Allergy Topic Search ===\n",
            "\n",
            "🔎 Top results for: 'What symptoms did the patient present with?'\n",
            "🧠 Topic ID: 1\n",
            "🔗 Related Entities: past week, shortness of breath, difficulty climbing stairs, rescue inhaler, breathlessness, wheezing, sputum production, chest pain (denied), chronic obstructive pulmonary disease, reduced exercise tolerance, fatigue, fever (denied)\n",
            "✓ she denies chest pain or fever but has noticed fatigue and reduced exercise tolerance\n",
            "✓ a 68yearold female with a history of chronic obstructive pulmonary disease copd presented with worsening shortness of breath increased sputum production and wheezing over the past week\n",
            "✓ she reports using her rescue inhaler more frequently and having difficulty climbing stairs due to breathlessness\n",
            "\n",
            "🔎 Top results for: '\"How long has the patient been symptomatic?'\n",
            "🧠 Topic ID: 1\n",
            "🔗 Related Entities: past week, shortness of breath, difficulty climbing stairs, rescue inhaler, breathlessness, wheezing, sputum production, chest pain (denied), chronic obstructive pulmonary disease, reduced exercise tolerance, fatigue, fever (denied)\n",
            "✓ a 68yearold female with a history of chronic obstructive pulmonary disease copd presented with worsening shortness of breath increased sputum production and wheezing over the past week\n",
            "✓ she denies chest pain or fever but has noticed fatigue and reduced exercise tolerance\n",
            "✓ she reports using her rescue inhaler more frequently and having difficulty climbing stairs due to breathlessness\n",
            "\n",
            "🔎 Top results for: 'What medications is the patient currently taking for COPD?'\n",
            "🧠 Topic ID: 0\n",
            "🔗 Related Entities: albuterol, tiotropium inhaler, primary care provider, home oxygen, smoking cessation counseling, inhaled therapies, former smoker, lives with daughter, prednisone taper, home medications, pulmonologist follow-up, nebulized bronchodilators, pulmonary rehabilitation\n",
            "✓ home medications include tiotropium inhaler albuterol as needed and a prednisone taper started 3 days ago by her primary care provider\n",
            "✓ discharge plan included followup with her pulmonologist in one week and continuation of home medications\n",
            "✓ she was started on nebulized bronchodilators and continued her inhaled therapies during the visit\n",
            "\n",
            "🔎 Top results for: '\"What counseling was provided?\"'\n",
            "🧠 Topic ID: 0\n",
            "🔗 Related Entities: albuterol, tiotropium inhaler, primary care provider, home oxygen, smoking cessation counseling, inhaled therapies, former smoker, lives with daughter, prednisone taper, home medications, pulmonologist follow-up, nebulized bronchodilators, pulmonary rehabilitation\n",
            "✓ pulmonary rehabilitation referral was placed and smoking cessation counseling was reinforced\n",
            "✓ discharge plan included followup with her pulmonologist in one week and continuation of home medications\n",
            "✓ home medications include tiotropium inhaler albuterol as needed and a prednisone taper started 3 days ago by her primary care provider\n",
            "\n",
            "🔎 Top results for: '\"What counseling was provided?\"'\n",
            "🧠 Topic ID: 0\n",
            "🔗 Related Entities: albuterol, tiotropium inhaler, primary care provider, home oxygen, smoking cessation counseling, inhaled therapies, former smoker, lives with daughter, prednisone taper, home medications, pulmonologist follow-up, nebulized bronchodilators, pulmonary rehabilitation\n",
            "✓ pulmonary rehabilitation referral was placed and smoking cessation counseling was reinforced\n",
            "✓ discharge plan included followup with her pulmonologist in one week and continuation of home medications\n",
            "✓ home medications include tiotropium inhaler albuterol as needed and a prednisone taper started 3 days ago by her primary care provider\n",
            "\n",
            "🔎 Top results for: '\"What was done during the visit?\"'\n",
            "🧠 Topic ID: 0\n",
            "🔗 Related Entities: albuterol, tiotropium inhaler, primary care provider, home oxygen, smoking cessation counseling, inhaled therapies, former smoker, lives with daughter, prednisone taper, home medications, pulmonologist follow-up, nebulized bronchodilators, pulmonary rehabilitation\n",
            "✓ discharge plan included followup with her pulmonologist in one week and continuation of home medications\n",
            "✓ home medications include tiotropium inhaler albuterol as needed and a prednisone taper started 3 days ago by her primary care provider\n",
            "✓ pulmonary rehabilitation referral was placed and smoking cessation counseling was reinforced\n",
            "\n",
            "Enter a query (or type 'exit' to quit): exit\n",
            "Goodbye!\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === IMPORTS & SETUP ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "import re\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from sklearn.metrics import silhouette_score, precision_recall_fscore_support\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "nltk.download(\"punkt\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# === CLEANING & CONTEXT EXTRACTION ===\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):\n",
        "    entity_context_pairs = []\n",
        "    for idx, ents in enumerate(entities_per_chunk):\n",
        "        chunk = clean_text(chunks[idx])\n",
        "        sentences = sent_tokenize(chunk)\n",
        "        for ent in ents:\n",
        "            ent_lower = ent.lower()\n",
        "            matched = False\n",
        "            for i, sent in enumerate(sentences):\n",
        "                if ent_lower in sent:\n",
        "                    context = (\n",
        "                        \" \".join(sentences[max(0, i - 1): i + 2])\n",
        "                        if use_multi_sentence else sent.strip()\n",
        "                    )\n",
        "                    entity_context_pairs.append((ent_lower, context.strip()))\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                entity_context_pairs.append((ent_lower, chunk))\n",
        "    return entity_context_pairs\n",
        "\n",
        "# === TOPIC SEARCHER CLASS ===\n",
        "class AllergyTopicSearcher:\n",
        "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params,\n",
        "                 model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"):\n",
        "        self.chunks = chunks\n",
        "        self.entities_per_chunk = entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "        self.umap_params = umap_params\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        entity_context_pairs = extract_entity_contexts(\n",
        "            self.chunks,\n",
        "            self.entities_per_chunk,\n",
        "            use_multi_sentence=True\n",
        "        )\n",
        "\n",
        "        if not entity_context_pairs:\n",
        "            raise ValueError(\"No entity-context pairs extracted!\")\n",
        "\n",
        "        contextual_texts = [context for _, context in entity_context_pairs]\n",
        "        # UPDATED: use full 768-dim embeddings for search\n",
        "        contextual_embeddings = self.embedding_model.encode(\n",
        "            contextual_texts, normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "        umap_model = UMAP(**self.umap_params)\n",
        "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
        "\n",
        "        self.topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
        "\n",
        "        topic_to_contexts = defaultdict(list)\n",
        "        topic_to_entities = defaultdict(set)\n",
        "        topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "        for i, topic in enumerate(topics):\n",
        "            ent, context = entity_context_pairs[i]\n",
        "            topic_to_contexts[topic].append(context)\n",
        "            topic_to_entities[topic].add(ent)\n",
        "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id in topic_to_contexts:\n",
        "            embeddings = topic_to_embeddings[topic_id]\n",
        "            mean_emb = np.mean(embeddings, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": list(topic_to_entities[topic_id]),\n",
        "                \"sentences\": topic_to_contexts[topic_id],\n",
        "                \"sentence_embeddings\": np.array(embeddings)\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "        if len(self.topic_embeddings) < 1:\n",
        "            raise RuntimeError(\"No topic embeddings to index.\")\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 3)\n",
        "\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=1):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        results = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            seen = set()\n",
        "            unique_sentences = []\n",
        "            unique_embeddings = []\n",
        "\n",
        "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "                if sent not in seen:\n",
        "                    seen.add(sent)\n",
        "                    unique_sentences.append(sent)\n",
        "                    unique_embeddings.append(emb)\n",
        "\n",
        "            sent_embs = np.array(unique_embeddings)\n",
        "            if len(sent_embs) == 0:\n",
        "                continue\n",
        "\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            top_sents = [(unique_sentences[i], float(sims[i])) for i in top_indices]\n",
        "\n",
        "            results.append({\n",
        "                \"topic_id\": meta[\"topic_id\"],\n",
        "                \"entities\": list(meta[\"entities\"]),  # <- ensure it's a list in case it's a set\n",
        "                \"sentences\": top_sents,\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# === EVALUATION METRICS ===\n",
        "def compute_bertopic_coherence(topic_model, topic_metadata, topk=15):\n",
        "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
        "    topic_word_lists = [[word for word, _ in topic] for topic in topics]\n",
        "\n",
        "    texts = []\n",
        "    for meta in topic_metadata:\n",
        "        for sent in meta[\"sentences\"]:\n",
        "            tokens = clean_text(sent).split()\n",
        "            texts.append(tokens)\n",
        "\n",
        "    dictionary = Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    coherence_model = CoherenceModel(\n",
        "        topics=topic_word_lists,\n",
        "        texts=texts,\n",
        "        dictionary=dictionary,\n",
        "        coherence=\"c_v\"\n",
        "    )\n",
        "    return coherence_model.get_coherence()\n",
        "\n",
        "def compute_topic_diversity(topic_model, topic_metadata, topk=10):\n",
        "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
        "    all_words = [word for topic in topics for word, _ in topic]\n",
        "    return len(set(all_words)) / (len(topics) * topk)\n",
        "\n",
        "def compute_silhouette_score_custom(topic_metadata):\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    for meta in topic_metadata:\n",
        "        embeddings = meta[\"sentence_embeddings\"]\n",
        "        labels = [meta[\"topic_id\"]] * len(embeddings)\n",
        "        all_embeddings.append(embeddings)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    if len(all_embeddings) == 0:\n",
        "        return None\n",
        "\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "    n_samples = all_embeddings.shape[0]\n",
        "    n_labels = len(set(all_labels))\n",
        "\n",
        "    if n_labels < 2 or n_labels > n_samples - 1:\n",
        "        return None\n",
        "\n",
        "    return silhouette_score(all_embeddings, all_labels, metric=\"cosine\")\n",
        "\n",
        "# === DATASET & INITIALIZATION ===\n",
        "allergy_dataset = {\n",
        "  \"chunks\": [\n",
        "  \"A 68-year-old woman with a 15-year history of hypertension and type 2 diabetes mellitus presents complaining of persistent, dull headaches localized to the occipital region, occasional dizziness, and intermittent blurred vision over the last 6 weeks.\",\n",
        "  \"She has struggled with blood pressure control despite taking amlodipine 10 mg daily and hydrochlorothiazide 25 mg daily for several years. Home BP measurements often exceed 160/95 mmHg, particularly in the mornings.\",\n",
        "  \"Her past medical history also includes stage 3 chronic kidney disease with baseline creatinine around 1.4 mg/dL, hyperlipidemia treated with atorvastatin, and osteoarthritis limiting her mobility.\",\n",
        "  \"She reports new symptoms of swelling in both ankles and occasional shortness of breath during moderate exertion, which she attributes to her limited physical activity due to joint pain.\",\n",
        "  \"On examination, her blood pressure measured in clinic is 168/98 mmHg sitting, with a heart rate of 88 beats per minute, respiratory rate 18, temperature 36.8 °C, and oxygen saturation 96% on room air.\",\n",
        "  \"Cardiovascular examination reveals a displaced apical impulse palpable in the 6th intercostal space anterior axillary line, a grade 2/6 systolic murmur best heard at the cardiac apex, and no jugular venous distension.\",\n",
        "  \"Fundoscopic exam shows mild hypertensive changes, including arteriolar narrowing and scattered cotton wool spots, but no papilledema.\",\n",
        "  \"Respiratory exam is clear with normal breath sounds bilaterally; there is mild 1+ pitting edema in the ankles.\",\n",
        "  \"Laboratory studies from her recent outpatient evaluation indicate stable renal function with creatinine 1.4 mg/dL and estimated glomerular filtration rate (eGFR) of 48 mL/min/1.73 m², normal electrolytes, and HbA1c of 7.8%.\",\n",
        "  \"Her lipid panel reveals LDL cholesterol of 130 mg/dL despite ongoing statin therapy, with triglycerides at 160 mg/dL and HDL at 38 mg/dL.\",\n",
        "  \"A 12-lead ECG demonstrates left ventricular hypertrophy with strain pattern but no arrhythmias or conduction abnormalities.\",\n",
        "  \"Echocardiography confirms left ventricular hypertrophy with preserved left ventricular ejection fraction of 60%, mild left atrial enlargement, and no valvular abnormalities aside from mild mitral regurgitation.\",\n",
        "  \"She admits difficulty adhering to a low-sodium diet because of a preference for processed foods and challenges preparing meals at home.\",\n",
        "  \"Her physical activity is limited by osteoarthritis pain affecting knees and hips, and she rarely exceeds 1000 steps per day according to her activity tracker.\",\n",
        "  \"Medications were adjusted to include lisinopril 10 mg daily to provide better blood pressure control and nephroprotection; hydrochlorothiazide dose was decreased due to borderline low potassium levels.\",\n",
        "  \"She was counseled extensively on lifestyle modifications, including sodium restriction, weight reduction, and gradual increase in physical activity tailored to her joint limitations.\",\n",
        "  \"Referrals were made to a dietitian for nutritional counseling focusing on kidney-friendly, low-sodium meals, and to physical therapy for osteoarthritis management and tailored exercise program.\",\n",
        "  \"A cardiology appointment was scheduled within 4 weeks for blood pressure reassessment and echocardiographic follow-up, and nephrology follow-up planned to monitor kidney function closely.\",\n",
        "  \"Home health services were arranged to assist with blood pressure monitoring, medication reminders, and reinforcement of dietary adherence.\",\n",
        "  \"Social history reveals she lives with her husband in a single-story home, has a strong family support network, but experiences caregiver stress related to her mother-in-law’s dementia care.\",\n",
        "  \"She denies tobacco use and alcohol consumption but expresses feelings of anxiety and occasional insomnia due to stress.\",\n",
        "  \"Patient education was provided on the importance of medication adherence, recognizing signs of hypertensive crisis, and understanding potential complications such as stroke and heart failure.\",\n",
        "  \"Follow-up labs including renal function, electrolytes, and lipid profile were ordered to be done prior to her next clinic visit.\",\n",
        "  \"She was encouraged to maintain a daily blood pressure log and report any symptoms such as chest pain, worsening shortness of breath, or neurological changes promptly.\",\n",
        "  \"The care team emphasized the importance of a multidisciplinary approach, including medical, nutritional, and psychosocial support to optimize her hypertension management.\"\n",
        "]\n",
        ",\n",
        "  \"entities\":[\n",
        "  [\"68-year-old woman\", \"hypertension\", \"type 2 diabetes mellitus\", \"headaches\", \"dizziness\", \"blurred vision\", \"6 weeks\"],\n",
        "  [\"poorly controlled BP\", \"amlodipine 10 mg\", \"hydrochlorothiazide 25 mg\", \"home BP readings\", \"160/95 mmHg\", \"mornings\"],\n",
        "  [\"stage 3 chronic kidney disease\", \"creatinine 1.4 mg/dL\", \"hyperlipidemia\", \"atorvastatin\", \"osteoarthritis\", \"limited mobility\"],\n",
        "  [\"bilateral ankle swelling\", \"shortness of breath\", \"limited physical activity\", \"joint pain\"],\n",
        "  [\"clinic BP 168/98 mmHg\", \"HR 88 bpm\", \"RR 18\", \"Temp 36.8°C\", \"SpO2 96%\"],\n",
        "  [\"displaced apical impulse\", \"grade 2/6 systolic murmur\", \"no JVD\"],\n",
        "  [\"hypertensive retinopathy\", \"arteriolar narrowing\", \"cotton wool spots\", \"no papilledema\"],\n",
        "  [\"clear lungs\", \"1+ pitting edema ankles\"],\n",
        "  [\"renal function stable\", \"creatinine 1.4 mg/dL\", \"eGFR 48 mL/min/1.73 m²\", \"normal electrolytes\", \"HbA1c 7.8%\"],\n",
        "  [\"LDL 130 mg/dL\", \"statin therapy\", \"triglycerides 160 mg/dL\", \"HDL 38 mg/dL\"],\n",
        "  [\"ECG\", \"left ventricular hypertrophy\", \"strain pattern\", \"no arrhythmias\"],\n",
        "  [\"echocardiogram\", \"LV hypertrophy\", \"EF 60%\", \"left atrial enlargement\", \"mild mitral regurgitation\"],\n",
        "  [\"dietary noncompliance\", \"processed foods\", \"meal prep challenges\"],\n",
        "  [\"limited physical activity\", \"osteoarthritis pain\", \"knee and hip involvement\", \"1000 steps/day\"],\n",
        "  [\"lisinopril 10 mg\", \"hydrochlorothiazide dose decreased\", \"borderline low potassium\"],\n",
        "  [\"lifestyle counseling\", \"sodium restriction\", \"weight loss\", \"exercise modification\"],\n",
        "  [\"dietitian referral\", \"nutritional counseling\", \"kidney-friendly diet\", \"physical therapy referral\", \"tailored exercise program\"],\n",
        "  [\"cardiology follow-up\", \"nephrology follow-up\"],\n",
        "  [\"home health services\", \"BP monitoring\", \"medication reminders\", \"diet adherence support\"],\n",
        "  [\"lives with husband\", \"family support\", \"caregiver stress\", \"mother-in-law dementia\"],\n",
        "  [\"no tobacco\", \"no alcohol\", \"anxiety\", \"insomnia\", \"stress\"],\n",
        "  [\"patient education\", \"medication adherence\", \"hypertensive crisis signs\", \"complications education\"],\n",
        "  [\"follow-up labs\", \"renal function\", \"electrolytes\", \"lipid profile\"],\n",
        "  [\"daily BP log\", \"symptom monitoring\", \"chest pain\", \"neurological symptoms\"],\n",
        "  [\"multidisciplinary care\", \"medical\", \"nutritional\", \"psychosocial support\"]\n",
        "]\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "best_umap = {\"n_neighbors\": 5, \"n_components\": 5, \"min_dist\": 0.25, \"metric\": \"cosine\"}\n",
        "best_hdbscan = {\"min_cluster_size\": 2, \"min_samples\": 1, \"metric\": \"euclidean\"}\n",
        "\n",
        "searcher = AllergyTopicSearcher(\n",
        "    chunks=allergy_dataset[\"chunks\"],\n",
        "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
        "    umap_params=best_umap,\n",
        "    hdbscan_params=best_hdbscan,\n",
        "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
        ")\n",
        "print(\"✅ Model ready for querying.\")\n",
        "\n",
        "# === METRICS ===\n",
        "coherence = compute_bertopic_coherence(searcher.topic_model, searcher.topic_metadata, topk=15)\n",
        "diversity = compute_topic_diversity(searcher.topic_model, searcher.topic_metadata, topk=10)\n",
        "sil_score = compute_silhouette_score_custom(searcher.topic_metadata)\n",
        "\n",
        "print(\"\\n=== Topic Quality Metrics ===\")\n",
        "print(f\"🧪 Coherence Score (c_v): {coherence:.4f}\")\n",
        "print(f\"🌈 Topic Diversity: {diversity:.4f}\")\n",
        "if sil_score is not None:\n",
        "    print(f\"📐 Silhouette Score: {sil_score:.4f}\")\n",
        "else:\n",
        "    print(\"📐 Silhouette Score: Not applicable.\")\n",
        "\n",
        "# === GROUND TRUTH TOPICS ===\n",
        "ground_truth_topics=[\n",
        "  { \"topic_id\": \"T1\", \"entities\": [\"hypertension\", \"headaches\", \"dizziness\", \"blurred vision\", \"poorly controlled BP\", \"medication adherence\"] },\n",
        "  { \"topic_id\": \"T2\", \"entities\": [\"medications\", \"amlodipine\", \"hydrochlorothiazide\", \"lisinopril\", \"potassium\"] },\n",
        "  { \"topic_id\": \"T3\", \"entities\": [\"comorbidities\", \"type 2 diabetes mellitus\", \"chronic kidney disease\", \"hyperlipidemia\", \"osteoarthritis\"] },\n",
        "  { \"topic_id\": \"T4\", \"entities\": [\"symptoms\", \"ankle swelling\", \"shortness of breath\", \"limited physical activity\", \"fatigue\"] },\n",
        "  { \"topic_id\": \"T5\", \"entities\": [\"physical exam\", \"BP 168/98\", \"heart rate\", \"murmur\", \"apical impulse\", \"retinopathy\", \"edema\"] },\n",
        "  { \"topic_id\": \"T6\", \"entities\": [\"labs\", \"creatinine\", \"eGFR\", \"HbA1c\", \"lipid panel\", \"electrolytes\"] },\n",
        "  { \"topic_id\": \"T7\", \"entities\": [\"imaging\", \"ECG\", \"LV hypertrophy\", \"strain pattern\", \"echocardiogram\", \"mitral regurgitation\"] },\n",
        "  { \"topic_id\": \"T8\", \"entities\": [\"lifestyle\", \"dietary noncompliance\", \"processed foods\", \"exercise limitation\", \"weight loss\"] },\n",
        "  { \"topic_id\": \"T9\", \"entities\": [\"referrals\", \"dietitian\", \"physical therapy\", \"cardiology\", \"nephrology\"] },\n",
        "  { \"topic_id\": \"T10\", \"entities\": [\"social history\", \"married\", \"lives with spouse\", \"caregiver stress\", \"anxiety\", \"insomnia\"] },\n",
        "  { \"topic_id\": \"T11\", \"entities\": [\"patient education\", \"hypertension complications\", \"medication adherence\", \"lifestyle modifications\", \"crisis signs\"] },\n",
        "  { \"topic_id\": \"T12\", \"entities\": [\"follow-up\", \"home health\", \"blood pressure monitoring\", \"medication reminders\", \"labs\"] },\n",
        "  { \"topic_id\": \"T13\", \"entities\": [\"multidisciplinary care\", \"medical\", \"nutritional\", \"psychosocial support\"] }\n",
        "]\n",
        "\n",
        "\n",
        "# === EVALUATION CODE ===\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def normalize(entities):\n",
        "    return [e.lower().strip() for e in entities]\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    set1, set2 = set(set1), set(set2)\n",
        "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0.0\n",
        "\n",
        "# Prepare model topics\n",
        "model_topics = [\n",
        "    {\"topic_id\": meta[\"topic_id\"], \"entities\": normalize(meta[\"entities\"])}\n",
        "    for meta in searcher.topic_metadata\n",
        "]\n",
        "\n",
        "# Matching model topics to ground truth\n",
        "matched_gt_ids = set()\n",
        "matches = []\n",
        "all_model_entities = []\n",
        "all_gt_entities = []\n",
        "\n",
        "for mt in model_topics:\n",
        "    best_score = 0\n",
        "    best_gt = None\n",
        "    for gt in ground_truth_topics:\n",
        "        score = jaccard_similarity(mt[\"entities\"], normalize(gt[\"entities\"]))\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_gt = gt\n",
        "    if best_gt:\n",
        "        matches.append((mt[\"topic_id\"], best_gt[\"topic_id\"], best_score))\n",
        "        matched_gt_ids.add(best_gt[\"topic_id\"])\n",
        "\n",
        "        # Collect entities for entity-level precision/recall\n",
        "        all_model_entities.extend(mt[\"entities\"])\n",
        "        all_gt_entities.extend(normalize(best_gt[\"entities\"]))\n",
        "\n",
        "# Entity-level metrics\n",
        "model_entity_counter = Counter(all_model_entities)\n",
        "gt_entity_counter = Counter(all_gt_entities)\n",
        "\n",
        "unique_entities = list(set(list(model_entity_counter.keys()) + list(gt_entity_counter.keys())))\n",
        "y_true = [gt_entity_counter[e] > 0 for e in unique_entities]\n",
        "y_pred = [model_entity_counter[e] > 0 for e in unique_entities]\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "\n",
        "# Print evaluation\n",
        "print(\"\\n=== 📊 Topic Matching Summary ===\")\n",
        "for model_id, gt_id, score in matches:\n",
        "    print(f\"🔗 Model Topic {model_id} ↔ Ground Truth {gt_id} — Jaccard: {score:.2f}\")\n",
        "\n",
        "print(f\"\\n🧮 Average Jaccard Similarity: {sum(score for _, _, score in matches) / len(matches):.4f}\")\n",
        "print(f\"📈 Ground Truth Coverage: {len(matched_gt_ids)}/{len(ground_truth_topics)} \"\n",
        "      f\"({(len(matched_gt_ids)/len(ground_truth_topics))*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n=== 🧠 Entity-Level Evaluation ===\")\n",
        "print(f\"🎯 Precision: {precision:.4f}\")\n",
        "print(f\"🧲 Recall:    {recall:.4f}\")\n",
        "print(f\"🏅 F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# === QUERY LOOP ===\n",
        "print(\"\\n=== Allergy Topic Search ===\")\n",
        "while True:\n",
        "    query = input(\"\\nEnter a query (or type 'exit' to quit): \").strip()\n",
        "    if query.lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    results = searcher.search(query, top_k_topics=1, top_k_sents=3)\n",
        "    print(f\"\\n🔎 Top results for: '{query}'\")\n",
        "    for res in results:\n",
        "        print(f\"🧠 Topic ID: {res['topic_id']}\")\n",
        "        print(f\"🔗 Related Entities: {', '.join(res['entities'])}\")\n",
        "        for sent, _ in res[\"sentences\"]:\n",
        "            print(f\"✓ {sent}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6OSvZ-kXyOD",
        "outputId": "4769270c-c8ac-42ea-b6f0-d7125be27a43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model ready for querying.\n",
            "\n",
            "=== Topic Quality Metrics ===\n",
            "🧪 Coherence Score (c_v): 0.7728\n",
            "🌈 Topic Diversity: 0.6667\n",
            "📐 Silhouette Score: 0.5705\n",
            "\n",
            "=== 📊 Topic Matching Summary ===\n",
            "🔗 Model Topic 2 ↔ Ground Truth T1 — Jaccard: 0.44\n",
            "🔗 Model Topic 9 ↔ Ground Truth T1 — Jaccard: 0.11\n",
            "🔗 Model Topic 18 ↔ Ground Truth T3 — Jaccard: 0.14\n",
            "🔗 Model Topic 21 ↔ Ground Truth T3 — Jaccard: 0.17\n",
            "🔗 Model Topic 1 ↔ Ground Truth T4 — Jaccard: 0.17\n",
            "🔗 Model Topic 12 ↔ Ground Truth T7 — Jaccard: 0.25\n",
            "🔗 Model Topic 11 ↔ Ground Truth T7 — Jaccard: 0.25\n",
            "🔗 Model Topic 0 ↔ Ground Truth T8 — Jaccard: 0.21\n",
            "🔗 Model Topic 10 ↔ Ground Truth T11 — Jaccard: 0.29\n",
            "🔗 Model Topic 16 ↔ Ground Truth T12 — Jaccard: 0.14\n",
            "🔗 Model Topic 5 ↔ Ground Truth T13 — Jaccard: 0.12\n",
            "🔗 Model Topic 17 ↔ Ground Truth T10 — Jaccard: 0.12\n",
            "🔗 Model Topic 22 ↔ Ground Truth T10 — Jaccard: 0.14\n",
            "🔗 Model Topic 19 ↔ Ground Truth T6 — Jaccard: 0.12\n",
            "🔗 Model Topic 20 ↔ Ground Truth T13 — Jaccard: 0.75\n",
            "\n",
            "🧮 Average Jaccard Similarity: 0.2295\n",
            "📈 Ground Truth Coverage: 10/13 (76.9%)\n",
            "\n",
            "=== 🧠 Entity-Level Evaluation ===\n",
            "🎯 Precision: 0.4118\n",
            "🧲 Recall:    0.5490\n",
            "🏅 F1 Score:  0.4706\n",
            "\n",
            "=== Allergy Topic Search ===\n",
            "\n",
            "Enter a query (or type 'exit' to quit): patient medical history\n",
            "\n",
            "🔎 Top results for: 'patient medical history'\n",
            "🧠 Topic ID: 10\n",
            "🔗 Related Entities: patient education, hypertensive crisis signs, home health services, medication adherence\n",
            "✓ patient education was provided on the importance of medication adherence recognizing signs of hypertensive crisis and understanding potential complications such as stroke and heart failure\n",
            "✓ home health services were arranged to assist with blood pressure monitoring medication reminders and reinforcement of dietary adherence\n",
            "\n",
            "Enter a query (or type 'exit' to quit): vital signs on admission\n",
            "\n",
            "🔎 Top results for: 'vital signs on admission'\n",
            "🧠 Topic ID: 4\n",
            "🔗 Related Entities: daily bp log, chest pain, symptom monitoring, neurological symptoms, complications education\n",
            "✓ patient education was provided on the importance of medication adherence recognizing signs of hypertensive crisis and understanding potential complications such as stroke and heart failure\n",
            "✓ she was encouraged to maintain a daily blood pressure log and report any symptoms such as chest pain worsening shortness of breath or neurological changes promptly\n",
            "\n",
            "Enter a query (or type 'exit' to quit): physical exam findings\"\n",
            "\n",
            "🔎 Top results for: 'physical exam findings\"'\n",
            "🧠 Topic ID: 15\n",
            "🔗 Related Entities: cardiology follow-up, nephrology follow-up, renal function\n",
            "✓ followup labs including renal function electrolytes and lipid profile were ordered to be done prior to her next clinic visit\n",
            "✓ a cardiology appointment was scheduled within 4 weeks for blood pressure reassessment and echocardiographic followup and nephrology followup planned to monitor kidney function closely\n",
            "\n",
            "Enter a query (or type 'exit' to quit): current health problems\n",
            "\n",
            "🔎 Top results for: 'current health problems'\n",
            "🧠 Topic ID: 10\n",
            "🔗 Related Entities: patient education, hypertensive crisis signs, home health services, medication adherence\n",
            "✓ patient education was provided on the importance of medication adherence recognizing signs of hypertensive crisis and understanding potential complications such as stroke and heart failure\n",
            "✓ home health services were arranged to assist with blood pressure monitoring medication reminders and reinforcement of dietary adherence\n",
            "\n",
            "Enter a query (or type 'exit' to quit): social and lifestyle factors\n",
            "\n",
            "🔎 Top results for: 'social and lifestyle factors'\n",
            "🧠 Topic ID: 0\n",
            "🔗 Related Entities: dietitian referral, processed foods, meal prep challenges, tailored exercise program, kidney-friendly diet, weight loss, exercise modification, sodium restriction, lifestyle counseling, nutritional counseling, physical therapy referral, dietary noncompliance\n",
            "✓ she was counseled extensively on lifestyle modifications including sodium restriction weight reduction and gradual increase in physical activity tailored to her joint limitations\n",
            "✓ referrals were made to a dietitian for nutritional counseling focusing on kidneyfriendly lowsodium meals and to physical therapy for osteoarthritis management and tailored exercise program\n",
            "✓ she admits difficulty adhering to a lowsodium diet because of a preference for processed foods and challenges preparing meals at home\n",
            "\n",
            "Enter a query (or type 'exit' to quit): medications list\n",
            "\n",
            "🔎 Top results for: 'medications list'\n",
            "🧠 Topic ID: 10\n",
            "🔗 Related Entities: patient education, hypertensive crisis signs, home health services, medication adherence\n",
            "✓ home health services were arranged to assist with blood pressure monitoring medication reminders and reinforcement of dietary adherence\n",
            "✓ patient education was provided on the importance of medication adherence recognizing signs of hypertensive crisis and understanding potential complications such as stroke and heart failure\n",
            "\n",
            "Enter a query (or type 'exit' to quit): exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# === Helper Functions ===\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):\n",
        "    entity_context_pairs = []\n",
        "    for idx, ents in enumerate(entities_per_chunk):\n",
        "        chunk = clean_text(chunks[idx])\n",
        "        sentences = sent_tokenize(chunk)\n",
        "        for ent in ents:\n",
        "            ent_lower = ent.lower()\n",
        "            matched = False\n",
        "            for i, sent in enumerate(sentences):\n",
        "                if ent_lower in sent:\n",
        "                    context = (\n",
        "                        \" \".join(sentences[max(0, i - 1): i + 2])\n",
        "                        if use_multi_sentence else sent.strip()\n",
        "                    )\n",
        "                    entity_context_pairs.append((ent_lower, context.strip()))\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                entity_context_pairs.append((ent_lower, chunk))\n",
        "    return entity_context_pairs\n",
        "\n",
        "def compute_coherence_and_diversity(topic_model, texts, topic_metadata, topk=10):\n",
        "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
        "    topic_word_lists = [[word for word, _ in topic] for topic in topics]\n",
        "\n",
        "    tokenized_texts = [clean_text(doc).split() for doc in texts]\n",
        "    dictionary = Dictionary(tokenized_texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "\n",
        "    coherence_model = CoherenceModel(\n",
        "        topics=topic_word_lists,\n",
        "        texts=tokenized_texts,\n",
        "        dictionary=dictionary,\n",
        "        coherence=\"c_v\"\n",
        "    )\n",
        "    coherence = coherence_model.get_coherence()\n",
        "\n",
        "    all_words = [word for topic in topic_word_lists for word in topic]\n",
        "    diversity = len(set(all_words)) / (len(topic_word_lists) * topk)\n",
        "    return coherence, diversity\n",
        "\n",
        "# === Your Dataset ===\n",
        "chunks = [\n",
        "  \"A 68-year-old woman with a 15-year history of hypertension and type 2 diabetes mellitus presents complaining of persistent, dull headaches localized to the occipital region, occasional dizziness, and intermittent blurred vision over the last 6 weeks.\",\n",
        "  \"She has struggled with blood pressure control despite taking amlodipine 10 mg daily and hydrochlorothiazide 25 mg daily for several years. Home BP measurements often exceed 160/95 mmHg, particularly in the mornings.\",\n",
        "  \"Her past medical history also includes stage 3 chronic kidney disease with baseline creatinine around 1.4 mg/dL, hyperlipidemia treated with atorvastatin, and osteoarthritis limiting her mobility.\",\n",
        "  \"She reports new symptoms of swelling in both ankles and occasional shortness of breath during moderate exertion, which she attributes to her limited physical activity due to joint pain.\",\n",
        "  \"On examination, her blood pressure measured in clinic is 168/98 mmHg sitting, with a heart rate of 88 beats per minute, respiratory rate 18, temperature 36.8 °C, and oxygen saturation 96% on room air.\",\n",
        "  \"Cardiovascular examination reveals a displaced apical impulse palpable in the 6th intercostal space anterior axillary line, a grade 2/6 systolic murmur best heard at the cardiac apex, and no jugular venous distension.\",\n",
        "  \"Fundoscopic exam shows mild hypertensive changes, including arteriolar narrowing and scattered cotton wool spots, but no papilledema.\",\n",
        "  \"Respiratory exam is clear with normal breath sounds bilaterally; there is mild 1+ pitting edema in the ankles.\",\n",
        "  \"Laboratory studies from her recent outpatient evaluation indicate stable renal function with creatinine 1.4 mg/dL and estimated glomerular filtration rate (eGFR) of 48 mL/min/1.73 m², normal electrolytes, and HbA1c of 7.8%.\",\n",
        "  \"Her lipid panel reveals LDL cholesterol of 130 mg/dL despite ongoing statin therapy, with triglycerides at 160 mg/dL and HDL at 38 mg/dL.\",\n",
        "  \"A 12-lead ECG demonstrates left ventricular hypertrophy with strain pattern but no arrhythmias or conduction abnormalities.\",\n",
        "  \"Echocardiography confirms left ventricular hypertrophy with preserved left ventricular ejection fraction of 60%, mild left atrial enlargement, and no valvular abnormalities aside from mild mitral regurgitation.\",\n",
        "  \"She admits difficulty adhering to a low-sodium diet because of a preference for processed foods and challenges preparing meals at home.\",\n",
        "  \"Her physical activity is limited by osteoarthritis pain affecting knees and hips, and she rarely exceeds 1000 steps per day according to her activity tracker.\",\n",
        "  \"Medications were adjusted to include lisinopril 10 mg daily to provide better blood pressure control and nephroprotection; hydrochlorothiazide dose was decreased due to borderline low potassium levels.\",\n",
        "  \"She was counseled extensively on lifestyle modifications, including sodium restriction, weight reduction, and gradual increase in physical activity tailored to her joint limitations.\",\n",
        "  \"Referrals were made to a dietitian for nutritional counseling focusing on kidney-friendly, low-sodium meals, and to physical therapy for osteoarthritis management and tailored exercise program.\",\n",
        "  \"A cardiology appointment was scheduled within 4 weeks for blood pressure reassessment and echocardiographic follow-up, and nephrology follow-up planned to monitor kidney function closely.\",\n",
        "  \"Home health services were arranged to assist with blood pressure monitoring, medication reminders, and reinforcement of dietary adherence.\",\n",
        "  \"Social history reveals she lives with her husband in a single-story home, has a strong family support network, but experiences caregiver stress related to her mother-in-law’s dementia care.\",\n",
        "  \"She denies tobacco use and alcohol consumption but expresses feelings of anxiety and occasional insomnia due to stress.\",\n",
        "  \"Patient education was provided on the importance of medication adherence, recognizing signs of hypertensive crisis, and understanding potential complications such as stroke and heart failure.\",\n",
        "  \"Follow-up labs including renal function, electrolytes, and lipid profile were ordered to be done prior to her next clinic visit.\",\n",
        "  \"She was encouraged to maintain a daily blood pressure log and report any symptoms such as chest pain, worsening shortness of breath, or neurological changes promptly.\",\n",
        "  \"The care team emphasized the importance of a multidisciplinary approach, including medical, nutritional, and psychosocial support to optimize her hypertension management.\"\n",
        "]\n",
        "entities = [\n",
        "  [\"68-year-old woman\", \"hypertension\", \"type 2 diabetes mellitus\", \"headaches\", \"dizziness\", \"blurred vision\", \"6 weeks\"],\n",
        "  [\"poorly controlled BP\", \"amlodipine 10 mg\", \"hydrochlorothiazide 25 mg\", \"home BP readings\", \"160/95 mmHg\", \"mornings\"],\n",
        "  [\"stage 3 chronic kidney disease\", \"creatinine 1.4 mg/dL\", \"hyperlipidemia\", \"atorvastatin\", \"osteoarthritis\", \"limited mobility\"],\n",
        "  [\"bilateral ankle swelling\", \"shortness of breath\", \"limited physical activity\", \"joint pain\"],\n",
        "  [\"clinic BP 168/98 mmHg\", \"HR 88 bpm\", \"RR 18\", \"Temp 36.8°C\", \"SpO2 96%\"],\n",
        "  [\"displaced apical impulse\", \"grade 2/6 systolic murmur\", \"no JVD\"],\n",
        "  [\"hypertensive retinopathy\", \"arteriolar narrowing\", \"cotton wool spots\", \"no papilledema\"],\n",
        "  [\"clear lungs\", \"1+ pitting edema ankles\"],\n",
        "  [\"renal function stable\", \"creatinine 1.4 mg/dL\", \"eGFR 48 mL/min/1.73 m²\", \"normal electrolytes\", \"HbA1c 7.8%\"],\n",
        "  [\"LDL 130 mg/dL\", \"statin therapy\", \"triglycerides 160 mg/dL\", \"HDL 38 mg/dL\"],\n",
        "  [\"ECG\", \"left ventricular hypertrophy\", \"strain pattern\", \"no arrhythmias\"],\n",
        "  [\"echocardiogram\", \"LV hypertrophy\", \"EF 60%\", \"left atrial enlargement\", \"mild mitral regurgitation\"],\n",
        "  [\"dietary noncompliance\", \"processed foods\", \"meal prep challenges\"],\n",
        "  [\"limited physical activity\", \"osteoarthritis pain\", \"knee and hip involvement\", \"1000 steps/day\"],\n",
        "  [\"lisinopril 10 mg\", \"hydrochlorothiazide dose decreased\", \"borderline low potassium\"],\n",
        "  [\"lifestyle counseling\", \"sodium restriction\", \"weight loss\", \"exercise modification\"],\n",
        "  [\"dietitian referral\", \"nutritional counseling\", \"kidney-friendly diet\", \"physical therapy referral\", \"tailored exercise program\"],\n",
        "  [\"cardiology follow-up\", \"nephrology follow-up\"],\n",
        "  [\"home health services\", \"BP monitoring\", \"medication reminders\", \"diet adherence support\"],\n",
        "  [\"lives with husband\", \"family support\", \"caregiver stress\", \"mother-in-law dementia\"],\n",
        "  [\"no tobacco\", \"no alcohol\", \"anxiety\", \"insomnia\", \"stress\"],\n",
        "  [\"patient education\", \"medication adherence\", \"hypertensive crisis signs\", \"complications education\"],\n",
        "  [\"follow-up labs\", \"renal function\", \"electrolytes\", \"lipid profile\"],\n",
        "  [\"daily BP log\", \"symptom monitoring\", \"chest pain\", \"neurological symptoms\"],\n",
        "  [\"multidisciplinary care\", \"medical\", \"nutritional\", \"psychosocial support\"]\n",
        "]\n",
        "\n",
        "# === Embedding & Context Preparation ===\n",
        "embedding_model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
        "entity_context_pairs = extract_entity_contexts(chunks, entities)\n",
        "texts = [context for _, context in entity_context_pairs]\n",
        "embeddings = embedding_model.encode(texts, normalize_embeddings=True)\n",
        "\n",
        "# === Parameter Grid ===\n",
        "umap_grid = [{\"n_neighbors\": n, \"n_components\": 5, \"min_dist\": d}\n",
        "             for n, d in product([5, 10], [0.1, 0.25])]\n",
        "hdbscan_grid = [{\"min_cluster_size\": c, \"min_samples\": s}\n",
        "                for c, s in product([2, 5], [1, 5])]\n",
        "\n",
        "best_score = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "best_coherence = 0\n",
        "best_diversity = 0\n",
        "\n",
        "# === Grid Search Loop ===\n",
        "for umap_params in umap_grid:\n",
        "    for hdbscan_params in hdbscan_grid:\n",
        "        try:\n",
        "            umap_model = UMAP(metric=\"cosine\", random_state=42, **umap_params)\n",
        "            hdbscan_model = HDBSCAN(metric=\"euclidean\", prediction_data=True, **hdbscan_params)\n",
        "\n",
        "            topic_model = BERTopic(\n",
        "                embedding_model=embedding_model,\n",
        "                umap_model=umap_model,\n",
        "                hdbscan_model=hdbscan_model,\n",
        "                representation_model=KeyBERTInspired(),\n",
        "                calculate_probabilities=False,\n",
        "                verbose=False,\n",
        "            )\n",
        "\n",
        "            topics, _ = topic_model.fit_transform(texts, embeddings)\n",
        "            labels = np.array(topics)\n",
        "            valid_idx = labels != -1\n",
        "            if sum(valid_idx) < 2:\n",
        "                continue\n",
        "\n",
        "            sil_score = silhouette_score(np.array(embeddings)[valid_idx], labels[valid_idx], metric=\"cosine\")\n",
        "            topic_metadata = []\n",
        "            for tid in set(topics):\n",
        "                if tid == -1:\n",
        "                    continue\n",
        "                inds = [i for i, t in enumerate(topics) if t == tid]\n",
        "                topic_metadata.append({\n",
        "                    \"topic_id\": tid,\n",
        "                    \"sentences\": [texts[i] for i in inds]\n",
        "                })\n",
        "\n",
        "            coherence, diversity = compute_coherence_and_diversity(topic_model, texts, topic_metadata)\n",
        "\n",
        "            if sil_score > best_score:\n",
        "                best_score = sil_score\n",
        "                best_model = topic_model\n",
        "                best_params = {\"umap\": umap_params, \"hdbscan\": hdbscan_params}\n",
        "                best_coherence = coherence\n",
        "                best_diversity = diversity\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "# === Print Best Model Results ===\n",
        "print(\"\\n📊 Best Hyperparameter Combination Found:\")\n",
        "print(f\"UMAP Params: {best_params['umap']}\")\n",
        "print(f\"HDBSCAN Params: {best_params['hdbscan']}\")\n",
        "print(f\"✅ Silhouette Score: {best_score:.4f}\")\n",
        "print(f\"🧠 Coherence Score (c_v): {best_coherence:.4f}\")\n",
        "print(f\"🌈 Topic Diversity: {best_diversity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuFk6bAdZr0I",
        "outputId": "1d08f41e-711b-4a85-89d2-f2c8df2621fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Best Hyperparameter Combination Found:\n",
            "UMAP Params: {'n_neighbors': 5, 'n_components': 5, 'min_dist': 0.25}\n",
            "HDBSCAN Params: {'min_cluster_size': 2, 'min_samples': 1}\n",
            "✅ Silhouette Score: 0.6048\n",
            "🧠 Coherence Score (c_v): 0.7722\n",
            "🌈 Topic Diversity: 0.6957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#top 3-topics-score\n",
        "#dynamic thrsholding on the output\n",
        "#3x of existing data(10000 characters)- fie tune parameters\n",
        "#entity occuring at multi places"
      ],
      "metadata": {
        "id": "n5h-whJPdJxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}