import json

response_content = response.content.decode("utf-8")
docai = json.loads(response_content)

import json

def load_docai_json(response_content):
    doc = json.loads(response_content)
    output = doc.get("output", {})
    full_text = output.get("text", "")
    pages = output.get("pages", [])
    return full_text, pages

full_text, pages = load_docai_json(response_content)

def explore_docai_structure(full_text, pages):
    for page_num, page in enumerate(pages):
        print(f"\n--- Page {page_num + 1} ---")
        for block_idx, block in enumerate(page.get("blocks", [])):
            layout = block.get("layout", {})
            anchor = layout.get("textAnchor", {})
            vertices = layout.get("boundingPoly", {}).get("vertices", [])
            block_text = extract_text_from_anchor_with_context(anchor, full_text).strip()

            print(f"\n[Block {block_idx}]")
            print(f"Text: {block_text}")
            print(f"Text Anchor: {anchor}")
            print(f"Bounding Poly: {vertices}")

def extract_text_from_anchor_with_context(text_anchor, full_text, window=0):
    if not text_anchor or "textSegments" not in text_anchor:
        return ""
    
    segments = text_anchor.get("textSegments", [])
    if not segments:
        return ""

    texts = []
    for segment in segments:
        start = int(segment.get("startIndex", 0))
        end = int(segment.get("endIndex", 0))
        if start >= end or end > len(full_text):
            continue
        texts.append(full_text[start:end])
    return " ".join(texts).strip()

explore_docai_structure(full_text, pages)

def merge_text_from_docai_blocks(full_text, pages):
    merged_texts = []
    for page in pages:
        for block in page.get("blocks", []):
            layout = block.get("layout", {})
            anchor = layout.get("textAnchor", {})
            block_text = extract_text_from_anchor_with_context(anchor, full_text)
            block_text = block_text.strip()
            if block_text:
                merged_texts.append(block_text)
    return merged_texts

def print_merged_docai_text(full_text, pages):
    merged_chunks = merge_text_from_docai_blocks(full_text, pages)
    if not merged_chunks:
        print("No text found in blocks.")
    else:
        print("=== Merged Text from DocAI Blocks ===")
        for idx, chunk in enumerate(merged_chunks):
            print(f"[Block {idx + 1}]:")
            print(chunk)
            print("-----------------------")

# Then just call:
print_merged_docai_text(full_text, pages)
# print(full_text)


#sample output
=== Merged Text from DocAI Blocks ===
[Block 1]:
09/06/2023 12:45:14 PM CLOUDFAX
-----------------------
[Block 2]:
Disney Land Health System
Savannah, Georgia
-----------------------
[Block 3]:
Patient: Mouse, Mini
DOB: 09/30/1952
Age/Sex: 70/F
-----------------------
[Block 4]:
Loc: C DS
-----------------------
[Block 5]:
Attending Dr: Donald Duck MD
-----------------------
[Block 6]:
Initialization Date/Time: 10/10/22,1106
-----------------------
[Block 7]:
Operative Note
-----------------------
[Block 8]:
Operative Note
Operative Note:
-----------------------
[Block 9]:
Pre-Op Diagnosis:
-----------------------
[Block 10]:
1. Left Breast Invasive Ductal Cancer
-----------------------
[Block 11]:
Post-Op Diagnosis:
-----------------------
[Block 12]:
Same
-----------------------
[Block 13]:
Operative Note
Signed
-----------------------
[Block 14]:
PAGE 11
-----------------------
[Block 15]:
OF 93
-----------------------
[Block 16]:
MR#: F29
Acct.J0 45
-----------------------
[Block 17]:
ADM Date:
10/10/22
-----------------------
#end of output

import os
import re
import json
import numpy as np
import requests
from typing import List, Dict, Optional
from dataclasses import dataclass

import vertexai
from vertexai.language_models import TextEmbeddingModel
from google.cloud import bigquery
from google.cloud import bigquery_storage

# --- CONFIG ---
PROJECT_ID = PROJECT_ID
LOCATION = LOCATION
BQ_CUI_TABLE = BQ_CUI_TABLE

# --- Initialize Vertex AI ---
vertexai.init(project=PROJECT_ID, location=LOCATION)
GEMINI_EMBEDDING_MODEL = "gemini-embedding-001"

# --- Embedding function ---
def gemini_embed_single(text: str) -> np.ndarray:
    model = TextEmbeddingModel.from_pretrained(GEMINI_EMBEDDING_MODEL)
    embedding = model.get_embeddings([text])[0]
    return np.array(embedding.values, dtype=np.float32)

# --- Cosine similarity ---
def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    a_norm = np.linalg.norm(a) + 1e-8
    b_norm = np.linalg.norm(b) + 1e-8
    return float(np.dot(a, b) / (a_norm * b_norm))

# --- BigQuery CUI Embedding Fetch ---
def fetch_cui_embeddings(project_id: str, table_fqn: str, cuis: List[str]) -> Dict[str, np.ndarray]:
    print(f"Fetching embeddings for CUIs: {cuis}")
    bq_client = bigquery.Client(project=project_id)
    bqs_client = bigquery_storage.BigQueryReadClient()

    if not cuis:
        print("No CUIs provided to fetch embeddings.")
        return {}

    formatted_cuis = ','.join([f"'{cui}'" for cui in cuis])
    query = f"""
        SELECT REF_cui, REF_Embedding
        FROM `{table_fqn}`
        WHERE REF_cui IN ({formatted_cuis})
    """
    query_job = bq_client.query(query)
    df = query_job.result().to_dataframe(bqstorage_client=bqs_client)

    embeddings = {}
    for _, row in df.iterrows():
        embeddings[row["REF_cui"]] = np.array(row["REF_Embedding"], dtype=np.float32).ravel()

    print(f"Fetched embeddings for CUIs: {list(embeddings.keys())}")
    return embeddings

# --- Entity dataclass ---
@dataclass
class Entity:
    entity_id: str
    text: str
    cui: Optional[str] = None
    emb: Optional[np.ndarray] = None

# --- Parse entities from internal NER response ---
def parse_entities_from_ner_response(phrases_json) -> List[Entity]:
    entities = []

    for ent_id, ent_val in phrases_json.items():
        if not isinstance(ent_val, dict):
            continue

        # Extract text safely
        text_list = ent_val.get("text", [])
        text = " ".join(str(t) for t in text_list if isinstance(t, str)) if text_list else ""

        mappings = ent_val.get("mappings", {})
        cuis = set()

        for mapping in mappings.values():
            candidates = mapping.get("candidates", {})
            if isinstance(candidates, dict):
                for candidate in candidates.values():
                    cui = candidate.get("cui")
                    if cui:
                        cuis.add(cui)

        if cuis:
            for idx, cui in enumerate(cuis):
                entity_id = f"{ent_id}_{idx}"
                entities.append(Entity(entity_id=entity_id, text=text, cui=cui))
        else:
            entities.append(Entity(entity_id=ent_id, text=text, cui=None))

    print(f"Parsed {len(entities)} entities. Extracted CUIs: {[e.cui for e in entities]}")
    return entities

# --- Attach embeddings to entities ---
def attach_embeddings(entities: List[Entity], project_id: str, cui_table: str):
    unique_cuis = list({e.cui for e in entities if e.cui})
    print(f"Unique CUIs to fetch embeddings for: {unique_cuis}")

    cui_embeddings = fetch_cui_embeddings(project_id, cui_table, unique_cuis)

    for e in entities:
        if e.cui and e.cui in cui_embeddings:
            e.emb = cui_embeddings[e.cui]
            print(f"Entity {e.entity_id} assigned embedding from CUI {e.cui}")
        else:
            e.emb = gemini_embed_single(e.text)
            print(f"Entity {e.entity_id} assigned embedding from text fallback")

# --- Query embedding and matching ---
def query_match(query_text: str, entities: List[Entity], top_k: int = 5) -> List[Dict]:
    query_emb = gemini_embed_single(query_text)
    print(f"Generated embedding for query: '{query_text}'")

    scored = []
    for e in entities:
        if e.emb is None:
            continue
        score = cosine_sim(query_emb, e.emb)
        scored.append((e, score))

    scored.sort(key=lambda x: x[1], reverse=True)

    results = []
    for e, score in scored[:top_k]:
        results.append({
            "entity_id": e.entity_id,
            "text": e.text,
            "cui": e.cui,
            "score": round(score, 4)
        })
    print(f"Top {top_k} matches: {results}")
    return results

# --- Example usage ---
try:
    ner_json = response2.json()
except json.JSONDecodeError:
    print("NER response is not JSON:", response2.text)
    ner_json = {}

phrases = {}
try:
    phrases = (
        ner_json["output"]["0"]["utterances"]["0"]["phrases"]
    )
except (KeyError, TypeError) as e:
    print("Failed to extract phrases:", str(e))
print(f"Phrases keys: {list(phrases.keys())}")
# Step 2: Extract all phrases from all utterances
phrases = {}

try:
    output = ner_json.get("output", {})
    for out_val in output.values():  # e.g., output["0"], output["1"], ...
        if not isinstance(out_val, dict):
            continue
        utterances = out_val.get("utterances", {})
        for utt_val in utterances.values():  # e.g., utterances["0"]
            if isinstance(utt_val, dict) and "phrases" in utt_val:
                phrases.update(utt_val["phrases"])
except Exception as e:
    print("Failed to extract phrases from NER JSON:", e)

print(f"Phrases keys: {list(phrases.keys())}")

# Step 3: Parse entities from phrases
entities = parse_entities_from_ner_response(phrases) or []

attach_embeddings(entities, PROJECT_ID, BQ_CUI_TABLE)


# Step 4: Query example
query = "admission"
matches = query_match(query, entities)

print("Top matches for query:", matches)

# After extracting phrases and parsing entities
print(f"Phrases keys: {list(phrases.keys())}")

entities = parse_entities_from_ner_response(phrases) or []

print(f"Parsed {len(entities)} entities:")
for e in entities:
    print(f"Entity ID: {e.entity_id}, Text: '{e.text}', CUI: {e.cui}")

#sample output
Entity ID: 0_0, Text: 'Health information Management Rpt', CUI: C3494384
Entity ID: 1, Text: ':', CUI: None
Entity ID: 2_0, Text: '1010-00867 Operative Note', CUI: C0551628
#end of sample output
