{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDfiooSu4Sng"
   },
   "outputs": [],
   "source": [
    "#Without LLM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5J8UH7f893d-"
   },
   "outputs": [],
   "source": [
    "# Google Cloud Document AI client\n",
    "!pip install google-cloud-documentai\n",
    "\n",
    "# Google Cloud BigQuery (for reading CUI vectors)\n",
    "!pip install pandas-gbq google-cloud-bigquery\n",
    "\n",
    "# Numpy\n",
    "!pip install numpy\n",
    "\n",
    "# Google GenAI SDK (for Vertex AI embeddings)\n",
    "!pip install google-genai\n",
    "\n",
    "# Optional but useful: pandas (for dataframe handling)\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufAgB9e194zx"
   },
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Complete Code: DocAI Extraction + Context Pyramid + CUI Graph\n",
    "# ==============================================\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ========== DocAI imports ==========\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from google.cloud.documentai_v1 import DocumentProcessorServiceClient\n",
    "\n",
    "# ========== Your CONFIG ==========\n",
    "PROJECT_ID = \"YOUR_GCP_PROJECT\"\n",
    "LOCATION = \"us\"  # or your DocAI location (\"us\", \"eu\", \"global\", etc)\n",
    "BQ_CUI_TABLE = \"your_project.your_dataset.cui_embeddings\"\n",
    "DOCAI_PROCESSOR_ID = \"your-docai-processor-id\"\n",
    "\n",
    "# Set environment variables for Vertex AI embeddings\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
    "\n",
    "# ========== Vertex AI embeddings ==========\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "_client = genai.Client()\n",
    "\n",
    "def gemini_embed(texts: List[str], *, task_type: str=\"RETRIEVAL_DOCUMENT\") -> np.ndarray:\n",
    "    \"\"\"Context-aware embeddings using Vertex AI Gemini.\"\"\"\n",
    "    cfg = EmbedContentConfig(task_type=task_type)\n",
    "    resp = _client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=texts,\n",
    "        config=cfg,\n",
    "    )\n",
    "    return np.array([e.values for e in resp.embeddings], dtype=np.float32)\n",
    "\n",
    "# ========== BigQuery CUI vectors ==========\n",
    "import pandas_gbq\n",
    "def load_cui_vectors_bq(table_fqn: str = BQ_CUI_TABLE) -> Dict[str, np.ndarray]:\n",
    "    df = pandas_gbq.read_gbq(f\"SELECT cui, embedding FROM `{table_fqn}`\", project_id=PROJECT_ID)\n",
    "    return {row[\"cui\"]: np.array(row[\"embedding\"], dtype=np.float32) for _, row in df.iterrows()}\n",
    "\n",
    "CUI_VECTORS = load_cui_vectors_bq(BQ_CUI_TABLE)\n",
    "\n",
    "# ========== UMLS linker stub ==========\n",
    "def umls_link(text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Replace with your UMLS linker; return list of {cui, score}.\"\"\"\n",
    "    return []\n",
    "\n",
    "# ========== DocAI Extraction ==========\n",
    "def extract_docai_document(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts document using Google Cloud Document AI and returns the parsed JSON.\n",
    "    Args:\n",
    "      file_path: path to local file (PDF, TIFF, etc.)\n",
    "\n",
    "    Returns:\n",
    "      dict: DocAI document response.\n",
    "    \"\"\"\n",
    "    client = DocumentProcessorServiceClient()\n",
    "\n",
    "    # Full resource name of the processor\n",
    "    name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/processors/{DOCAI_PROCESSOR_ID}\"\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    raw_document = documentai.RawDocument(content=file_content, mime_type=\"application/pdf\")\n",
    "\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name,\n",
    "        raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    document = result.document\n",
    "    # Convert protobuf Document to dict/json\n",
    "    doc_dict = json.loads(document.to_json())\n",
    "    return doc_dict\n",
    "\n",
    "# ========== Date regex and normalization ==========\n",
    "DATE_REGEX = re.compile(r\"\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})(?:[ ,;]+(\\d{1,2}[:.]\\d{2}))?\\b\")\n",
    "\n",
    "def norm_date(s: str) -> Optional[str]:\n",
    "    s = s.strip().replace('.', ':').replace(',', ' ')\n",
    "    try:\n",
    "        for fmt in (\"%m/%d/%Y\", \"%m/%d/%y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "            try:\n",
    "                return datetime.strptime(s.split()[0], fmt).strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        m = DATE_REGEX.search(s)\n",
    "        if m:\n",
    "            raw = m.group(1)\n",
    "            for fmt in (\"%m/%d/%Y\", \"%m/%d/%y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "                try:\n",
    "                    return datetime.strptime(raw, fmt).strftime(\"%Y-%m-%d\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ========== Data classes ==========\n",
    "@dataclass\n",
    "class Section:\n",
    "    section_id: str\n",
    "    page: int\n",
    "    title: str\n",
    "    text_snippet: str\n",
    "    emb: Optional[np.ndarray] = None\n",
    "    cui: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    entity_id: str\n",
    "    page: int\n",
    "    text: str\n",
    "    kind: str\n",
    "    section_title: Optional[str]\n",
    "    value_norm_date: Optional[str]\n",
    "    emb: Optional[np.ndarray] = None\n",
    "    cui: Optional[str] = None\n",
    "\n",
    "# ========== Parse DocAI response into pyramid ==========\n",
    "def parse_docai_to_pyramid(docai: Dict):\n",
    "    pages = docai.get(\"pages\", [])\n",
    "    sections: List[Section] = []\n",
    "    entities: List[Entity] = []\n",
    "    headers_by_page: Dict[int, List[str]] = {}\n",
    "\n",
    "    for p_idx, page in enumerate(pages):\n",
    "        headers_by_page[p_idx] = []\n",
    "\n",
    "        # Form fields (KVs)\n",
    "        for ff in page.get(\"formFields\", []) or []:\n",
    "            name = ff.get(\"fieldName\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ff.get(\"fieldName\", {}).get(\"content\", \"\")\n",
    "            value = ff.get(\"fieldValue\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ff.get(\"fieldValue\", {}).get(\"content\", \"\")\n",
    "            kv_text = f\"{name.strip()}: {value.strip()}\".strip(\": \")\n",
    "            entities.append(Entity(\n",
    "                entity_id=f\"kv:{p_idx}:{len(entities)}\",\n",
    "                page=p_idx, text=kv_text, kind=\"kv\",\n",
    "                section_title=None, value_norm_date=norm_date(kv_text)\n",
    "            ))\n",
    "            if DATE_REGEX.search(kv_text or \"\"):\n",
    "                entities.append(Entity(\n",
    "                    entity_id=f\"date:{p_idx}:{len(entities)}\",\n",
    "                    page=p_idx, text=kv_text, kind=\"date\",\n",
    "                    section_title=None, value_norm_date=norm_date(kv_text)\n",
    "                ))\n",
    "\n",
    "        # Lines → headings/sections\n",
    "        current_title = None\n",
    "        buf: List[str] = []\n",
    "        def flush():\n",
    "            nonlocal buf, current_title, sections\n",
    "            if current_title and buf:\n",
    "                text = \" \".join(buf)[:1000]\n",
    "                sections.append(Section(\n",
    "                    section_id=f\"sec:{len(sections)}\",\n",
    "                    page=p_idx, title=current_title.upper(),\n",
    "                    text_snippet=text\n",
    "                ))\n",
    "                buf = []\n",
    "        for ln in page.get(\"lines\", []) or []:\n",
    "            t = ln.get(\"layout\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ln.get(\"layout\", {}).get(\"content\", \"\")\n",
    "            if not t:\n",
    "                continue\n",
    "            if t.endswith(\":\") or (t.isupper() and len(t) <= 40):\n",
    "                flush()\n",
    "                current_title = t.strip().strip(\":\")\n",
    "                headers_by_page[p_idx].append(current_title)\n",
    "            else:\n",
    "                if current_title:\n",
    "                    buf.append(t)\n",
    "                    for m in DATE_REGEX.finditer(t or \"\"):\n",
    "                        entities.append(Entity(\n",
    "                            entity_id=f\"datei:{p_idx}:{len(entities)}\",\n",
    "                            page=p_idx, text=t, kind=\"date\",\n",
    "                            section_title=current_title.upper(),\n",
    "                            value_norm_date=norm_date(m.group(0))\n",
    "                        ))\n",
    "        flush()\n",
    "\n",
    "    # Deterministic context strings (no LLM)\n",
    "    doc_headers = list(dict.fromkeys([h for lst in headers_by_page.values() for h in lst]))\n",
    "    doc_ctx = \"[DOC] \" + \" | \".join(doc_headers)[:800]\n",
    "    page_ctx = {p: \"[PAGE] \" + \" | \".join(headers_by_page.get(p, []))[:400] for p in headers_by_page}\n",
    "    return sections, entities, doc_ctx, page_ctx\n",
    "\n",
    "# ========== Attach CUIs and context embeddings ==========\n",
    "def attach_cuis_and_context_embeddings(sections: List[Section], entities: List[Entity],\n",
    "                                       doc_ctx: str, page_ctx: Dict[int, str]) -> None:\n",
    "    texts, idx = [], []\n",
    "\n",
    "    # Sections\n",
    "    for s in sections:\n",
    "        cand = umls_link(f\"{s.title}: {s.text_snippet}\")\n",
    "        if cand:\n",
    "            best = max(cand, key=lambda x: x.get(\"score\", 0.0))\n",
    "            s.cui = best[\"cui\"]\n",
    "        if s.cui and s.cui in CUI_VECTORS:\n",
    "            s.emb = CUI_VECTORS[s.cui]\n",
    "        else:\n",
    "            t = f\"{doc_ctx}\\n{page_ctx.get(s.page,'')}\\n[SECTION] {s.title}\\n[TEXT] {s.text_snippet[:600]}\"\n",
    "            texts.append(t); idx.append((\"section\", s))\n",
    "\n",
    "    # Entities (dates/KVs)\n",
    "    for e in entities:\n",
    "        cand = umls_link(e.text)\n",
    "        if cand:\n",
    "            best = max(cand, key=lambda x: x.get(\"score\", 0.0))\n",
    "            e.cui = best[\"cui\"]\n",
    "        if e.cui and e.cui in CUI_VECTORS:\n",
    "            e.emb = CUI_VECTORS[e.cui]\n",
    "        else:\n",
    "            t = f\"{doc_ctx}\\n{page_ctx.get(e.page,'')}\\n[SECTION] {e.section_title or 'NA'}\\n[ENTITY:{e.kind}] {e.text[:400]}\"\n",
    "            texts.append(t); idx.append((\"entity\", e))\n",
    "\n",
    "    if texts:\n",
    "        embs = gemini_embed(texts)\n",
    "        for (kind, obj), vec in zip(idx, embs):\n",
    "            obj.emb = vec\n",
    "\n",
    "# ========== Similarity and scoring ==========\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    da = float(np.linalg.norm(a) + 1e-8); db = float(np.linalg.norm(b) + 1e-8)\n",
    "    return float(np.dot(a, b) / (da * db))\n",
    "\n",
    "def page_has_init_same_day(entities: List[Entity], day_iso: str, page: int) -> bool:\n",
    "    for e in entities:\n",
    "        if e.page == page and e.kind == \"date\" and e.value_norm_date == day_iso:\n",
    "            if \"initialization\" in (e.text or \"\").lower() or \"init\" in (e.text or \"\").lower():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def same_day_score(day_iso: Optional[str], section: Section, entities_all: List[Entity]) -> float:\n",
    "    if not day_iso:\n",
    "        return 0.0\n",
    "    for m in DATE_REGEX.finditer(section.text_snippet or \"\"):\n",
    "        if norm_date(m.group(0)) == day_iso:\n",
    "            return 1.0\n",
    "    return 0.6 if page_has_init_same_day(entities_all, day_iso, section.page) else 0.0\n",
    "\n",
    "def header_semantics(title: str) -> float:\n",
    "    t = (title or \"\").lower()\n",
    "    if \"procedure\" in t or \"operation\" in t:\n",
    "        return 1.0\n",
    "    if \"brief history\" in t or \"clinical summary\" in t or \"history\" in t:\n",
    "        return 0.8\n",
    "    return 0.3\n",
    "\n",
    "def association_score(date_ent: Entity, section: Section, day_iso: str, entities_all: List[Entity]) -> Tuple[float, Dict[str,float]]:\n",
    "    S_local = cosine(date_ent.emb, section.emb) if (date_ent.emb is not None and section.emb is not None) else 0.0\n",
    "    T_same = same_day_score(day_iso, section, entities_all)\n",
    "    H_sem  = header_semantics(section.title)\n",
    "    P_page = 0.2 if date_ent.page == section.page else 0.0\n",
    "    score = (0.35*S_local) + (0.30*T_same) + (0.25*H_sem) + (0.10*P_page)\n",
    "    return float(score), {\"S_local\": S_local, \"T_same\": T_same, \"H_sem\": H_sem, \"P_page\": P_page}\n",
    "\n",
    "# ========== Build model context ==========\n",
    "def build_model_context(query: str, day_iso: Optional[str], sections: List[Section], entities: List[Entity],\n",
    "                        top_n: int = 4) -> Tuple[str, List[Section], Dict[str,float]]:\n",
    "    query_emb = gemini_embed([query])[0]\n",
    "    # Rank sections by cosine to query embedding\n",
    "    ranked_secs = sorted(\n",
    "        [(cosine(query_emb, s.emb), s) for s in sections if s.emb is not None],\n",
    "        key=lambda x: x[0], reverse=True\n",
    "    )\n",
    "    selected_secs: List[Section] = []\n",
    "    score_breakdown = {}\n",
    "\n",
    "    # Include sections associated with date entities (like \"initialization\" or exact date matches)\n",
    "    for sscore, s in ranked_secs:\n",
    "        # Find date entities on the same page\n",
    "        date_ents = [e for e in entities if e.page == s.page and e.kind == \"date\" and e.value_norm_date == day_iso]\n",
    "        assoc_scores = [association_score(de, s, day_iso, entities) for de in date_ents]\n",
    "        if assoc_scores:\n",
    "            best_score, breakdown = max(assoc_scores, key=lambda x: x[0])\n",
    "            score_breakdown[s.section_id] = best_score\n",
    "            if best_score > 0.5 and len(selected_secs) < top_n:\n",
    "                selected_secs.append(s)\n",
    "        elif len(selected_secs) < top_n:\n",
    "            selected_secs.append(s)\n",
    "        if len(selected_secs) >= top_n:\n",
    "            break\n",
    "\n",
    "    # Build final context string\n",
    "    context_str = \"\\n---\\n\".join([f\"[SECTION] {s.title}\\n{s.text_snippet[:700]}\" for s in selected_secs])\n",
    "    return context_str, selected_secs, score_breakdown\n",
    "\n",
    "# ========== Query the pyramid ==========\n",
    "def query_context_cui(query: str, day_iso: Optional[str], sections: List[Section], entities: List[Entity],\n",
    "                      top_n: int = 4):\n",
    "    context_str, selected_secs, _ = build_model_context(query, day_iso, sections, entities, top_n)\n",
    "    # For demo, just print results; replace with your LLM prompt as needed\n",
    "    print(f\"Query: {query} | Date: {day_iso}\\n\")\n",
    "    print(\"Selected context sections:\")\n",
    "    for s in selected_secs:\n",
    "        print(f\" - {s.title} (Page {s.page}): {s.text_snippet[:150]}...\")\n",
    "    print(\"\\nFull context passed to LLM:\\n\", context_str)\n",
    "\n",
    "# ========== MAIN ==========\n",
    "def main():\n",
    "    # 1. Extract DocAI JSON from file\n",
    "    file_path = \"your_document.pdf\"  # Replace with your PDF path\n",
    "    print(f\"Extracting document via DocAI: {file_path} ...\")\n",
    "    docai_json = extract_docai_document(file_path)\n",
    "\n",
    "    # 2. Parse to pyramid\n",
    "    print(\"Parsing DocAI response to sections/entities ...\")\n",
    "    sections, entities, doc_ctx, page_ctx = parse_docai_to_pyramid(docai_json)\n",
    "\n",
    "    # 3. Attach embeddings and CUIs\n",
    "    print(\"Attaching CUIs and embeddings ...\")\n",
    "    attach_cuis_and_context_embeddings(sections, entities, doc_ctx, page_ctx)\n",
    "\n",
    "    # 4. Run query example\n",
    "    query_text = \"What procedure was done on 02/21/2022?\"\n",
    "    day_iso = \"2022-02-21\"  # Example normalized date\n",
    "\n",
    "    print(\"\\nQuerying context pyramid ...\")\n",
    "    query_context_cui(query_text, day_iso, sections, entities)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YixEt1mv-CDb"
   },
   "outputs": [],
   "source": [
    "#Use below code if we have already document output generated\n",
    "!pip install google-cloud-documentai\n",
    "!pip install google-cloud-aiplatform\n",
    "# or if you want the latest genai package\n",
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h-z_qoPaPAd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wm4RdOjr4T3c"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "PROJECT_ID = \"YOUR_GCP_PROJECT\"\n",
    "LOCATION = \"global\"\n",
    "BQ_CUI_TABLE = \"your_project.your_dataset.cui_embeddings\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig, TextGenerationConfig\n",
    "\n",
    "import pandas_gbq\n",
    "\n",
    "_client = genai.Client()\n",
    "\n",
    "# ----------------------------\n",
    "# Utility functions & classes\n",
    "# ----------------------------\n",
    "\n",
    "DATE_REGEX = re.compile(r\"\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})(?:[ ,;]+(\\d{1,2}[:.]\\d{2}))?\\b\")\n",
    "\n",
    "def norm_date(s: str) -> Optional[str]:\n",
    "    s = s.strip().replace('.', ':').replace(',', ' ')\n",
    "    try:\n",
    "        for fmt in (\"%m/%d/%Y\", \"%m/%d/%y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "            try:\n",
    "                return datetime.strptime(s.split()[0], fmt).strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        m = DATE_REGEX.search(s)\n",
    "        if m:\n",
    "            raw = m.group(1)\n",
    "            for fmt in (\"%m/%d/%Y\", \"%m/%d/%y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "                try:\n",
    "                    return datetime.strptime(raw, fmt).strftime(\"%Y-%m-%d\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "@dataclass\n",
    "class Section:\n",
    "    section_id: str\n",
    "    page: int\n",
    "    title: str\n",
    "    text_snippet: str\n",
    "    emb: Optional[np.ndarray] = None\n",
    "    cui: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    entity_id: str\n",
    "    page: int\n",
    "    text: str\n",
    "    kind: str\n",
    "    section_title: Optional[str]\n",
    "    value_norm_date: Optional[str]\n",
    "    emb: Optional[np.ndarray] = None\n",
    "    cui: Optional[str] = None\n",
    "\n",
    "# ----------------------------\n",
    "# Load CUI vectors from BigQuery\n",
    "# ----------------------------\n",
    "def load_cui_vectors_bq(table_fqn: str = BQ_CUI_TABLE) -> Dict[str, np.ndarray]:\n",
    "    df = pandas_gbq.read_gbq(f\"SELECT cui, embedding FROM `{table_fqn}`\", project_id=PROJECT_ID)\n",
    "    return {row[\"cui\"]: np.array(row[\"embedding\"], dtype=np.float32) for _, row in df.iterrows()}\n",
    "\n",
    "CUI_VECTORS = load_cui_vectors_bq(BQ_CUI_TABLE)\n",
    "\n",
    "# ----------------------------\n",
    "# Embeddings - Gemini embedding API\n",
    "# ----------------------------\n",
    "def gemini_embed(texts: List[str], *, task_type: str=\"RETRIEVAL_DOCUMENT\") -> np.ndarray:\n",
    "    \"\"\"Context-aware embeddings using Vertex AI Gemini.\"\"\"\n",
    "    cfg = EmbedContentConfig(task_type=task_type)\n",
    "    resp = _client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=texts,\n",
    "        config=cfg,\n",
    "    )\n",
    "    return np.array([e.values for e in resp.embeddings], dtype=np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# Stub: UMLS linker - Replace with your own\n",
    "# ----------------------------\n",
    "def umls_link(text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return list of {cui, score} linking text to CUIs.\"\"\"\n",
    "    # For now, empty. Implement your UMLS/CUI linking here.\n",
    "    return []\n",
    "\n",
    "# ----------------------------\n",
    "# Parse DocAI response into sections and entities\n",
    "# ----------------------------\n",
    "def parse_docai_to_pyramid(docai: Dict):\n",
    "    pages = docai.get(\"document\", {}).get(\"pages\", [])\n",
    "    sections: List[Section] = []\n",
    "    entities: List[Entity] = []\n",
    "    headers_by_page: Dict[int, List[str]] = {}\n",
    "\n",
    "    for p_idx, page in enumerate(pages):\n",
    "        headers_by_page[p_idx] = []\n",
    "        # Key-Value pairs as entities\n",
    "        for ff in page.get(\"formFields\", []) or []:\n",
    "            name = ff.get(\"fieldName\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ff.get(\"fieldName\", {}).get(\"content\", \"\")\n",
    "            value = ff.get(\"fieldValue\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ff.get(\"fieldValue\", {}).get(\"content\", \"\")\n",
    "            kv_text = f\"{name.strip()}: {value.strip()}\".strip(\": \")\n",
    "            entities.append(Entity(\n",
    "                entity_id=f\"kv:{p_idx}:{len(entities)}\",\n",
    "                page=p_idx, text=kv_text, kind=\"kv\",\n",
    "                section_title=None, value_norm_date=norm_date(kv_text)\n",
    "            ))\n",
    "            if DATE_REGEX.search(kv_text or \"\"):\n",
    "                entities.append(Entity(\n",
    "                    entity_id=f\"date:{p_idx}:{len(entities)}\",\n",
    "                    page=p_idx, text=kv_text, kind=\"date\",\n",
    "                    section_title=None, value_norm_date=norm_date(kv_text)\n",
    "                ))\n",
    "        # Lines → headings/sections\n",
    "        current_title = None\n",
    "        buf: List[str] = []\n",
    "        def flush():\n",
    "            nonlocal buf, current_title, sections\n",
    "            if current_title and buf:\n",
    "                text = \" \".join(buf)[:1000]\n",
    "                sections.append(Section(\n",
    "                    section_id=f\"sec:{len(sections)}\",\n",
    "                    page=p_idx, title=current_title.upper(),\n",
    "                    text_snippet=text\n",
    "                ))\n",
    "                buf = []\n",
    "        for ln in page.get(\"lines\", []) or []:\n",
    "            t = ln.get(\"layout\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ln.get(\"layout\", {}).get(\"content\", \"\")\n",
    "            if not t:\n",
    "                continue\n",
    "            if t.endswith(\":\") or (t.isupper() and len(t) <= 40):\n",
    "                flush()\n",
    "                current_title = t.strip().strip(\":\")\n",
    "                headers_by_page[p_idx].append(current_title)\n",
    "            else:\n",
    "                if current_title:\n",
    "                    buf.append(t)\n",
    "                    for m in DATE_REGEX.finditer(t or \"\"):\n",
    "                        entities.append(Entity(\n",
    "                            entity_id=f\"datei:{p_idx}:{len(entities)}\",\n",
    "                            page=p_idx, text=t, kind=\"date\",\n",
    "                            section_title=current_title.upper(),\n",
    "                            value_norm_date=norm_date(m.group(0))\n",
    "                        ))\n",
    "        flush()\n",
    "\n",
    "    # Deterministic context strings (no LLM)\n",
    "    doc_headers = list(dict.fromkeys([h for lst in headers_by_page.values() for h in lst]))\n",
    "    doc_ctx = \"[DOC] \" + \" | \".join(doc_headers)[:800]\n",
    "    page_ctx = {p: \"[PAGE] \" + \" | \".join(headers_by_page.get(p, []))[:400] for p in headers_by_page}\n",
    "    return sections, entities, doc_ctx, page_ctx\n",
    "\n",
    "# ----------------------------\n",
    "# Attach CUIs and embeddings\n",
    "# ----------------------------\n",
    "def attach_cuis_and_context_embeddings(sections: List[Section], entities: List[Entity],\n",
    "                                       doc_ctx: str, page_ctx: Dict[int, str]) -> None:\n",
    "    texts, idx = [], []\n",
    "\n",
    "    # Sections\n",
    "    for s in sections:\n",
    "        cand = umls_link(f\"{s.title}: {s.text_snippet}\")\n",
    "        if cand:\n",
    "            best = max(cand, key=lambda x: x.get(\"score\", 0.0))\n",
    "            s.cui = best[\"cui\"]\n",
    "        if s.cui and s.cui in CUI_VECTORS:\n",
    "            s.emb = CUI_VECTORS[s.cui]\n",
    "        else:\n",
    "            t = f\"{doc_ctx}\\n{page_ctx.get(s.page,'')}\\n[SECTION] {s.title}\\n[TEXT] {s.text_snippet[:600]}\"\n",
    "            texts.append(t); idx.append((\"section\", s))\n",
    "\n",
    "    # Entities (dates/KVs)\n",
    "    for e in entities:\n",
    "        cand = umls_link(e.text)\n",
    "        if cand:\n",
    "            best = max(cand, key=lambda x: x.get(\"score\", 0.0))\n",
    "            e.cui = best[\"cui\"]\n",
    "        if e.cui and e.cui in CUI_VECTORS:\n",
    "            e.emb = CUI_VECTORS[e.cui]\n",
    "        else:\n",
    "            t = f\"{doc_ctx}\\n{page_ctx.get(e.page,'')}\\n[SECTION] {e.section_title or 'NA'}\\n[ENTITY:{e.kind}] {e.text[:400]}\"\n",
    "            texts.append(t); idx.append((\"entity\", e))\n",
    "\n",
    "    if texts:\n",
    "        embs = gemini_embed(texts)\n",
    "        for (kind, obj), vec in zip(idx, embs):\n",
    "            obj.emb = vec\n",
    "\n",
    "# ----------------------------\n",
    "# Cosine similarity\n",
    "# ----------------------------\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    da = float(np.linalg.norm(a) + 1e-8); db = float(np.linalg.norm(b) + 1e-8)\n",
    "    return float(np.dot(a, b) / (da * db))\n",
    "\n",
    "# ----------------------------\n",
    "# Build section similarity graph (in-memory)\n",
    "# ----------------------------\n",
    "def build_similarity_graph(sections: List[Section]) -> Dict[Tuple[str, str], float]:\n",
    "    \"\"\"\n",
    "    Build a similarity graph where edges are cosine similarity > threshold\n",
    "    Returns dict of edge tuples and similarity score\n",
    "    \"\"\"\n",
    "    sec_with_emb = [s for s in sections if s.emb is not None]\n",
    "    S = {}\n",
    "    threshold = 0.75  # similarity threshold, tune as needed\n",
    "\n",
    "    for i in range(len(sec_with_emb)):\n",
    "        for j in range(i + 1, len(sec_with_emb)):\n",
    "            a, b = sec_with_emb[i], sec_with_emb[j]\n",
    "            sim = cosine(a.emb, b.emb)\n",
    "            if sim > threshold:\n",
    "                S[(a.section_id, b.section_id)] = sim\n",
    "                S[(b.section_id, a.section_id)] = sim\n",
    "    return S\n",
    "\n",
    "# ----------------------------\n",
    "# Query embedding and search\n",
    "# ----------------------------\n",
    "def query_similar_sections(query: str, sections: List[Section], top_k=5) -> List[Tuple[Section, float]]:\n",
    "    \"\"\"\n",
    "    Embed the query, then find top_k most similar sections by cosine similarity\n",
    "    \"\"\"\n",
    "    q_emb = gemini_embed([query])[0]\n",
    "    scored = []\n",
    "    for s in sections:\n",
    "        if s.emb is not None:\n",
    "            sim = cosine(q_emb, s.emb)\n",
    "            scored.append((s, sim))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_k]\n",
    "\n",
    "# ----------------------------\n",
    "# Generate answer with LLM\n",
    "# ----------------------------\n",
    "def generate_answer_llm(query: str, relevant_sections: List[Section]) -> str:\n",
    "    \"\"\"\n",
    "    Compose prompt from query and relevant context, then generate answer via Vertex AI Gemini LLM\n",
    "    \"\"\"\n",
    "    context_text = \"\\n---\\n\".join(f\"[{s.title}]\\n{s.text_snippet}\" for s in relevant_sections)\n",
    "    prompt = f\"Using the following context from the document, answer the query:\\n\\nContext:\\n{context_text}\\n\\nQuery:\\n{query}\\n\\nAnswer:\"\n",
    "\n",
    "    resp = _client.predict(\n",
    "        model=\"gemini-002\",\n",
    "        parameters=TextGenerationConfig(max_tokens=256, temperature=0.0),\n",
    "        prompt=prompt,\n",
    "    )\n",
    "    return resp.generations[0].text.strip()\n",
    "\n",
    "# ----------------------------\n",
    "# Example main flow\n",
    "# ----------------------------\n",
    "def main(docai_response: Dict, user_query: str) -> str:\n",
    "    # Parse DocAI output\n",
    "    sections, entities, doc_ctx, page_ctx = parse_docai_to_pyramid(docai_response)\n",
    "\n",
    "    # Attach CUIs and embeddings\n",
    "    attach_cuis_and_context_embeddings(sections, entities, doc_ctx, page_ctx)\n",
    "\n",
    "    # Build similarity graph (optional, for exploration)\n",
    "    graph = build_similarity_graph(sections)\n",
    "\n",
    "    # Find relevant sections for the user query\n",
    "    relevant_sections = [s for s, score in query_similar_sections(user_query, sections)]\n",
    "\n",
    "    # Generate an LLM answer\n",
    "    answer = generate_answer_llm(user_query, relevant_sections)\n",
    "    return answer\n",
    "\n",
    "# ----------------------------\n",
    "# If you want to test:\n",
    "----------------------------\n",
    "docai_resp = ... # Load your DocAI JSON output here\n",
    "query = \"diagnosis of diabetes\"\n",
    "print(main(docai_resp, query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w76nazmA73Vi"
   },
   "outputs": [],
   "source": [
    "#use this code for complete pipelinbe inlcuding LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sdaykyO7rQd"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Google imports\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig, TextGenerationConfig\n",
    "import pandas_gbq\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG (fill in your own)\n",
    "# ----------------------------\n",
    "PROJECT_ID = \"YOUR_GCP_PROJECT\"\n",
    "LOCATION = \"us\"  # or your processor location\n",
    "PROCESSOR_ID = \"YOUR_PROCESSOR_ID\"\n",
    "GCS_INPUT_URI = \"gs://your-bucket/your-file.pdf\"\n",
    "BQ_CUI_TABLE = \"your_project.your_dataset.cui_embeddings\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
    "\n",
    "# Clients\n",
    "docai_client = documentai.DocumentProcessorServiceClient()\n",
    "genai_client = genai.Client()\n",
    "\n",
    "# ----------------------------\n",
    "# 1. DocAI Extraction (from first script)\n",
    "# ----------------------------\n",
    "\n",
    "def get_text(text_anchor, document_text) -> str:\n",
    "    if not text_anchor or not text_anchor.text_segments:\n",
    "        return \"\"\n",
    "    text = \"\"\n",
    "    for segment in text_anchor.text_segments:\n",
    "        start = segment.start_index or 0\n",
    "        end = segment.end_index or 0\n",
    "        text += document_text[start:end]\n",
    "    return text.strip()\n",
    "\n",
    "def run_docai_extraction() -> Dict:\n",
    "    name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/processors/{PROCESSOR_ID}\"\n",
    "    gcs_document = documentai.GcsDocument(gcs_uri=GCS_INPUT_URI, mime_type=\"application/pdf\")\n",
    "    input_config = documentai.BatchDocumentsInputConfig(gcs_documents=documentai.GcsDocuments(documents=[gcs_document]))\n",
    "    request = documentai.ProcessRequest(name=name, input_documents=input_config)\n",
    "    result = docai_client.process_document(request=request)\n",
    "    document = result.document\n",
    "\n",
    "    output_data = {\n",
    "        \"text\": document.text,\n",
    "        \"document\": {\n",
    "            \"pages\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for page in document.pages:\n",
    "        page_text = get_text(page.layout.text_anchor, document.text)\n",
    "        page_data = {\n",
    "            \"pageNumber\": page.page_number,\n",
    "            \"formFields\": [],\n",
    "            \"lines\": [],\n",
    "        }\n",
    "        # formFields\n",
    "        for field in page.form_fields:\n",
    "            field_name = get_text(field.field_name.text_anchor, document.text)\n",
    "            field_value = get_text(field.field_value.text_anchor, document.text)\n",
    "            page_data[\"formFields\"].append({\n",
    "                \"fieldName\": {\"textAnchor\": {\"content\": field_name}},\n",
    "                \"fieldValue\": {\"textAnchor\": {\"content\": field_value}},\n",
    "            })\n",
    "        # lines (for parser)\n",
    "        for line in page.lines:\n",
    "            content = get_text(line.layout.text_anchor, document.text)\n",
    "            page_data[\"lines\"].append({\n",
    "                \"layout\": {\"textAnchor\": {\"content\": content}},\n",
    "                \"text\": content\n",
    "            })\n",
    "        output_data[\"document\"][\"pages\"].append(page_data)\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Everything else from second script (unchanged)\n",
    "# ----------------------------\n",
    "\n",
    "DATE_REGEX = re.compile(r\"\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})(?:[ ,;]+(\\d{1,2}[:.]\\d{2}))?\\b\")\n",
    "\n",
    "def norm_date(s: str) -> Optional[str]:\n",
    "    s = s.strip().replace('.', ':').replace(',', ' ')\n",
    "    try:\n",
    "        for fmt in (\"%m/%d/%Y\", \"%m/%d/%y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "            try:\n",
    "                return datetime.strptime(s.split()[0], fmt).strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        m = DATE_REGEX.search(s)\n",
    "        if m:\n",
    "            raw = m.group(1)\n",
    "            for fmt in (\"%m/%d/%Y\", \"%m/%d/%y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "                try:\n",
    "                    return datetime.strptime(raw, fmt).strftime(\"%Y-%m-%d\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "@dataclass\n",
    "class Section:\n",
    "    section_id: str\n",
    "    page: int\n",
    "    title: str\n",
    "    text_snippet: str\n",
    "    emb: Optional[np.ndarray] = None\n",
    "    cui: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    entity_id: str\n",
    "    page: int\n",
    "    text: str\n",
    "    kind: str\n",
    "    section_title: Optional[str]\n",
    "    value_norm_date: Optional[str]\n",
    "    emb: Optional[np.ndarray] = None\n",
    "    cui: Optional[str] = None\n",
    "\n",
    "def load_cui_vectors_bq(table_fqn: str = BQ_CUI_TABLE) -> Dict[str, np.ndarray]:\n",
    "    df = pandas_gbq.read_gbq(f\"SELECT cui, embedding FROM `{table_fqn}`\", project_id=PROJECT_ID)\n",
    "    return {row[\"cui\"]: np.array(row[\"embedding\"], dtype=np.float32) for _, row in df.iterrows()}\n",
    "\n",
    "CUI_VECTORS = load_cui_vectors_bq(BQ_CUI_TABLE)\n",
    "\n",
    "def gemini_embed(texts: List[str], *, task_type: str = \"RETRIEVAL_DOCUMENT\") -> np.ndarray:\n",
    "    cfg = EmbedContentConfig(task_type=task_type)\n",
    "    resp = genai_client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=texts,\n",
    "        config=cfg,\n",
    "    )\n",
    "    return np.array([e.values for e in resp.embeddings], dtype=np.float32)\n",
    "\n",
    "def umls_link(text: str) -> List[Dict[str, Any]]:\n",
    "    # Stub: implement your own UMLS linking here\n",
    "    return []\n",
    "\n",
    "def parse_docai_to_pyramid(docai: Dict):\n",
    "    pages = docai.get(\"document\", {}).get(\"pages\", [])\n",
    "    sections: List[Section] = []\n",
    "    entities: List[Entity] = []\n",
    "    headers_by_page: Dict[int, List[str]] = {}\n",
    "\n",
    "    for p_idx, page in enumerate(pages):\n",
    "        headers_by_page[p_idx] = []\n",
    "        for ff in page.get(\"formFields\", []) or []:\n",
    "            name = ff.get(\"fieldName\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ff.get(\"fieldName\", {}).get(\"content\", \"\")\n",
    "            value = ff.get(\"fieldValue\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ff.get(\"fieldValue\", {}).get(\"content\", \"\")\n",
    "            kv_text = f\"{name.strip()}: {value.strip()}\".strip(\": \")\n",
    "            entities.append(Entity(\n",
    "                entity_id=f\"kv:{p_idx}:{len(entities)}\",\n",
    "                page=p_idx, text=kv_text, kind=\"kv\",\n",
    "                section_title=None, value_norm_date=norm_date(kv_text)\n",
    "            ))\n",
    "            if DATE_REGEX.search(kv_text or \"\"):\n",
    "                entities.append(Entity(\n",
    "                    entity_id=f\"date:{p_idx}:{len(entities)}\",\n",
    "                    page=p_idx, text=kv_text, kind=\"date\",\n",
    "                    section_title=None, value_norm_date=norm_date(kv_text)\n",
    "                ))\n",
    "        current_title = None\n",
    "        buf: List[str] = []\n",
    "        def flush():\n",
    "            nonlocal buf, current_title, sections\n",
    "            if current_title and buf:\n",
    "                text = \" \".join(buf)[:1000]\n",
    "                sections.append(Section(\n",
    "                    section_id=f\"sec:{len(sections)}\",\n",
    "                    page=p_idx, title=current_title.upper(),\n",
    "                    text_snippet=text\n",
    "                ))\n",
    "                buf = []\n",
    "        for ln in page.get(\"lines\", []) or []:\n",
    "            t = ln.get(\"layout\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ln.get(\"layout\", {}).get(\"content\", \"\")\n",
    "            if not t:\n",
    "                continue\n",
    "            if t.endswith(\":\") or (t.isupper() and len(t) <= 40):\n",
    "                flush()\n",
    "                current_title = t.strip().strip(\":\")\n",
    "                headers_by_page[p_idx].append(current_title)\n",
    "            else:\n",
    "                if current_title:\n",
    "                    buf.append(t)\n",
    "                    for m in DATE_REGEX.finditer(t or \"\"):\n",
    "                        entities.append(Entity(\n",
    "                            entity_id=f\"datei:{p_idx}:{len(entities)}\",\n",
    "                            page=p_idx, text=t, kind=\"date\",\n",
    "                            section_title=current_title.upper(),\n",
    "                            value_norm_date=norm_date(m.group(0))\n",
    "                        ))\n",
    "        flush()\n",
    "\n",
    "    doc_headers = list(dict.fromkeys([h for lst in headers_by_page.values() for h in lst]))\n",
    "    doc_ctx = \"[DOC] \" + \" | \".join(doc_headers)[:800]\n",
    "    page_ctx = {p: \"[PAGE] \" + \" | \".join(headers_by_page.get(p, []))[:400] for p in headers_by_page}\n",
    "    return sections, entities, doc_ctx, page_ctx\n",
    "\n",
    "def attach_cuis_and_context_embeddings(sections: List[Section], entities: List[Entity],\n",
    "                                       doc_ctx: str, page_ctx: Dict[int, str]) -> None:\n",
    "    texts, idx = [], []\n",
    "\n",
    "    for s in sections:\n",
    "        cand = umls_link(f\"{s.title}: {s.text_snippet}\")\n",
    "        if cand:\n",
    "            best = max(cand, key=lambda x: x.get(\"score\", 0.0))\n",
    "            s.cui = best[\"cui\"]\n",
    "        if s.cui and s.cui in CUI_VECTORS:\n",
    "            s.emb = CUI_VECTORS[s.cui]\n",
    "        else:\n",
    "            t = f\"{doc_ctx}\\n{page_ctx.get(s.page,'')}\\n[SECTION] {s.title}\\n[TEXT] {s.text_snippet[:600]}\"\n",
    "            texts.append(t); idx.append((\"section\", s))\n",
    "\n",
    "    for e in entities:\n",
    "        cand = umls_link(e.text)\n",
    "        if cand:\n",
    "            best = max(cand, key=lambda x: x.get(\"score\", 0.0))\n",
    "            e.cui = best[\"cui\"]\n",
    "        if e.cui and e.cui in CUI_VECTORS:\n",
    "            e.emb = CUI_VECTORS[e.cui]\n",
    "        else:\n",
    "            t = f\"{doc_ctx}\\n{page_ctx.get(e.page,'')}\\n[SECTION] {e.section_title or 'NA'}\\n[ENTITY:{e.kind}] {e.text[:400]}\"\n",
    "            texts.append(t); idx.append((\"entity\", e))\n",
    "\n",
    "    if texts:\n",
    "        embs = gemini_embed(texts)\n",
    "        for (kind, obj), vec in zip(idx, embs):\n",
    "            obj.emb = vec\n",
    "\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    da = float(np.linalg.norm(a) + 1e-8); db = float(np.linalg.norm(b) + 1e-8)\n",
    "    return float(np.dot(a, b) / (da * db))\n",
    "\n",
    "def build_similarity_graph(sections: List[Section]) -> Dict[Tuple[str, str], float]:\n",
    "    sec_with_emb = [s for s in sections if s.emb is not None]\n",
    "    S = {}\n",
    "    threshold = 0.75\n",
    "    for i in range(len(sec_with_emb)):\n",
    "        for j in range(i + 1, len(sec_with_emb)):\n",
    "            a, b = sec_with_emb[i], sec_with_emb[j]\n",
    "            sim = cosine(a.emb, b.emb)\n",
    "            if sim > threshold:\n",
    "                S[(a.section_id, b.section_id)] = sim\n",
    "                S[(b.section_id, a.section_id)] = sim\n",
    "    return S\n",
    "\n",
    "def query_similar_sections(query: str, sections: List[Section], top_k=5) -> List[Tuple[Section, float]]:\n",
    "    q_emb = gemini_embed([query])[0]\n",
    "    scored = []\n",
    "    for s in sections:\n",
    "        if s.emb is not None:\n",
    "            sim = cosine(q_emb, s.emb)\n",
    "            scored.append((s, sim))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_k]\n",
    "\n",
    "def generate_answer_llm(query: str, relevant_sections: List[Section]) -> str:\n",
    "    context_text = \"\\n---\\n\".join(f\"[{s.title}]\\n{s.text_snippet}\" for s in relevant_sections)\n",
    "    prompt = f\"Using the following context from the document, answer the query:\\n\\nContext:\\n{context_text}\\n\\nQuery:\\n{query}\\n\\nAnswer:\"\n",
    "    resp = genai_client.predict(\n",
    "        model=\"gemini-002\",\n",
    "        parameters=TextGenerationConfig(max_tokens=256, temperature=0.0),\n",
    "        prompt=prompt\n",
    "    )\n",
    "    return resp.text\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN RUN\n",
    "# ----------------------------\n",
    "\n",
    "def main():\n",
    "    # 1. Extract with DocAI\n",
    "    print(\"Running DocAI extraction...\")\n",
    "    docai_json = run_docai_extraction()\n",
    "\n",
    "    # 2. Parse to sections/entities + context\n",
    "    print(\"Parsing extracted document...\")\n",
    "    sections, entities, doc_ctx, page_ctx = parse_docai_to_pyramid(docai_json)\n",
    "\n",
    "    # 3. Attach embeddings + CUI\n",
    "    print(\"Attaching embeddings and CUIs...\")\n",
    "    attach_cuis_and_context_embeddings(sections, entities, doc_ctx, page_ctx)\n",
    "\n",
    "    # 4. Build similarity graph (optional)\n",
    "    print(\"Building similarity graph...\")\n",
    "    sim_graph = build_similarity_graph(sections)\n",
    "\n",
    "    # 5. Query and generate answers example\n",
    "    query = \"What is the patient's diagnosis?\"\n",
    "    print(f\"Querying: {query}\")\n",
    "    relevant = query_similar_sections(query, sections, top_k=5)\n",
    "    answer = generate_answer_llm(query, [s for s, _ in relevant])\n",
    "\n",
    "    print(\"Answer:\", answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
