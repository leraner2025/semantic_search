!pip install faiss-cpu sentence-transformers bertopic hdbscan umap-learn gensim scikit-learn nltk
# Download NLTK tokenizer data
import nltk
nltk.download("punkt")

import os, random, numpy as np, torch, nltk, logging, re
from collections import defaultdict
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from hdbscan import HDBSCAN
from umap import UMAP
import faiss
from sklearn.metrics.pairwise import cosine_similarity

# === SETUP ===
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.manual_seed_all(SEED)
torch.use_deterministic_algorithms(True)
nltk.download("punkt")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === CLEANING ===
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    return re.sub(r"\s+", " ", text).strip()

# === CONTEXT EXTRACTION ===
def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):
    entity_context_pairs = []
    for idx, ents in enumerate(entities_per_chunk):
        chunk = clean_text(chunks[idx])
        sentences = sent_tokenize(chunk)
        for ent in ents:
            ent_lower = ent.lower()
            matched = False
            for i, sent in enumerate(sentences):
                if ent_lower in sent:
                    context = " ".join(sentences[max(0, i - 1): i + 2]) if use_multi_sentence else sent.strip()
                    enriched = f"The concept '{ent_lower}' appears in the following context: {context}"
                    entity_context_pairs.append((ent_lower, enriched.strip()))
                    matched = True
                    break
            if not matched:
                fallback = f"The concept '{ent_lower}' appears in the following context: {chunk}"
                entity_context_pairs.append((ent_lower, fallback.strip()))
    return entity_context_pairs

# === TOPIC ADAPTER CLASS ===
class TopicAdapter:
    def __init__(self, chunks, entities_per_chunk, model_name="pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"):
        self.chunks = chunks
        self.entities_per_chunk = entities_per_chunk
        self.embedding_model = SentenceTransformer(model_name)
        self.topic_metadata = []
        self.topic_embeddings = None
        self.search_index = None
        self._build_topics()

    def _build_topics(self):
        # === Manually defined best parameters ===
        umap_model = UMAP(n_neighbors=5, n_components=3, min_dist=0.1, metric="cosine", random_state=SEED)
        hdbscan_model = HDBSCAN(min_cluster_size=2, min_samples=1, metric="euclidean", prediction_data=True)

        # === Extract contexts and embeddings ===
        entity_context_pairs = extract_entity_contexts(self.chunks, self.entities_per_chunk)
        contextual_texts = [ctx for _, ctx in entity_context_pairs]
        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=False)

        # === Train BERTopic ===
        topic_model = BERTopic(
            embedding_model=self.embedding_model,
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            representation_model=KeyBERTInspired(),
            calculate_probabilities=True,
            verbose=False,
        )

        topics, _ = topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)

        topic_to_entities = defaultdict(set)
        topic_to_embeddings = defaultdict(list)

        for i, topic in enumerate(topics):
            if topic == -1:
                continue
            ent, _ = entity_context_pairs[i]
            topic_to_entities[topic].add(ent)
            topic_to_embeddings[topic].append(contextual_embeddings[i])

        topic_metadata = []
        topic_embeddings = []

        for topic_id in topic_to_entities:
            emb = topic_to_embeddings[topic_id]
            centroid = np.mean(emb, axis=0)
            centroid /= np.linalg.norm(centroid) + 1e-10
            topic_embeddings.append(centroid)
            topic_metadata.append({
                "topic_id": topic_id,
                "entities": list(topic_to_entities[topic_id]),
                "embeddings": np.array(emb)
            })

        self.topic_metadata = topic_metadata
        self.topic_embeddings = np.array(topic_embeddings).astype(np.float32)

        # === FAISS Index Setup ===
        dim = self.topic_embeddings.shape[1]
        self.search_index = faiss.IndexFlatIP(dim)
        self.search_index.add(self.topic_embeddings)

    def match_query_to_topics(self, query, top_k=3):
        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]
        query_emb = query_emb.astype(np.float32).reshape(1, -1)
        scores, indices = self.search_index.search(query_emb, top_k)

        results = []
        for i, idx in enumerate(indices[0]):
            meta = self.topic_metadata[idx]
            results.append({
                "topic_id": meta["topic_id"],
                "score": float(scores[0][i]),
                "entities": meta["entities"]
            })
        return results


# === Sample Data ===
allergy_dataset = {
    "chunks": [
        "Patient was discharged with carvedilol, lisinopril, and warfarin.",
        "He has a history of diabetes and uses insulin.",
        "Social work arranged transport and medication delivery."
    ],
    "entities": [
        ["carvedilol", "lisinopril", "warfarin"],
        ["diabetes", "insulin"],
        ["transport", "medication", "delivery"]
    ]
}

# === Initialize Adapter ===
adapter = TopicAdapter(
    chunks=allergy_dataset["chunks"],
    entities_per_chunk=allergy_dataset["entities"]
)

# === Query Example ===
query = "What medications was the patient discharged with?"
topics = adapter.match_query_to_topics(query)

# === Output ===
for topic in topics:
    print(f"\nTopic ID: {topic['topic_id']}")
    print(f"Score: {topic['score']:.4f}")
    print(f"Entities: {topic['entities']}")

#Adding grid search cv inside the class
import os, random, numpy as np, torch, nltk, logging, re
from collections import defaultdict
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from hdbscan import HDBSCAN
from umap import UMAP
import faiss
from sklearn.metrics import silhouette_score
from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora import Dictionary

# === SETUP ===
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.manual_seed_all(SEED)
torch.use_deterministic_algorithms(True)
nltk.download("punkt")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === CLEANING ===
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    return re.sub(r"\s+", " ", text).strip()

# === CONTEXT EXTRACTION ===
def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):
    entity_context_pairs = []
    for idx, ents in enumerate(entities_per_chunk):
        chunk = clean_text(chunks[idx])
        sentences = sent_tokenize(chunk)
        for ent in ents:
            ent_lower = ent.lower()
            matched = False
            for i, sent in enumerate(sentences):
                if ent_lower in sent:
                    context = " ".join(sentences[max(0, i - 1): i + 2]) if use_multi_sentence else sent.strip()
                    enriched = f"The concept '{ent_lower}' appears in the following context: {context}"
                    entity_context_pairs.append((ent_lower, enriched.strip()))
                    matched = True
                    break
            if not matched:
                fallback = f"The concept '{ent_lower}' appears in the following context: {chunk}"
                entity_context_pairs.append((ent_lower, fallback.strip()))
    return entity_context_pairs

# === TOPIC ADAPTER CLASS ===
class TopicAdapter:
    def __init__(self, chunks, entities_per_chunk, model_name="pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"):
        self.chunks = chunks
        self.entities_per_chunk = entities_per_chunk
        self.embedding_model = SentenceTransformer(model_name)
        self.topic_metadata = []
        self.topic_embeddings = None
        self.search_index = None
        self._run_grid_search_and_build()

    def _run_grid_search_and_build(self):
        umap_grid = {
            "n_neighbors": [3, 5, 8],
            "n_components": [3, 5],
            "min_dist": [0.0, 0.1, 0.3]
        }
        hdbscan_grid = {
            "min_cluster_size": [2, 5],
            "min_samples": [1, 3]
        }

        combinations = [(u, c, d, s, m) for u in umap_grid["n_neighbors"]
                        for c in umap_grid["n_components"]
                        for d in umap_grid["min_dist"]
                        for s in hdbscan_grid["min_cluster_size"]
                        for m in hdbscan_grid["min_samples"]]

        best_score = -np.inf
        best_metadata = None
        best_embeddings = None

        entity_context_pairs = extract_entity_contexts(self.chunks, self.entities_per_chunk)
        contextual_texts = [ctx for _, ctx in entity_context_pairs]
        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=False)

        for (u, c, d, s, m) in combinations:
            try:
                umap_model = UMAP(n_neighbors=u, n_components=c, min_dist=d, metric="cosine", random_state=SEED)
                hdbscan_model = HDBSCAN(min_cluster_size=s, min_samples=m, metric="euclidean", prediction_data=True)

                topic_model = BERTopic(
                    embedding_model=self.embedding_model,
                    umap_model=umap_model,
                    hdbscan_model=hdbscan_model,
                    representation_model=KeyBERTInspired(),
                    calculate_probabilities=True,
                    verbose=False,
                )

                topics, _ = topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)

                topic_to_entities = defaultdict(set)
                topic_to_embeddings = defaultdict(list)

                for i, topic in enumerate(topics):
                    if topic == -1:
                        continue
                    ent, _ = entity_context_pairs[i]
                    topic_to_entities[topic].add(ent)
                    topic_to_embeddings[topic].append(contextual_embeddings[i])

                topic_metadata = []
                topic_embeddings = []

                for topic_id in topic_to_entities:
                    emb = topic_to_embeddings[topic_id]
                    if len(emb) == 0:
                        continue
                    centroid = np.mean(emb, axis=0)
                    centroid /= np.linalg.norm(centroid) + 1e-10
                    topic_embeddings.append(centroid)
                    topic_metadata.append({
                        "topic_id": topic_id,
                        "entities": list(topic_to_entities[topic_id]),
                        "embeddings": np.array(emb)
                    })

                if len(topic_metadata) < 2:
                    continue

                texts = [clean_text(ctx).split() for ctx in contextual_texts]
                dictionary = Dictionary(texts)
                corpus = [dictionary.doc2bow(t) for t in texts]
                word_lists = []
                for meta in topic_metadata:
                    topic_words = topic_model.get_topic(meta["topic_id"])
                    if topic_words:
                        word_lists.append([word for word, _ in topic_words[:10]])

                if not word_lists:
                    continue

                cm = CoherenceModel(topics=word_lists, texts=texts, dictionary=dictionary, coherence="c_v")
                coherence = cm.get_coherence()

                unique_words = set(word for topic in word_lists for word in topic)
                diversity = len(unique_words) / (len(word_lists) * 10)

                all_embeddings = []
                all_labels = []
                for meta in topic_metadata:
                    emb = meta["embeddings"]
                    if len(emb) < 2:
                        continue
                    all_embeddings.extend(emb)
                    all_labels.extend([meta["topic_id"]] * len(emb))
                silhouette = 0.0
                if len(all_embeddings) >= 3:
                    silhouette = silhouette_score(np.vstack(all_embeddings), all_labels, metric="cosine")

                score = coherence + 0.5 * diversity + 0.5 * silhouette

                if score > best_score:
                    best_score = score
                    best_metadata = topic_metadata
                    best_embeddings = topic_embeddings

            except Exception as e:
                logger.warning(f"Grid search failed for combination: {e}")
                continue

        # Deduplicate topic IDs
        unique_metadata = {}
        for meta in best_metadata:
            tid = meta["topic_id"]
            if tid not in unique_metadata:
                unique_metadata[tid] = meta
        self.topic_metadata = list(unique_metadata.values())

        self.topic_embeddings = np.array([
            np.mean(m["embeddings"], axis=0) /
            (np.linalg.norm(np.mean(m["embeddings"], axis=0)) + 1e-10)
            for m in self.topic_metadata
        ]).astype(np.float32)

        # === FAISS Index Setup ===
        dim = self.topic_embeddings.shape[1]
        self.search_index = faiss.IndexFlatIP(dim)
        self.search_index.add(self.topic_embeddings)

    def match_query_to_topics(self, query, top_k=3):
        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]
        query_emb /= np.linalg.norm(query_emb) + 1e-10
        query_emb = query_emb.astype(np.float32).reshape(1, -1)
        scores, indices = self.search_index.search(query_emb, top_k)

        results = []
        for i, idx in enumerate(indices[0]):
            score = float(scores[0][i])
            if score < -1e+10:  # Filter invalid scores
                continue
            meta = self.topic_metadata[idx]
            results.append({
                "topic_id": meta["topic_id"],
                "score": score,
                "entities": meta["entities"]
            })
        return results

      # === Sample Data ===
allergy_dataset = {
    "chunks": [
        "Patient was discharged with carvedilol, lisinopril, and warfarin.",
        "He has a history of diabetes and uses insulin.",
        "Social work arranged transport and medication delivery."
    ],
    "entities": [
        ["carvedilol", "lisinopril", "warfarin"],
        ["diabetes", "insulin"],
        ["transport", "medication", "delivery"]
    ]
}

# === Initialize Adapter ===
adapter = TopicAdapter(
    chunks=allergy_dataset["chunks"],
    entities_per_chunk=allergy_dataset["entities"]
)

# === Query Example ===
query = "What medications was the patient discharged with?"
topics = adapter.match_query_to_topics(query)

# === Output ===
for topic in topics:
    print(f"\nTopic ID: {topic['topic_id']}")
    print(f"Score: {topic['score']:.4f}")
    print(f"Entities: {topic['entities']}")


#consuming adaptor in the pipeline main code
# from shopping_list_adaptor import TopicAdapter
# chunks = [
#     "Patient was discharged with carvedilol, lisinopril, and warfarin.",
#     "He has a history of diabetes and uses insulin.",
#     "Social work arranged transport and medication delivery."
# ]

# entities_per_chunk = [
#     ["carvedilol", "lisinopril", "warfarin"],
#     ["diabetes", "insulin"],
#     ["transport", "medication", "delivery"]
# ]
# adapter = TopicAdapter(chunks=chunks, entities_per_chunk=entities_per_chunk)
# query = "What medications was the patient discharged with?"
# results = adapter.match_query_to_topics(query, top_k=3)
# for topic in results:
#     print(f"\nTopic ID: {topic['topic_id']}")
#     print(f"Score: {topic['score']:.4f}")
#     print(f"Entities: {topic['entities']}")





#assuming we hav eht topic as json file
# json=[
#   {
#     "topic_id": 0]1,
#     "entities": ["warfarin", "carvedilol"],
#     "embedding": [0.12, 0.34, ..., 0.98]
#   },
#   {
#     "topic_id": 1,
#     "entities": ["medication", "transport"],
#     "embedding": [0.22, 0.11, ..., 0.87]
#   }
# ]


import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class TopicAdapter:
    def __init__(self, topic_json_path, model_name="pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"):
        self.embedding_model = SentenceTransformer(model_name)
        self.topic_metadata = []
        self.topic_embeddings = None
        self.search_index = None
        self._load_topics_from_json(topic_json_path)

    def _load_topics_from_json(self, json_path):
        import json
        with open(json_path, "r") as f:
            data = json.load(f)

        topic_embeddings = []
        for item in data:
            emb = np.array(item["embedding"], dtype=np.float32)
            emb /= np.linalg.norm(emb) + 1e-10
            topic_embeddings.append(emb)
            self.topic_metadata.append({
                "topic_id": item["topic_id"],
                "entities": item["entities"]
            })

        self.topic_embeddings = np.array(topic_embeddings, dtype=np.float32)

        # === FAISS Index Setup ===
        dim = self.topic_embeddings.shape[1]
        self.search_index = faiss.IndexFlatIP(dim)
        self.search_index.add(self.topic_embeddings)

    def match_query_to_topics(self, query, top_k=3):
        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]
        query_emb /= np.linalg.norm(query_emb) + 1e-10
        query_emb = query_emb.astype(np.float32).reshape(1, -1)
        scores, indices = self.search_index.search(query_emb, top_k)

        results = []
        for i, idx in enumerate(indices[0]):
            score = float(scores[0][i])
            if score < -1e+10:
                continue
            meta = self.topic_metadata[idx]
            results.append({
                "topic_id": meta["topic_id"],
                "score": score,
                "entities": meta["entities"]
            })
        return results

