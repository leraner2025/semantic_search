{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDBqgK7yE5Pl"
   },
   "outputs": [],
   "source": [
    "#install dependency\n",
    "\n",
    "# Install Tesseract and Poppler\n",
    "!sudo apt update\n",
    "!sudo apt install -y tesseract-ocr poppler-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMdhw-dbJIR4"
   },
   "outputs": [],
   "source": [
    "# #optional:Configure Tesseract Path if Needed\n",
    "# import pytesseract\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9osdqnUjJBqO"
   },
   "outputs": [],
   "source": [
    "!pip install pytesseract pdf2image pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmNE-O23SK9M"
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhpFnw-HHF20"
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "# Core Vertex AI SDK\n",
    "from google.cloud import aiplatform\n",
    "# For working with Gemini (Text Generation) models\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "# Initialize Vertex AI client\n",
    "aiplatform.init(\n",
    "    project=\"your-gcp-project-id\",     # Replace with your actual project ID\n",
    "    location=\"us-central1\"             # Use supported region like \"us-central1\"\n",
    ")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pages = convert_from_path(pdf_path)\n",
    "    all_lines = []\n",
    "\n",
    "    for page_num, page in enumerate(pages):\n",
    "        ocr_data = pytesseract.image_to_data(page, output_type=pytesseract.Output.DICT)\n",
    "        n_boxes = len(ocr_data['text'])\n",
    "\n",
    "        line_map = defaultdict(lambda: {\n",
    "            \"text\": \"\",\n",
    "            \"words\": [],\n",
    "            \"bbox\": [],\n",
    "            \"page\": page_num + 1,\n",
    "            \"block_num\": None,\n",
    "            \"par_num\": None,\n",
    "            \"line_num\": None\n",
    "        })\n",
    "\n",
    "        for i in range(n_boxes):\n",
    "            word = ocr_data['text'][i].strip()\n",
    "            conf = int(ocr_data['conf'][i])\n",
    "            if not word or conf < 0:\n",
    "                continue\n",
    "\n",
    "            block_num = ocr_data['block_num'][i]\n",
    "            par_num = ocr_data['par_num'][i]\n",
    "            line_num = ocr_data['line_num'][i]\n",
    "\n",
    "            line_id = (block_num, par_num, line_num)\n",
    "\n",
    "            left = ocr_data['left'][i]\n",
    "            top = ocr_data['top'][i]\n",
    "            width = ocr_data['width'][i]\n",
    "            height = ocr_data['height'][i]\n",
    "            bottom = top + height\n",
    "\n",
    "            bbox = (left, top, left + width, bottom)\n",
    "\n",
    "            # Add to line map\n",
    "            line = line_map[line_id]\n",
    "            line[\"text\"] += word + ' '\n",
    "            line[\"block_num\"] = block_num\n",
    "            line[\"par_num\"] = par_num\n",
    "            line[\"line_num\"] = line_num\n",
    "            line[\"bbox\"].append(bbox)\n",
    "\n",
    "            line[\"words\"].append({\n",
    "                \"word\": word,\n",
    "                \"bbox\": bbox,\n",
    "                \"conf\": conf,\n",
    "                \"index\": i,\n",
    "                \"superscript\": None  # To be determined after\n",
    "            })\n",
    "\n",
    "        # Superscript detection based on baseline average\n",
    "        for line in line_map.values():\n",
    "            if not line[\"words\"]:\n",
    "                continue\n",
    "            avg_top = sum(b[1] for b in line[\"bbox\"]) / len(line[\"bbox\"])\n",
    "            for word_data in line[\"words\"]:\n",
    "                top = word_data[\"bbox\"][1]\n",
    "                height = word_data[\"bbox\"][3] - word_data[\"bbox\"][1]\n",
    "                # Superscript if higher than baseline and short height\n",
    "                word_data[\"superscript\"] = top < (avg_top - height * 0.3)\n",
    "\n",
    "        for line in line_map.values():\n",
    "            all_lines.append({\n",
    "                \"text\": line[\"text\"].strip(),\n",
    "                \"words\": line[\"words\"],\n",
    "                \"bbox\": line[\"bbox\"],\n",
    "                \"page\": line[\"page\"],\n",
    "                \"block_num\": line[\"block_num\"],\n",
    "                \"par_num\": line[\"par_num\"],\n",
    "                \"line_num\": line[\"line_num\"]\n",
    "            })\n",
    "\n",
    "    return all_lines\n",
    "\n",
    "def extract_dates(line_text):\n",
    "    date_pattern = r'\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{2}[-/]\\d{2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2},?\\s+\\d{4})\\b'\n",
    "    return re.findall(date_pattern, line_text)\n",
    "\n",
    "def is_probable_footnote(line_text):\n",
    "    return (\n",
    "        re.match(r'^\\(?\\d{1,2}\\)?[\\.\\s]', line_text) or\n",
    "        len(line_text.strip()) < 30 and bool(re.search(r'\\d{4}', line_text)) or\n",
    "        any(kw in line_text.lower() for kw in ['note:', 'ref.', 'copyright', 'source:', 'doi'])\n",
    "    )\n",
    "\n",
    "def is_header_like(line_text):\n",
    "    line_text = line_text.strip()\n",
    "    if not line_text or len(line_text) > 150:\n",
    "        return False\n",
    "    has_numbering = re.match(r'^(\\d+(\\.\\d+)*[\\)\\.]?)\\s+', line_text)\n",
    "    has_superscript = re.match(r'^[¹²³⁴⁵⁶⁷⁸⁹⁰]+\\s*', line_text)\n",
    "    is_caps = line_text.isupper()\n",
    "    is_title = line_text.istitle()\n",
    "    return has_numbering or has_superscript or is_caps or is_title\n",
    "\n",
    "def group_entities_and_context(lines):\n",
    "    blocks = []\n",
    "    current_block = None\n",
    "    found_entity = False\n",
    "\n",
    "    for line_data in lines:\n",
    "        stripped = line_data[\"text\"].strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "\n",
    "        if is_probable_footnote(stripped):\n",
    "            continue\n",
    "\n",
    "        dates = extract_dates(stripped)\n",
    "        is_header = is_header_like(stripped)\n",
    "\n",
    "        if dates:\n",
    "            found_entity = True\n",
    "            current_block = {\n",
    "                \"type\": \"Date\",\n",
    "                \"value\": dates[0],\n",
    "                \"raw\": stripped,\n",
    "                \"context\": [],\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]  # Include word-level info\n",
    "            }\n",
    "            blocks.append(current_block)\n",
    "\n",
    "        elif is_header:\n",
    "            found_entity = True\n",
    "            current_block = {\n",
    "                \"type\": \"Header\",\n",
    "                \"value\": stripped,\n",
    "                \"raw\": stripped,\n",
    "                \"context\": [],\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]  # Include word-level info\n",
    "            }\n",
    "            blocks.append(current_block)\n",
    "\n",
    "        elif current_block:\n",
    "            current_block[\"context\"].append({\n",
    "                \"text\": stripped,\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]  # Include word-level info for context lines\n",
    "            })\n",
    "\n",
    "    if not found_entity:\n",
    "        return \"No valid dates, headers, or subheaders found in input.\"\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def analyze_icr_pdf(pdf_path):\n",
    "    ocr_lines = extract_text_from_pdf(pdf_path)\n",
    "    result = group_entities_and_context(ocr_lines)\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"your_icr_file.pdf\"  # Replace with actual PDF path\n",
    "    results = analyze_icr_pdf(pdf_path)\n",
    "\n",
    "    if isinstance(ocr_result, str):\n",
    "      print(ocr_result)\n",
    "    else:\n",
    "        print(\"\\nSample Output: Showing 5 Identified Entities with Bounding Boxes, Indices, Superscripts, and Metadata:\")\n",
    "        for block in ocr_result[:5]:  # Only show first 5\n",
    "            print(f\"\\n[{block['type']}] {block['value']} (Page {block['page']})\")\n",
    "            print(f\"Bounding Box: {block['bbox']}\")\n",
    "            print(f\"Line Meta: {block['line_meta']}\")\n",
    "            print(\"Words:\")\n",
    "            for w in block[\"words\"]:\n",
    "                print(f\"  - Word: '{w['word']}', Index: {w['index']}, Superscript: {w['superscript']}, BBox: {w['bbox']}, Conf: {w['conf']}\")\n",
    "\n",
    "            if block[\"context\"]:\n",
    "                print(\"Context:\")\n",
    "                for ctx in block[\"context\"]:\n",
    "                    print(f\"  - (Page {ctx['page']}) {ctx['text']}\")\n",
    "                    print(f\"    BBox: {ctx['bbox']}\")\n",
    "                    print(f\"    Meta: {ctx['line_meta']}\")\n",
    "                    print(\"    Words:\")\n",
    "                    for w in ctx[\"words\"]:\n",
    "                        print(f\"      * Word: '{w['word']}', Index: {w['index']}, Superscript: {w['superscript']}, BBox: {w['bbox']}, Conf: {w['conf']}\")\n",
    "            else:\n",
    "                print(\"No meaningful context found.\")\n",
    "            print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFvbC6VnIC6m"
   },
   "outputs": [],
   "source": [
    "# [\n",
    "#     {\n",
    "#         \"type\": \"Header\",\n",
    "#         \"value\": \"RESUME\",\n",
    "#         \"raw\": \"RESUME\",\n",
    "#         \"bbox\": [(50, 100, 150, 130)],  # bounding box for \"RESUME\"\n",
    "#         \"page\": 1,\n",
    "#         \"line_meta\": {\n",
    "#             \"block_num\": 1,\n",
    "#             \"par_num\": 1,\n",
    "#             \"line_num\": 1\n",
    "#         },\n",
    "#         \"words\": [\n",
    "#             {\n",
    "#                 \"word\": \"RESUME\",\n",
    "#                 \"bbox\": (50, 100, 150, 130),\n",
    "#                 \"conf\": 96,      # OCR confidence\n",
    "#                 \"index\": 5,      # word index from pytesseract output\n",
    "#                 \"superscript\": False\n",
    "#             }\n",
    "#         ],\n",
    "#         \"context\": [\n",
    "#             {\n",
    "#                 \"text\": \"John Doe is a software engineer with 5 years of experience.\",\n",
    "#                 \"bbox\": [(50, 140, 500, 160)],  # bounding box of this line\n",
    "#                 \"page\": 1,\n",
    "#                 \"line_meta\": {\n",
    "#                     \"block_num\": 1,\n",
    "#                     \"par_num\": 2,\n",
    "#                     \"line_num\": 2\n",
    "#                 },\n",
    "#                 \"words\": [\n",
    "#                     {\"word\": \"John\", \"bbox\": (50, 140, 90, 160), \"conf\": 95, \"index\": 6, \"superscript\": False},\n",
    "#                     {\"word\": \"Doe\", \"bbox\": (95, 140, 130, 160), \"conf\": 94, \"index\": 7, \"superscript\": False},\n",
    "#                     {\"word\": \"is\", \"bbox\": (135, 140, 160, 160), \"conf\": 96, \"index\": 8, \"superscript\": False},\n",
    "#                     {\"word\": \"a\", \"bbox\": (165, 140, 175, 160), \"conf\": 93, \"index\": 9, \"superscript\": False},\n",
    "#                     {\"word\": \"software\", \"bbox\": (180, 140, 250, 160), \"conf\": 97, \"index\": 10, \"superscript\": False},\n",
    "#                     {\"word\": \"engineer\", \"bbox\": (255, 140, 320, 160), \"conf\": 96, \"index\": 11, \"superscript\": False},\n",
    "#                     {\"word\": \"with\", \"bbox\": (325, 140, 360, 160), \"conf\": 95, \"index\": 12, \"superscript\": False},\n",
    "#                     {\"word\": \"5\", \"bbox\": (365, 140, 375, 160), \"conf\": 90, \"index\": 13, \"superscript\": False},\n",
    "#                     {\"word\": \"years\", \"bbox\": (380, 140, 420, 160), \"conf\": 96, \"index\": 14, \"superscript\": False},\n",
    "#                     {\"word\": \"of\", \"bbox\": (425, 140, 450, 160), \"conf\": 94, \"index\": 15, \"superscript\": False},\n",
    "#                     {\"word\": \"experience.\", \"bbox\": (455, 140, 520, 160), \"conf\": 97, \"index\": 16, \"superscript\": False}\n",
    "#                 ]\n",
    "#             },\n",
    "#             {\n",
    "#                 \"text\": \"He specializes in Python and Machine Learning.\",\n",
    "#                 \"bbox\": [(50, 170, 450, 190)],\n",
    "#                 \"page\": 1,\n",
    "#                 \"line_meta\": {\n",
    "#                     \"block_num\": 1,\n",
    "#                     \"par_num\": 3,\n",
    "#                     \"line_num\": 3\n",
    "#                 },\n",
    "#                 \"words\": [\n",
    "#                     {\"word\": \"He\", \"bbox\": (50, 170, 70, 190), \"conf\": 95, \"index\": 17, \"superscript\": False},\n",
    "#                     {\"word\": \"specializes\", \"bbox\": (75, 170, 150, 190), \"conf\": 94, \"index\": 18, \"superscript\": False},\n",
    "#                     {\"word\": \"in\", \"bbox\": (155, 170, 180, 190), \"conf\": 95, \"index\": 19, \"superscript\": False},\n",
    "#                     {\"word\": \"Python\", \"bbox\": (185, 170, 230, 190), \"conf\": 96, \"index\": 20, \"superscript\": False},\n",
    "#                     {\"word\": \"and\", \"bbox\": (235, 170, 260, 190), \"conf\": 94, \"index\": 21, \"superscript\": False},\n",
    "#                     {\"word\": \"Machine\", \"bbox\": (265, 170, 315, 190), \"conf\": 96, \"index\": 22, \"superscript\": False},\n",
    "#                     {\"word\": \"Learning.\", \"bbox\": (320, 170, 390, 190), \"conf\": 95, \"index\": 23, \"superscript\": False}\n",
    "#                 ]\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mokscVK-Ugmi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-5PK8YMUcvS"
   },
   "outputs": [],
   "source": [
    "#Using data from bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eFpxxWTUODs"
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "# For reading files from GCS\n",
    "import gcsfs\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=\"your-gcp-project-id\",  #  Replace with your GCP project ID\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "def download_pdf_from_gcs(gcs_path, local_path=\"temp_icr_file.pdf\"):\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    with fs.open(gcs_path, 'rb') as f:\n",
    "        file_bytes = f.read()\n",
    "    with open(local_path, 'wb') as out_file:\n",
    "        out_file.write(file_bytes)\n",
    "    return local_path\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pages = convert_from_path(pdf_path)\n",
    "    all_lines = []\n",
    "\n",
    "    for page_num, page in enumerate(pages):\n",
    "        ocr_data = pytesseract.image_to_data(page, output_type=pytesseract.Output.DICT)\n",
    "        n_boxes = len(ocr_data['text'])\n",
    "\n",
    "        line_map = defaultdict(lambda: {\n",
    "            \"text\": \"\",\n",
    "            \"words\": [],\n",
    "            \"bbox\": [],\n",
    "            \"page\": page_num + 1,\n",
    "            \"block_num\": None,\n",
    "            \"par_num\": None,\n",
    "            \"line_num\": None\n",
    "        })\n",
    "\n",
    "        for i in range(n_boxes):\n",
    "            word = ocr_data['text'][i].strip()\n",
    "            conf = int(ocr_data['conf'][i])\n",
    "            if not word or conf < 0:\n",
    "                continue\n",
    "\n",
    "            block_num = ocr_data['block_num'][i]\n",
    "            par_num = ocr_data['par_num'][i]\n",
    "            line_num = ocr_data['line_num'][i]\n",
    "\n",
    "            line_id = (block_num, par_num, line_num)\n",
    "\n",
    "            left = ocr_data['left'][i]\n",
    "            top = ocr_data['top'][i]\n",
    "            width = ocr_data['width'][i]\n",
    "            height = ocr_data['height'][i]\n",
    "            bottom = top + height\n",
    "\n",
    "            bbox = (left, top, left + width, bottom)\n",
    "\n",
    "            line = line_map[line_id]\n",
    "            line[\"text\"] += word + ' '\n",
    "            line[\"block_num\"] = block_num\n",
    "            line[\"par_num\"] = par_num\n",
    "            line[\"line_num\"] = line_num\n",
    "            line[\"bbox\"].append(bbox)\n",
    "\n",
    "            line[\"words\"].append({\n",
    "                \"word\": word,\n",
    "                \"bbox\": bbox,\n",
    "                \"conf\": conf,\n",
    "                \"index\": i,\n",
    "                \"superscript\": None\n",
    "            })\n",
    "\n",
    "        for line in line_map.values():\n",
    "            if not line[\"words\"]:\n",
    "                continue\n",
    "            avg_top = sum(b[1] for b in line[\"bbox\"]) / len(line[\"bbox\"])\n",
    "            for word_data in line[\"words\"]:\n",
    "                top = word_data[\"bbox\"][1]\n",
    "                height = word_data[\"bbox\"][3] - word_data[\"bbox\"][1]\n",
    "                word_data[\"superscript\"] = top < (avg_top - height * 0.3)\n",
    "\n",
    "        for line in line_map.values():\n",
    "            all_lines.append({\n",
    "                \"text\": line[\"text\"].strip(),\n",
    "                \"words\": line[\"words\"],\n",
    "                \"bbox\": line[\"bbox\"],\n",
    "                \"page\": line[\"page\"],\n",
    "                \"block_num\": line[\"block_num\"],\n",
    "                \"par_num\": line[\"par_num\"],\n",
    "                \"line_num\": line[\"line_num\"]\n",
    "            })\n",
    "\n",
    "    return all_lines\n",
    "\n",
    "def extract_dates(line_text):\n",
    "    date_pattern = r'\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{2}[-/]\\d{2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2},?\\s+\\d{4})\\b'\n",
    "    return re.findall(date_pattern, line_text)\n",
    "\n",
    "def is_probable_footnote(line_text):\n",
    "    return (\n",
    "        re.match(r'^\\(?\\d{1,2}\\)?[\\.\\s]', line_text) or\n",
    "        len(line_text.strip()) < 30 and bool(re.search(r'\\d{4}', line_text)) or\n",
    "        any(kw in line_text.lower() for kw in ['note:', 'ref.', 'copyright', 'source:', 'doi'])\n",
    "    )\n",
    "\n",
    "def is_header_like(line_text):\n",
    "    line_text = line_text.strip()\n",
    "    if not line_text or len(line_text) > 150:\n",
    "        return False\n",
    "    has_numbering = re.match(r'^(\\d+(\\.\\d+)*[\\)\\.]?)\\s+', line_text)\n",
    "    has_superscript = re.match(r'^[¹²³⁴⁵⁶⁷⁸⁹⁰]+\\s*', line_text)\n",
    "    is_caps = line_text.isupper()\n",
    "    is_title = line_text.istitle()\n",
    "    return has_numbering or has_superscript or is_caps or is_title\n",
    "\n",
    "def group_entities_and_context(lines):\n",
    "    blocks = []\n",
    "    current_block = None\n",
    "    found_entity = False\n",
    "\n",
    "    for line_data in lines:\n",
    "        stripped = line_data[\"text\"].strip()\n",
    "        if not stripped or is_probable_footnote(stripped):\n",
    "            continue\n",
    "\n",
    "        dates = extract_dates(stripped)\n",
    "        is_header = is_header_like(stripped)\n",
    "\n",
    "        if dates:\n",
    "            found_entity = True\n",
    "            current_block = {\n",
    "                \"type\": \"Date\",\n",
    "                \"value\": dates[0],\n",
    "                \"raw\": stripped,\n",
    "                \"context\": [],\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]\n",
    "            }\n",
    "            blocks.append(current_block)\n",
    "\n",
    "        elif is_header:\n",
    "            found_entity = True\n",
    "            current_block = {\n",
    "                \"type\": \"Header\",\n",
    "                \"value\": stripped,\n",
    "                \"raw\": stripped,\n",
    "                \"context\": [],\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]\n",
    "            }\n",
    "            blocks.append(current_block)\n",
    "\n",
    "        elif current_block:\n",
    "            current_block[\"context\"].append({\n",
    "                \"text\": stripped,\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]\n",
    "            })\n",
    "\n",
    "    return blocks if found_entity else \"No valid dates, headers, or subheaders found.\"\n",
    "\n",
    "def analyze_icr_pdf_from_gcs(gcs_pdf_path):\n",
    "    local_path = download_pdf_from_gcs(gcs_pdf_path)\n",
    "    ocr_lines = extract_text_from_pdf(local_path)\n",
    "    result = group_entities_and_context(ocr_lines)\n",
    "    os.remove(local_path)\n",
    "    return result\n",
    "\n",
    "# Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    gcs_path = \"gs://your-bucket-name/path/to/your.pdf\"  # Replace with your GCS PDF path\n",
    "    ocr_result = analyze_icr_pdf_from_gcs(gcs_path)\n",
    "\n",
    "    if isinstance(ocr_result, str):\n",
    "        print(ocr_result)\n",
    "    else:\n",
    "        print(\"\\nSample Output: Showing 5 Identified Entities with Bounding Boxes, Indices, Superscripts, and Metadata:\")\n",
    "        for block in ocr_result[:5]:  # Only show first 5\n",
    "            print(f\"\\n[{block['type']}] {block['value']} (Page {block['page']})\")\n",
    "            print(f\"Bounding Box: {block['bbox']}\")\n",
    "            print(f\"Line Meta: {block['line_meta']}\")\n",
    "            print(\"Words:\")\n",
    "            for w in block[\"words\"]:\n",
    "                print(f\"  - Word: '{w['word']}', Index: {w['index']}, Superscript: {w['superscript']}, BBox: {w['bbox']}, Conf: {w['conf']}\")\n",
    "\n",
    "            if block[\"context\"]:\n",
    "                print(\"Context:\")\n",
    "                for ctx in block[\"context\"]:\n",
    "                    print(f\"  - (Page {ctx['page']}) {ctx['text']}\")\n",
    "                    print(f\"    BBox: {ctx['bbox']}\")\n",
    "                    print(f\"    Meta: {ctx['line_meta']}\")\n",
    "                    print(\"    Words:\")\n",
    "                    for w in ctx[\"words\"]:\n",
    "                        print(f\"      * Word: '{w['word']}', Index: {w['index']}, Superscript: {w['superscript']}, BBox: {w['bbox']}, Conf: {w['conf']}\")\n",
    "            else:\n",
    "                print(\"No meaningful context found.\")\n",
    "            print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VujlItOMV24b"
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "# For reading files from GCS\n",
    "import gcsfs\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=\"your-gcp-project-id\",  # Replace with your GCP project ID\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "def download_pdf_from_gcs(gcs_path, local_path=\"temp_icr_file.pdf\"):\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    with fs.open(gcs_path, 'rb') as f:\n",
    "        file_bytes = f.read()\n",
    "    with open(local_path, 'wb') as out_file:\n",
    "        out_file.write(file_bytes)\n",
    "    return local_path\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pages = convert_from_path(pdf_path)\n",
    "    all_lines = []\n",
    "\n",
    "    for page_num, page in enumerate(pages):\n",
    "        ocr_data = pytesseract.image_to_data(page, output_type=pytesseract.Output.DICT)\n",
    "        n_boxes = len(ocr_data['text'])\n",
    "\n",
    "        line_map = defaultdict(lambda: {\n",
    "            \"text\": \"\",\n",
    "            \"words\": [],\n",
    "            \"bbox\": [],\n",
    "            \"page\": page_num + 1,\n",
    "            \"block_num\": None,\n",
    "            \"par_num\": None,\n",
    "            \"line_num\": None\n",
    "        })\n",
    "\n",
    "        for i in range(n_boxes):\n",
    "            word = ocr_data['text'][i].strip()\n",
    "            conf = int(ocr_data['conf'][i])\n",
    "            if not word or conf < 0:\n",
    "                continue\n",
    "\n",
    "            block_num = ocr_data['block_num'][i]\n",
    "            par_num = ocr_data['par_num'][i]\n",
    "            line_num = ocr_data['line_num'][i]\n",
    "\n",
    "            line_id = (block_num, par_num, line_num)\n",
    "\n",
    "            left = ocr_data['left'][i]\n",
    "            top = ocr_data['top'][i]\n",
    "            width = ocr_data['width'][i]\n",
    "            height = ocr_data['height'][i]\n",
    "            bottom = top + height\n",
    "\n",
    "            bbox = (left, top, left + width, bottom)\n",
    "\n",
    "            line = line_map[line_id]\n",
    "            line[\"text\"] += word + ' '\n",
    "            line[\"block_num\"] = block_num\n",
    "            line[\"par_num\"] = par_num\n",
    "            line[\"line_num\"] = line_num\n",
    "            line[\"bbox\"].append(bbox)\n",
    "\n",
    "            line[\"words\"].append({\n",
    "                \"word\": word,\n",
    "                \"bbox\": bbox,\n",
    "                \"conf\": conf,\n",
    "                \"index\": i,\n",
    "                \"superscript\": None\n",
    "            })\n",
    "\n",
    "        for line in line_map.values():\n",
    "            if not line[\"words\"]:\n",
    "                continue\n",
    "            avg_top = sum(b[1] for b in line[\"bbox\"]) / len(line[\"bbox\"])\n",
    "            for word_data in line[\"words\"]:\n",
    "                top = word_data[\"bbox\"][1]\n",
    "                height = word_data[\"bbox\"][3] - word_data[\"bbox\"][1]\n",
    "                word_data[\"superscript\"] = top < (avg_top - height * 0.3)\n",
    "\n",
    "        for line in line_map.values():\n",
    "            all_lines.append({\n",
    "                \"text\": line[\"text\"].strip(),\n",
    "                \"words\": line[\"words\"],\n",
    "                \"bbox\": line[\"bbox\"],\n",
    "                \"page\": line[\"page\"],\n",
    "                \"block_num\": line[\"block_num\"],\n",
    "                \"par_num\": line[\"par_num\"],\n",
    "                \"line_num\": line[\"line_num\"]\n",
    "            })\n",
    "\n",
    "    return all_lines\n",
    "\n",
    "def extract_dates(line_text):\n",
    "    date_pattern = r'\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{2}[-/]\\d{2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2},?\\s+\\d{4})\\b'\n",
    "    return re.findall(date_pattern, line_text)\n",
    "\n",
    "def is_probable_footnote(line_text):\n",
    "    return (\n",
    "        re.match(r'^\\(?\\d{1,2}\\)?[\\.\\s]', line_text) or\n",
    "        len(line_text.strip()) < 30 and bool(re.search(r'\\d{4}', line_text)) or\n",
    "        any(kw in line_text.lower() for kw in ['note:', 'ref.', 'copyright', 'source:', 'doi'])\n",
    "    )\n",
    "\n",
    "def is_header_like(line_text):\n",
    "    line_text = line_text.strip()\n",
    "    if not line_text or len(line_text) > 150:\n",
    "        return False\n",
    "    has_numbering = re.match(r'^(\\d+(\\.\\d+)*[\\)\\.]?)\\s+', line_text)\n",
    "    has_superscript = re.match(r'^[¹²³⁴⁵⁶⁷⁸⁹⁰]+\\s*', line_text)\n",
    "    is_caps = line_text.isupper()\n",
    "    is_title = line_text.istitle()\n",
    "    return has_numbering or has_superscript or is_caps or is_title\n",
    "\n",
    "def group_entities_and_context(lines):\n",
    "    blocks = []\n",
    "    current_block = None\n",
    "    found_entity = False\n",
    "\n",
    "    for line_data in lines:\n",
    "        stripped = line_data[\"text\"].strip()\n",
    "        if not stripped or is_probable_footnote(stripped):\n",
    "            continue\n",
    "\n",
    "        dates = extract_dates(stripped)\n",
    "        is_header = is_header_like(stripped)\n",
    "\n",
    "        if dates:\n",
    "            found_entity = True\n",
    "            current_block = {\n",
    "                \"type\": \"Date\",\n",
    "                \"value\": dates[0],\n",
    "                \"raw\": stripped,\n",
    "                \"context\": [],\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]\n",
    "            }\n",
    "            blocks.append(current_block)\n",
    "\n",
    "        elif is_header:\n",
    "            found_entity = True\n",
    "            current_block = {\n",
    "                \"type\": \"Header\",\n",
    "                \"value\": stripped,\n",
    "                \"raw\": stripped,\n",
    "                \"context\": [],\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]\n",
    "            }\n",
    "            blocks.append(current_block)\n",
    "\n",
    "        elif current_block:\n",
    "            current_block[\"context\"].append({\n",
    "                \"text\": stripped,\n",
    "                \"bbox\": line_data[\"bbox\"],\n",
    "                \"page\": line_data[\"page\"],\n",
    "                \"line_meta\": {\n",
    "                    \"block_num\": line_data[\"block_num\"],\n",
    "                    \"par_num\": line_data[\"par_num\"],\n",
    "                    \"line_num\": line_data[\"line_num\"]\n",
    "                },\n",
    "                \"words\": line_data[\"words\"]\n",
    "            })\n",
    "\n",
    "    return blocks if found_entity else \"No valid dates, headers, or subheaders found.\"\n",
    "\n",
    "def ocr_blocks_to_text(blocks):\n",
    "    text_output = []\n",
    "    for block in blocks:\n",
    "        header_or_date = f\"{block['type']}: {block['value']}\"\n",
    "        text_output.append(header_or_date)\n",
    "        if block.get(\"context\"):\n",
    "            for ctx in block[\"context\"]:\n",
    "                text_output.append(f\"  - {ctx['text']}\")\n",
    "        text_output.append(\"-\" * 40)\n",
    "    return \"\\n\".join(text_output)\n",
    "\n",
    "def call_gemini_llm(ocr_text):\n",
    "    prompt = f\"\"\"\n",
    "You are given OCR extracted blocks from a document.\n",
    "Each block has a 'Header' or 'Date' followed by associated context lines.\n",
    "\n",
    "Please organize the output in the following format:\n",
    "Each Header or Date should be printed as a title, and the context lines under it with bullet points.\n",
    "\n",
    "Example format:\n",
    "Header: <Header Name>\n",
    "- <Context Line 1>\n",
    "- <Context Line 2>\n",
    "\n",
    "Date: <Date>\n",
    "- <Context Line 1>\n",
    "- <Context Line 2>\n",
    "\n",
    "If no context is available, just print the header/date without bullets.\n",
    "\n",
    "Here is the OCR text:\n",
    "\\\"\\\"\\\"\n",
    "{ocr_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    model = TextGenerationModel.from_pretrained(\"gemini-1.0-pro\")\n",
    "    response = model.predict(\n",
    "        prompt=prompt,\n",
    "        temperature=0.3,\n",
    "        max_output_tokens=2048,\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "def analyze_icr_pdf_from_gcs(gcs_pdf_path):\n",
    "    local_path = download_pdf_from_gcs(gcs_pdf_path)\n",
    "    ocr_lines = extract_text_from_pdf(local_path)\n",
    "    result = group_entities_and_context(ocr_lines)\n",
    "    os.remove(local_path)\n",
    "    return result\n",
    "\n",
    "# ======= Main Execution =======\n",
    "if __name__ == \"__main__\":\n",
    "    gcs_path = \"gs://your-bucket-name/path/to/your.pdf\"  # Update this path\n",
    "    ocr_result = analyze_icr_pdf_from_gcs(gcs_path)\n",
    "\n",
    "    if isinstance(ocr_result, str):\n",
    "        print(\"OCR Error or no valid content:\", ocr_result)\n",
    "    else:\n",
    "        print(\"\\nSample Output: Showing 5 Identified Entities\")\n",
    "        for block in ocr_result[:5]:\n",
    "            print(f\"[{block['type']}] {block['value']} (Page {block['page']})\")\n",
    "            print(f\"Context lines: {len(block.get('context', []))}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        ocr_text = ocr_blocks_to_text(ocr_result)\n",
    "        print(\"\\n=== OCR Text for LLM ===\")\n",
    "        print(ocr_text)\n",
    "\n",
    "        llm_response = call_gemini_llm(ocr_text)\n",
    "        print(\"\\n=== LLM Response ===\")\n",
    "        print(llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehk0bO8uZQcJ"
   },
   "outputs": [],
   "source": [
    "#expected ouput\n",
    "\n",
    "# Header: Executive Summary\n",
    "# - The report highlights key trends in revenue growth.\n",
    "# - Focus areas include marketing ROI and customer retention.\n",
    "\n",
    "# Date: January 10, 2023\n",
    "# - Annual performance review conducted.\n",
    "# - Strategy planning sessions initiated.\n",
    "\n",
    "# Header: Recommendations\n",
    "# - Improve data integration workflows.\n",
    "# - Reevaluate vendor contracts in Q2.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
