{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jQ_l2BOsULLR",
    "outputId": "5a134313-cc35-4236-96d9-711017bee161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Collecting scann\n",
      "  Downloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.9.post2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.43.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n",
      "Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scann, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bertopic\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bertopic-0.17.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 scann-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
      "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
      "Requirement already satisfied: scann in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.43.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m619.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.16.1\n",
      "    Uninstalling scipy-1.16.1:\n",
      "      Successfully uninstalled scipy-1.16.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scann 1.4.0 requires numpy~=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "101a66c08a4b4319834fb1a4c4ed20d5",
       "pip_warning": {
        "packages": [
         "numpy",
         "scipy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install numpy sentence-transformers bertopic hdbscan nltk scann\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "!pip install sentence-transformers bertopic hdbscan umap-learn scann nltk datasets\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P16N5paOAvgg"
   },
   "source": [
    "# Topic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540,
     "referenced_widgets": [
      "c08194336d8242798173f27d1e0a40ea",
      "87fec3ec8a0748b3aedf9fa063508a33",
      "5fb72593b6ce41f98ea371997b7884df",
      "a3dccdbbe4c7471a9a216c24ad2ef773",
      "3aa8f17ef71f4249b2a1d92441467514",
      "2df5255d5eec45d6878c68eb9f5d09ab",
      "3a1879304faf4d2bb483057ab506d26c",
      "69a356b86708468e934bfb009b331729",
      "37ad0968be7c4d57a9ad3f8bbabe0916",
      "d5df752d5fdd4d1f9eb6f3b53a8c464e",
      "5eaeda0c309b461dadbdccd5c86e2190",
      "c6bed6d3a51b43689c781fd170eca5e5",
      "f99394feceef442bab6c94a3487331fd",
      "d49f3289d2f9431f9e357f10ac3535ea",
      "153080483b374025b53e560fb285d8da",
      "affc591511c74e2082cbfa94250c9e26",
      "1d4aa65450894663a8f53c13cbdbd984",
      "bcded68949f64996a6966b07df3e515e",
      "7a49f9a8f5934046891ea09fcc36ebb8",
      "a6701bf4989c4b2488fe0e1f549c6507",
      "ce211cff2e4f4a57b0013fc8c9ebcb84",
      "81a015a727bf41f3816fedf11cbb3149",
      "445b3a0caa694e76977b40dbac8ec24f",
      "b476bf70acba4163b9c90211a6f43194",
      "7f5d176fe30d42828d7cac9d6cbf3108",
      "f347a0bd188648d1be754f223744965e",
      "c9cb7e2967d04a81a5d6628cab69400f",
      "ea79c5edeab74fbba24a120c6c14f600",
      "76df3fb2a1b94f158ed3b056b7d0f720",
      "bb852c39cc424255b199278ec540b895",
      "3b585c724ad14d7780a76388b0aeb0d6",
      "a37ce4e5d43e45f8b8dc17e6b831cfb9",
      "c6f205cd416941e4aab6306b8d153934",
      "07344fa63e7c4f7bba2d103033895e79",
      "24e8707c1fe54813b13945563f6e096d",
      "16b6e8aad3f44bd8b43ec6c8f177b19b",
      "bd1758a15d8e4f8faa58a66948926146",
      "f956fa2c34e64da89129e87c34e9cd9c",
      "b9fdb6eebab542b9849be757d70bb78e",
      "47e38d757207423791ac0d4d52e2af08",
      "6d461824d8164b56ad6e7a9aae653360",
      "3fcc25139a154acc9c498990dc768234",
      "ba8fc29228d04b1bbb950721fd45c62b",
      "706cea0cf7024fb7a0817e93b6d2f8a4",
      "d4b3cba492f54e4992429f927b7ba9e5",
      "a37de51d3d194da1833e1ebe69895fe0",
      "eab3d8ef466e4e6aa9131475d3c04b5b",
      "09ac9b68173d46bcacd3f31bedf219b9",
      "04632b4a91ec41c1902137a8f26ee500",
      "583f5917dfe749b1adb0c11f39063c67",
      "f43aadba0951432aadd1aed064b98166",
      "1aacf8453a7a4a0f87911c7dfdbc1d2c",
      "1d5a6c20fd7b4814835b282ecc70294d",
      "1424ec830e9c487db89294bd2e0366ca",
      "e11d8fb8e6254737a32d67bc94afaa52",
      "b4e0afe74c7843a181aef4f56336a2d3",
      "9b834e92c3de4e31bdc3fd5c7db89df1",
      "f5b17fc2ce7740179acfa9713adef0b1",
      "fcbae7ba31cb4b1fb384f58b32f80de3",
      "edd62f856b21419b8ee11a16be3cdf8d",
      "9450f1c4c0ef40d0acd4aa6020feee36",
      "fea549ffe4244c8f94c2f9ab1adeb4df",
      "8807495025434ab0b0336f771c50ee7e",
      "75391452ae944f829727f14c75a33ee6",
      "ce7e573c2d9342559c6e33c03f1d34db",
      "659cadb127074d06a66ec0c312fb3da9",
      "8b1b592cc46149fc9e5c84bf452e0ac6",
      "20a5beb0eb6949db826234186bb9cbf4",
      "e6b672b45105495ca812604a9b2296c6",
      "45092176c7bb4af6ad5e548189c93bc9",
      "88cb4e0b2b094977be61ac57e6c9c5f4",
      "f5b9e5d5aa154ec4a8dbb504bc41ff29",
      "1be1373fd93b4d7f8023f5c22d3e0c2f",
      "a6f9c0e2b0ad46cca818e7ddf820cc69",
      "dde1f82621bb4a47aec7999dcc563301",
      "f79871e84f3d4ec1b05888d9ac9270ef",
      "78a276bf6fc6452f9cb1ba81c5be9d06",
      "3b4b740294df4357916853ed99d5025e",
      "34a61c4d77c149979813dd2fe2a58990",
      "c0f3584c64ce4938a66d1b5286b5f066",
      "03fa1af1cb374251bb79dca29802a389",
      "805400f71ae34d24a34fac73570fd04c",
      "ccad9f376e75490cb261a0de097d3f6b",
      "267a9f5257834b61801fd4b28056d5b7",
      "1e5dcdec726d4054a95ca7edd8ce4e5e",
      "36415151f7ef42948f27c799f20b624e",
      "9ad0f4a5dc1947db8243936b95244d15",
      "e9c72216dc5140318f554d692a28a4ec",
      "36b3bb37d62746338f6e81cfe7f80378",
      "11d4fbb421944dc788290b83cc5cef24",
      "b23254d1c1d44cb99a7aaf3d30893818",
      "38642fbe43ef4785984adb12b651df4f",
      "071a121832c3405294b3b3a80bfa4f66",
      "31986ce86aa24d55b9e8ac823fb2e949",
      "7341417692a9472bb594e5bedb58ae13",
      "44c845644ce94841a64eb1cd73d49300",
      "fd45154e4eb54f439f880ac3961313ff",
      "f3ef097c70244f2c9b933c832dc7519f",
      "6521adfafb0e4fd9b72219827c82588b",
      "dae6d7eb70944ab18230a99f43d692fc",
      "82dc4ae4baf34a3cbe73f22fc9514966",
      "5ed9f36d8fcc447eb04890d8143f2207",
      "9f502d455c2b49d5b596c298ef129fc7",
      "5278e3ab01284178a16277ac322e0ffb",
      "edc7e737e0b24237adbb1665953f345c",
      "0e9f8c4ee41f4205814dc750a282328f",
      "baac83f67f5f4e02b1598fc75c0ff09e",
      "8d884a21ba6e4839b86332673f411099",
      "a062882ffd014001b3fb31afa3c063bb",
      "47e2c71e8f0342c3971d169b7d719c2c",
      "7466575752ef4efd9030ce3f41103341",
      "ab82927abab04608903bd56b34f321a5",
      "54bbebb783404434a235f80dd4925616",
      "3b60d7cce9794823a2b37d1e9d4fa3c1",
      "c14ca3142f5f47c998671c5c9f9ada92",
      "76154dc98a8b48f497acf27a352914ee",
      "be4e54eed727452e8424cba08d26f1d2",
      "474339ca29644a42a77edf376255a9b6",
      "155cdd19f8024e61bd9f74826988c0c7",
      "ad6af38745da4b96a23069578a712d4e",
      "460fa212499c47f0b228c1579ee2a04e",
      "42e8c711fffd41e686bf43b4906035af",
      "a5bff2435f5648f79ca6435de3b52b37",
      "5a3e003edbdf44e89e5668dee26d7f8e",
      "bab8be8f63034cfca727831083cda7df",
      "87a19186e056436c8293d60d33020945",
      "64c96c9dd61840ffb26e7c5034501e7b",
      "973cc7454ac341928228b51a7fca9247",
      "2f2c460b53d44662b9f955748538452a",
      "7434f4e1c7664734bdc5f0e277635c2e",
      "22c9f8aaf65d4ffdade42391d53ce270",
      "e1eeeacd927548739b43db23f42aadc9"
     ]
    },
    "id": "yp3mKY-GDbN6",
    "outputId": "8a659e0e-b47d-4e2b-9dc1-c31fb8e75350"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08194336d8242798173f27d1e0a40ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bed6d3a51b43689c781fd170eca5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445b3a0caa694e76977b40dbac8ec24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07344fa63e7c4f7bba2d103033895e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b3cba492f54e4992429f927b7ba9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/691 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e0afe74c7843a181aef4f56336a2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1b592cc46149fc9e5c84bf452e0ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4b740294df4357916853ed99d5025e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b3bb37d62746338f6e81cfe7f80378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae6d7eb70944ab18230a99f43d692fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7466575752ef4efd9030ce3f41103341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e8c711fffd41e686bf43b4906035af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model ready for querying.\n",
      "\n",
      "=== Topic Quality Metrics ===\n",
      "ğŸ§ª Coherence Score (c_v): 0.5826\n",
      "ğŸŒˆ Topic Diversity: 0.4672\n",
      "ğŸ“ Silhouette Score: 0.5386\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS & SETUP ===\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "import scann\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics import silhouette_score, precision_recall_fscore_support\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === SEED FIXING ===\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed_all(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "nltk.download(\"punkt\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# === CLEANING & CONTEXT EXTRACTION (IMPROVED) ===\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):\n",
    "    entity_context_pairs = []\n",
    "    for idx, ents in enumerate(entities_per_chunk):\n",
    "        chunk = clean_text(chunks[idx])\n",
    "        sentences = sent_tokenize(chunk)\n",
    "        for ent in ents:\n",
    "            ent_lower = ent.lower()\n",
    "            matched = False\n",
    "            for i, sent in enumerate(sentences):\n",
    "                if ent_lower in sent:\n",
    "                    context = \" \".join(sentences[max(0, i - 1): i + 2]) if use_multi_sentence else sent.strip()\n",
    "                    enriched = f\"The concept '{ent_lower}' appears in the following context: {context}\"\n",
    "                    entity_context_pairs.append((ent_lower, enriched.strip()))\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                fallback = f\"The concept '{ent_lower}' appears in the following context: {chunk}\"\n",
    "                entity_context_pairs.append((ent_lower, fallback.strip()))\n",
    "    return entity_context_pairs\n",
    "\n",
    "\n",
    "# === TOPIC SEARCHER CLASS (WITH DEDUPLICATION, NOISE FILTERING) ===\n",
    "class AllergyTopicSearcher:\n",
    "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params,\n",
    "                 model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"):\n",
    "        self.chunks = chunks\n",
    "        self.entities_per_chunk = entities_per_chunk\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "        self.umap_params = umap_params\n",
    "        self.hdbscan_params = hdbscan_params\n",
    "\n",
    "        self.topic_model = None\n",
    "        self.topic_metadata = []\n",
    "        self.topic_embeddings = None\n",
    "        self.searcher = None\n",
    "\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        entity_context_pairs = extract_entity_contexts(self.chunks, self.entities_per_chunk)\n",
    "        contextual_texts = [ctx for _, ctx in entity_context_pairs]\n",
    "        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=False)\n",
    "\n",
    "        umap_model = UMAP(**self.umap_params, random_state=SEED)\n",
    "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
    "\n",
    "        self.topic_model = BERTopic(\n",
    "            embedding_model=self.embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            representation_model=KeyBERTInspired(),\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
    "\n",
    "        topic_to_contexts = defaultdict(list)\n",
    "        topic_to_entities = defaultdict(set)\n",
    "        topic_to_embeddings = defaultdict(list)\n",
    "\n",
    "        for i, topic in enumerate(topics):\n",
    "            if topic == -1:\n",
    "                continue  # Skip noisy topics\n",
    "            ent, ctx = entity_context_pairs[i]\n",
    "            topic_to_contexts[topic].append(ctx)\n",
    "            topic_to_entities[topic].add(ent)\n",
    "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
    "\n",
    "        topic_embeddings = []\n",
    "        topic_metadata = []\n",
    "\n",
    "        for topic_id in topic_to_contexts:\n",
    "            emb = topic_to_embeddings[topic_id]\n",
    "            centroid = np.mean(emb, axis=0)\n",
    "            centroid /= np.linalg.norm(centroid) + 1e-10\n",
    "            topic_embeddings.append(centroid)\n",
    "            topic_metadata.append({\n",
    "                \"topic_id\": topic_id,\n",
    "                \"entities\": list(topic_to_entities[topic_id]),\n",
    "                \"sentences\": topic_to_contexts[topic_id],\n",
    "                \"sentence_embeddings\": np.array(emb)\n",
    "            })\n",
    "\n",
    "        # === OPTIONAL: Merge semantically similar topics (cosine sim > 0.95)\n",
    "        deduped_metadata = []\n",
    "        used = set()\n",
    "\n",
    "        for i, emb_i in enumerate(topic_embeddings):\n",
    "            if i in used:\n",
    "                continue\n",
    "            group = [i]\n",
    "            sim_scores = cosine_similarity([emb_i], topic_embeddings)[0]\n",
    "            for j in range(i + 1, len(sim_scores)):\n",
    "                if sim_scores[j] > 0.95:\n",
    "                    group.append(j)\n",
    "                    used.add(j)\n",
    "\n",
    "            merged = {\n",
    "                \"topic_id\": i,\n",
    "                \"sentences\": [],\n",
    "                \"entities\": [],\n",
    "                \"sentence_embeddings\": []\n",
    "            }\n",
    "            for g in group:\n",
    "                merged[\"sentences\"] += topic_metadata[g][\"sentences\"]\n",
    "                merged[\"entities\"] += topic_metadata[g][\"entities\"]\n",
    "                merged[\"sentence_embeddings\"] += list(topic_metadata[g][\"sentence_embeddings\"])\n",
    "\n",
    "            merged[\"sentence_embeddings\"] = np.array(merged[\"sentence_embeddings\"])\n",
    "            merged[\"entities\"] = list(set(merged[\"entities\"]))\n",
    "            deduped_metadata.append(merged)\n",
    "\n",
    "        self.topic_metadata = deduped_metadata\n",
    "        self.topic_embeddings = np.array([\n",
    "            np.mean(m[\"sentence_embeddings\"], axis=0) /\n",
    "            (np.linalg.norm(np.mean(m[\"sentence_embeddings\"], axis=0)) + 1e-10)\n",
    "            for m in deduped_metadata\n",
    "        ])\n",
    "\n",
    "        self.searcher = (\n",
    "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
    "            .tree(num_leaves=10, num_leaves_to_search=5, training_sample_size=len(self.topic_embeddings))\n",
    "            .score_brute_force()\n",
    "            .reorder(5)\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "    import re\n",
    "\n",
    "    def search(self, query, top_k_topics=3, top_k_sents=3):\n",
    "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
    "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
    "\n",
    "        results = []\n",
    "        prefix_pattern = r\"^the concept '.*?' appears in (the following )?context:\\s*\"\n",
    "\n",
    "        for i, idx in enumerate(neighbors):\n",
    "            meta = self.topic_metadata[idx]\n",
    "            topic_score = float(scores[i])\n",
    "\n",
    "            # Deduplicate sentences\n",
    "            seen = set()\n",
    "            cleaned_sentences = []\n",
    "            cleaned_embeddings = []\n",
    "\n",
    "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
    "            # Apply regex to remove beginning prefix\n",
    "                cleaned = re.sub(prefix_pattern, \"\", sent, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        # No duplicates\n",
    "                if cleaned not in seen:\n",
    "                    seen.add(cleaned)\n",
    "                    cleaned_sentences.append(cleaned)\n",
    "                    cleaned_embeddings.append(emb)\n",
    "\n",
    "            if not cleaned_sentences:\n",
    "                continue\n",
    "\n",
    "            emb_array = np.array(cleaned_embeddings)\n",
    "            sims = np.dot(emb_array / np.linalg.norm(emb_array, axis=1, keepdims=True), query_emb)\n",
    "            top_ids = sims.argsort()[::-1][:top_k_sents]\n",
    "\n",
    "            top_sents = [(cleaned_sentences[j], float(sims[j])) for j in top_ids]\n",
    "            results.append({\n",
    "            \"topic_id\": meta[\"topic_id\"],\n",
    "            \"topic_score\": topic_score,\n",
    "            \"entities\": meta[\"entities\"],\n",
    "            \"sentences\": top_sents,\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === METRICS ===\n",
    "def compute_bertopic_coherence(topic_model, topic_metadata, topk=15):\n",
    "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
    "    word_lists = [[word for word, _ in topic] for topic in topics]\n",
    "\n",
    "    texts = []\n",
    "    for meta in topic_metadata:\n",
    "        for s in meta[\"sentences\"]:\n",
    "            tokens = clean_text(s).split()\n",
    "            texts.append(tokens)\n",
    "\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "\n",
    "    cm = CoherenceModel(\n",
    "        topics=word_lists,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\"\n",
    "    )\n",
    "    return cm.get_coherence()\n",
    "\n",
    "\n",
    "def compute_topic_diversity(topic_model, topic_metadata, topk=10):\n",
    "    topic_words = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
    "    unique_words = set(word for topic in topic_words for word, _ in topic)\n",
    "    return len(unique_words) / (len(topic_words) * topk)\n",
    "\n",
    "\n",
    "def compute_silhouette_score_custom(topic_metadata):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    for meta in topic_metadata:\n",
    "        emb = meta[\"sentence_embeddings\"]\n",
    "        if len(emb) < 2:  # skip small clusters\n",
    "            continue\n",
    "        all_embeddings.extend(emb)\n",
    "        all_labels.extend([meta[\"topic_id\"]] * len(emb))\n",
    "\n",
    "    if len(all_embeddings) < 3:\n",
    "        return None\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return silhouette_score(all_embeddings, all_labels, metric=\"cosine\")\n",
    "\n",
    "\n",
    "# === DATASET & INITIALIZATION ===\n",
    "allergy_dataset = {\n",
    "  \"chunks\": [\n",
    "  \"Mr. James H., a 79-year-old male with a long-standing history of cardiovascular and metabolic diseases, was brought to the emergency department due to acute confusion and generalized weakness.\",\n",
    "  \"According to his daughter, he had wandered outside disoriented and was unable to identify family members or recall events from the previous day.\",\n",
    "  \"He has known medical conditions including hypertension, heart failure with reduced ejection fraction, insulin-dependent diabetes mellitus, stage 4 chronic kidney disease, and major depressive disorder.\",\n",
    "  \"His medication regimen includes daily doses of lisinopril, furosemide, carvedilol, insulin glargine, sertraline, and donepezil.\",\n",
    "  \"In the past 24 hours, the patient experienced decreased appetite, an episode of vomiting, and two instances of urinary incontinence.\",\n",
    "  \"Vital signs upon arrival included a blood pressure of 98/56 mmHg, heart rate of 112 beats per minute (irregularly irregular), respiratory rate of 20, oxygen saturation of 93% on room air, and a temperature of 37.6Â°C.\",\n",
    "  \"Physical examination revealed dry mucous membranes, poor skin turgor, moderate lower limb pitting edema, and delayed capillary refill.\",\n",
    "  \"Auscultation of the lungs revealed bilateral basal crackles, and cardiac exam confirmed an irregularly irregular heartbeat without murmurs.\",\n",
    "  \"Neurological examination showed fluctuating attention span but no signs of focal deficits or lateralizing neurological signs.\",\n",
    "  \"Initial lab studies demonstrated an elevated blood glucose of 421 mg/dL, serum sodium of 129 mmol/L, potassium at 5.7 mmol/L, and creatinine at 2.9 mg/dL.\",\n",
    "  \"Serum BUN was elevated at 59 mg/dL and the patientâ€™s anion gap was calculated to be 19, consistent with an anion-gap metabolic acidosis.\",\n",
    "  \"Urinalysis revealed glucosuria and ketonuria without signs of infection, and serum ketones were modestly elevated.\",\n",
    "  \"His HbA1c on record from two months ago was 8.1%, confirming chronic poor glycemic control.\",\n",
    "  \"An ECG showed atrial fibrillation with rapid ventricular response but no acute ischemic changes.\",\n",
    "  \"Chest radiograph revealed cardiomegaly and pulmonary vascular congestion with mild bilateral pleural effusions.\",\n",
    "  \"CT head without contrast was negative for acute infarct, hemorrhage, or mass effect, but showed chronic microvascular changes.\",\n",
    "  \"Given the presentation, he was admitted to the medical ward for acute hyperosmolar hyperglycemic state (HHS) and acute on chronic kidney injury.\",\n",
    "  \"A diagnosis of acute delirium, likely secondary to metabolic derangements, volume depletion, and possible infection, was made.\",\n",
    "  \"He was started on intravenous normal saline, correctional insulin, and telemetry monitoring.\",\n",
    "  \"Furosemide was temporarily held due to volume depletion, and electrolytes were repleted cautiously under nephrology guidance.\",\n",
    "  \"Blood cultures, urine cultures, and chest x-ray were obtained to rule out infection as a potential delirium trigger.\",\n",
    "  \"Empiric antibiotics (ceftriaxone and azithromycin) were initiated pending culture data due to concern for possible aspiration pneumonia.\",\n",
    "  \"On day two, the patientâ€™s mental status began to improve with the resolution of hyperglycemia and normalization of serum osmolarity.\",\n",
    "  \"Repeat labs showed trending down of BUN and creatinine, with sodium rising to 134 and potassium corrected to 4.5 mmol/L.\",\n",
    "  \"He remained in atrial fibrillation and required continuation of beta-blocker therapy to manage ventricular rate.\",\n",
    "  \"Apixaban was continued upon nephrology clearance given acceptable bleeding risk and stable renal function.\",\n",
    "  \"He was evaluated by geriatrics for worsening cognitive decline and safety evaluation related to home discharge.\",\n",
    "  \"PT/OT performed a bedside mobility assessment showing weakness, unsteadiness, and need for moderate assistance with transfers.\",\n",
    "  \"Case management consulted social work regarding home safety, fall prevention, and caregiver support.\",\n",
    "  \"His hospital stay was complicated by mild hypoglycemia on hospital day 3, prompting insulin dose adjustments.\",\n",
    "  \"Nutritional support was consulted to optimize diabetic-friendly, renal-adjusted diet appropriate for age and mobility.\",\n",
    "  \"His depression management was reviewed with psychiatry, and sertraline was continued at 100 mg/day with no suggestion for dose change.\",\n",
    "  \"A Montreal Cognitive Assessment (MoCA) was done revealing a score of 19/30, indicating significant mild cognitive impairment.\",\n",
    "  \"Audiology was recommended due to hearing difficulty interfering with care discussions.\",\n",
    "  \"Oral exam noted poor dentition; dental evaluation was recommended for follow-up to address suspected pain and poor appetite.\",\n",
    "  \"After 6 days, the patient was clinically improved, mentally oriented, and ambulatory with the help of physical therapy.\",\n",
    "  \"Cardiac and renal parameters stabilized sufficiently to permit safe discharge planning.\",\n",
    "  \"The final hospital diagnosis included hyperosmolar hyperglycemic state, volume depletion, acute-on-chronic kidney injury, atrial fibrillation with RVR, and acute delirium.\",\n",
    "  \"He was discharged on a simplified diabetic regimen including basal insulin and correctional sliding scale doses only.\",\n",
    "  \"Apixaban, carvedilol, donepezil, and sertraline were continued with no changes.\",\n",
    "  \"Discharge medication reconciliation included temporary hold of furosemide with plan for outpatient reassessment after fluid status recovery.\",\n",
    "  \"Caregiver role was assumed by daughter who had durable power of attorney and assisted with all home-based needs.\",\n",
    "  \"Written instructions and red flags for hyperglycemia, dizziness, and recurrent confusion were provided.\",\n",
    "  \"A follow-up with his primary care physician, nephrologist, and endocrinologist were scheduled within one and two weeks respectively.\",\n",
    "  \"Home health nursing was arranged to provide medication support and glucose monitoring.\",\n",
    "  \"Nutritionist and physical therapy were ordered for continued improvement in diet and mobility.\",\n",
    "  \"Advanced care planning was briefly discussed including code status, proxy, and end-of-life preferences.\",\n",
    "  \"He is currently listed as full code but family is open to further discussion at next provider visit.\",\n",
    "  \"Patient was grateful for hospital care and expressed motivation to remain active and well at home.\",\n",
    "  \"The overall prognosis remains guarded due to progressive cognitive decline and limited renal reserve.\",\n",
    "  \"Close monitoring for new signs of decompensation or medication nonadherence was advised.\",\n",
    "  \"Pulmonology follow-up was discussed due to prior mild restrictive spirometry suggestive of early interstitial lung disease.\",\n",
    "  \"Family history reveals mother died of complications from dementia and father from ischemic stroke.\",\n",
    "  \"No reported use of tobacco, alcohol, or recreational drugs throughout his life.\",\n",
    "  \"Lives in a single-story home with grab bars and minimal clutter, although risks for falls still persist.\",\n",
    "  \"Wears eyeglasses but rarely uses his hearing aids, sometimes leading to miscommunication or withdrawal.\",\n",
    "  \"History of previous admission 1 year ago for pneumonia requiring IV antibiotics and 6-day hospitalization.\",\n",
    "  \"Documentation from that admission revealed transient delirium and impaired oral intake similar to current episode.\",\n",
    "  \"Goals-of-care conversations were initiated during this admission but deferred for primary care setting follow-up.\",\n",
    "  \"Social isolation remains a concern, especially since his wife passed away 3 years ago.\",\n",
    "  \"Patient receives Meals on Wheels but misses many meal deliveries due to lack of reliable caregiver at times.\",\n",
    "  \"Transportation to medical appointments is provided by his daughter, who balances full-time work responsibilities.\",\n",
    "  \"No current enrollment in adult day health programs; options discussed with case management on discharge.\",\n",
    "  \"Insurance covers home nursing and outpatient labs but does not cover custodial care.\",\n",
    "  \"Patient was educated about Medicare Advantage benefits and encouraged to review covered services with the plan coordinator.\",\n",
    "  \"He was also reminded of the importance of daily glucose checks and hydration in summer months.\",\n",
    "  \"Foot exam demonstrated mild calluses and intact sensation; he denies new ulcers or foot injuries.\",\n",
    "  \"Vaccination status confirmed: received influenza and COVID vaccines last fall, but is due for pneumococcal booster.\",\n",
    "  \"Dentition issues may be contributing to decreased intake; dental clinic referral was sent through EHR.\",\n",
    "  \"Assistive device for walking was provided (four-point cane) after physical therapy evaluation.\",\n",
    "  \"Contact dermatitis on legs due to prolonged pressure and incontinence was treated with barrier cream.\",\n",
    "  \"Skin care and bathing guidance were reviewed with family nursing staff prior to discharge.\",\n",
    "  \"Patient verbalized understanding of all discharge instructions with support from daughter.\",\n",
    "  \"Hospital team closed chart after discussing active problems list, response to therapy, and continued plan.\",\n",
    "  \"Patient left the hospital in a wheelchair, accompanied by family, and appeared in good spirits.\",\n",
    "  \"The full discharge plan was documented and faxed to his primary provider for continuity of care.\",\n",
    "  \"Medication reconciliation showed no potential drug interactions or allergy mismatches.\",\n",
    "  \"He was warned against use of NSAIDs due to underlying CKD and risk of acute worsening.\",\n",
    "  \"Hydration goals of at least 1.5 liters per day were set; urination logs and symptom review were encouraged.\",\n",
    "  \"Emergency instructions included what to do in case of unresponsiveness, low blood glucose, sudden confusion, or chest pain.\",\n",
    "  \"Digital blood glucose monitor was reviewed at bedside; daughter demonstrated appropriate calibration and use.\",\n",
    "  \"All prescriptions were sent electronically to their local pharmacy located eight blocks from their home.\",\n",
    "  \"Patient prefers morning appointments due to increased alertness and energy early in the day.\",\n",
    "  \"A follow-up MoCA test was recommended in 3â€“6 months to assess cognitive trajectory.\",\n",
    "  \"Updated advance directives were placed in the chart and a copy was given to the daughter.\",\n",
    "  \"Fall prevention strategies were emphasized including appropriate lighting, footwear, and scheduled ambulation.\",\n",
    "  \"Use of automatic pill organizers was encouraged to improve adherence across complex medication schedules.\",\n",
    "  \"Daily weights will be tracked at home to monitor for unexpected fluid retention or heart failure.\",\n",
    "  \"Serum creatinine will be rechecked in one week given borderline rise during admission.\",\n",
    "  \"A nephrology note was sent to alert about potential need for long-term planning if GFR continues to decline.\",\n",
    "  \"Patient qualifies for shared savings Medicare model and was assigned a care coordinator temporarily.\",\n",
    "  \"Patient support group information was handed out, including resources for caregivers.\",\n",
    "  \"He is open to exploring telehealth check-ins for medication titration and early symptom triage.\",\n",
    "  \"Daughter confirmed she has portal access to review labs and visit summaries on his behalf.\",\n",
    "  \"Patient and daughter expressed appreciation for the hospital care coordination team.\",\n",
    "  \"Case closed with summary of diagnosis, medications, specialists involved, and plan for 30-day transitional care.\",\n",
    "  \"Status post discharge: stable, safe for home, alert and oriented with supervision.\"\n",
    "]\n",
    "\n",
    ",\n",
    "\"entities\":[\n",
    "  [\"confusion\", \"weakness\", \"cardiovascular\", \"metabolic\"],\n",
    "  [\"disorientation\", \"memory\"],\n",
    "  [\"hypertension\", \"failure\", \"diabetes\", \"kidney\", \"depression\"],\n",
    "  [\"lisinopril\", \"furosemide\", \"carvedilol\", \"insulin\", \"sertraline\", \"donepezil\"],\n",
    "  [\"appetite\", \"vomiting\", \"incontinence\"],\n",
    "  [\"pressure\", \"rate\", \"rhythm\", \"respiration\", \"saturation\", \"temperature\"],\n",
    "  [\"mucosa\", \"turgor\", \"edema\", \"refill\"],\n",
    "  [\"crackles\", \"heartbeat\", \"murmurs\"],\n",
    "  [\"attention\", \"deficits\"],\n",
    "  [\"glucose\", \"sodium\", \"potassium\", \"creatinine\"],\n",
    "  [\"bun\", \"acidosis\"],\n",
    "  [\"glucosuria\", \"ketonuria\", \"ketones\"],\n",
    "  [\"hba1c\", \"control\"],\n",
    "  [\"ecg\", \"fibrillation\", \"response\", \"ischemia\"],\n",
    "  [\"cardiomegaly\", \"congestion\", \"effusions\"],\n",
    "  [\"infarct\", \"hemorrhage\", \"microvascular\"],\n",
    "  [\"hyperglycemia\", \"injury\"],\n",
    "  [\"delirium\", \"derangements\", \"infection\"],\n",
    "  [\"saline\", \"insulin\", \"telemetry\"],\n",
    "  [\"furosemide\", \"depletion\", \"electrolytes\"],\n",
    "  [\"cultures\", \"infection\"],\n",
    "  [\"antibiotics\", \"ceftriaxone\", \"azithromycin\", \"pneumonia\"],\n",
    "  [\"status\", \"hyperglycemia\", \"osmolarity\"],\n",
    "  [\"bun\", \"creatinine\", \"sodium\", \"potassium\"],\n",
    "  [\"fibrillation\", \"rate\", \"blocker\"],\n",
    "  [\"apixaban\", \"function\", \"bleeding\"],\n",
    "  [\"geriatrics\", \"cognition\"],\n",
    "  [\"pt\", \"ot\", \"mobility\", \"weakness\", \"transfers\"],\n",
    "  [\"safety\", \"falls\"],\n",
    "  [\"hypoglycemia\", \"insulin\"],\n",
    "  [\"nutrition\", \"diet\"],\n",
    "  [\"depression\", \"psychiatry\", \"sertraline\"],\n",
    "  [\"moca\", \"impairment\"],\n",
    "  [\"audiology\", \"hearing\"],\n",
    "  [\"dentition\", \"pain\"],\n",
    "  [\"therapy\", \"ambulation\"],\n",
    "  [\"parameters\"],\n",
    "  [\"hyperglycemia\", \"depletion\", \"injury\", \"fibrillation\", \"delirium\"],\n",
    "  [\"regimen\", \"insulin\"],\n",
    "  [\"apixaban\", \"carvedilol\", \"donepezil\", \"sertraline\"],\n",
    "  [\"reconciliation\", \"furosemide\"],\n",
    "  [\"power\"],\n",
    "  [\"hyperglycemia\", \"dizziness\", \"confusion\"],\n",
    "  [\"nephrologist\", \"endocrinologist\"],\n",
    "  [\"nursing\", \"glucose\"],\n",
    "  [\"nutritionist\", \"therapy\"],\n",
    "  [\"planning\", \"status\", \"proxy\"],\n",
    "  [\"code\"],\n",
    "  [\"prognosis\", \"cognition\", \"reserve\"],\n",
    "  [\"monitoring\", \"decompensation\", \"adherence\"],\n",
    "  [\"pulmonology\", \"spirometry\", \"disease\"],\n",
    "  [\"dementia\", \"stroke\"],\n",
    "  [\"tobacco\", \"alcohol\", \"drugs\"],\n",
    "  [\"falls\"],\n",
    "  [\"hearing\"],\n",
    "  [\"pneumonia\", \"antibiotics\"],\n",
    "  [\"delirium\"],\n",
    "  [\"conversations\"],\n",
    "  [\"isolation\"],\n",
    "  [\"meals\"],\n",
    "  [\"transportation\"],\n",
    "  [\"enrollment\"],\n",
    "  [\"insurance\", \"nursing\", \"labs\"],\n",
    "  [\"medicare\"],\n",
    "  [\"glucose\", \"hydration\"],\n",
    "  [\"exam\", \"calluses\", \"ulcers\"],\n",
    "  [\"vaccination\", \"influenza\", \"covid\", \"booster\"],\n",
    "  [\"dentition\", \"referral\"],\n",
    "  [\"cane\"],\n",
    "  [\"dermatitis\", \"cream\"],\n",
    "  [\"skin\"],\n",
    "  [\"instructions\"],\n",
    "  [\"problems\", \"therapy\", \"plan\"],\n",
    "  [\"wheelchair\"],\n",
    "  [\"continuity\"],\n",
    "  [\"reconciliation\", \"interactions\", \"allergies\"],\n",
    "  [\"nsaids\"],\n",
    "  [\"hydration\", \"urination\", \"symptoms\"],\n",
    "  [\"instructions\", \"glucose\", \"confusion\", \"pain\"],\n",
    "  [\"monitor\", \"calibration\"],\n",
    "  [\"prescriptions\", \"pharmacy\"],\n",
    "  [\"appointments\", \"alertness\", \"energy\"],\n",
    "  [\"moca\"],\n",
    "  [\"directives\"],\n",
    "  [\"prevention\", \"lighting\", \"footwear\", \"ambulation\"],\n",
    "  [\"organizer\", \"adherence\"],\n",
    "  [\"weight\", \"retention\", \"failure\"],\n",
    "  [\"creatinine\"],\n",
    "  [\"nephrology\", \"gfr\"],\n",
    "  [\"medicare\", \"coordinator\"],\n",
    "  [\"group\", \"caregivers\"],\n",
    "  [\"telehealth\", \"titration\", \"triage\"],\n",
    "  [\"portal\", \"labs\", \"summaries\"],\n",
    "  [\"coordination\"],\n",
    "  [\"diagnosis\", \"medications\", \"specialists\", \"care\"],\n",
    "  [\"discharge\", \"supervision\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "best_umap = {\"n_neighbors\": 5, \"n_components\": 5, \"min_dist\":0.25,  \"metric\": \"cosine\"}\n",
    "best_hdbscan = {\"min_cluster_size\": 2, \"min_samples\": 1, \"metric\": \"euclidean\"}\n",
    "\n",
    "searcher = AllergyTopicSearcher(\n",
    "    chunks=allergy_dataset[\"chunks\"],\n",
    "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
    "    umap_params=best_umap,\n",
    "    hdbscan_params=best_hdbscan,\n",
    "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    ")\n",
    "print(\"âœ… Model ready for querying.\")\n",
    "# print(\"\\n=== ğŸ§  Generated Topics and Entities ===\")\n",
    "# for meta in searcher.topic_metadata:\n",
    "#     topic_id = meta[\"topic_id\"]\n",
    "#     entities = \", \".join(meta[\"entities\"])\n",
    "#     print(f\"ğŸ”¹ Topic ID: {topic_id} â€” Entities: {entities}\")\n",
    "\n",
    "# === METRICS ===\n",
    "coherence = compute_bertopic_coherence(searcher.topic_model, searcher.topic_metadata, topk=15)\n",
    "diversity = compute_topic_diversity(searcher.topic_model, searcher.topic_metadata, topk=10)\n",
    "sil_score = compute_silhouette_score_custom(searcher.topic_metadata)\n",
    "\n",
    "print(\"\\n=== Topic Quality Metrics ===\")\n",
    "print(f\"ğŸ§ª Coherence Score (c_v): {coherence:.4f}\")\n",
    "print(f\"ğŸŒˆ Topic Diversity: {diversity:.4f}\")\n",
    "if sil_score is not None:\n",
    "    print(f\"ğŸ“ Silhouette Score: {sil_score:.4f}\")\n",
    "else:\n",
    "    print(\"ğŸ“ Silhouette Score: Not applicable.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEgBQhI0AqBT"
   },
   "source": [
    "# Topic labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrDpMCeb_ziL"
   },
   "source": [
    "# Data read from the big query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6uTH8gCA5HHK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def normalize_vectors(vectors: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    # Avoid division by zero by replacing zeros with small epsilon\n",
    "    norms = np.where(norms == 0, 1e-10, norms)\n",
    "    return vectors / norms\n",
    "\n",
    "\n",
    "# === 1. Load CUI definitions and embeddings from BigQuery ===\n",
    "project_id = \"YOUR_PROJECT_ID\"\n",
    "dataset_id = \"YOUR_DATASET\"\n",
    "table_id = \"YOUR_TABLE_NAME\"\n",
    "table_ref = f\"`{project_id}.{dataset_id}.{table_id}`\"\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "query = f\"SELECT cui, definition, embedding FROM {table_ref}\"\n",
    "df_cui = client.query(query).to_dataframe()\n",
    "\n",
    "# Stack all embeddings into np.array of shape (num_concepts, embedding_dim)\n",
    "cui_embeddings = np.vstack(df_cui['embedding'].values).astype(float)\n",
    "\n",
    "# Normalize CUI embeddings here explicitly\n",
    "cui_embeddings = normalize_vectors(cui_embeddings)\n",
    "\n",
    "cui_defs = df_cui['definition'].tolist()\n",
    "\n",
    "top_k = 3  # Number of top candidate labels for each topic\n",
    "\n",
    "rows = []\n",
    "\n",
    "# === 2. LOOP OVER TOPICS from your AllergyTopicSearcher instance ===\n",
    "# Assumes searcher.topic_metadata elements with keys:\n",
    "# 'topic_id', 'entities', 'sentence_embeddings' (np.array, shape=(num_entities, embedding_dim))\n",
    "for meta in searcher.topic_metadata:\n",
    "    topic_id = meta[\"topic_id\"]\n",
    "    entities = meta[\"entities\"]\n",
    "\n",
    "    # Normalize all sentence embeddings before averaging\n",
    "    sentence_embeddings_normalized = normalize_vectors(meta[\"sentence_embeddings\"])\n",
    "\n",
    "    # Compute average topic embedding and normalize again\n",
    "    topic_emb = np.mean(sentence_embeddings_normalized, axis=0)\n",
    "    topic_emb /= (np.linalg.norm(topic_emb) + 1e-10)\n",
    "\n",
    "    # Compute cosine similarity: topic embedding vs all CUI embeddings (dot product since normalized)\n",
    "    topic_term_sims = np.dot(cui_embeddings, topic_emb)\n",
    "\n",
    "    # Clip similarity scores to [-1, 1]\n",
    "    topic_term_sims = np.clip(topic_term_sims, -1.0, 1.0)\n",
    "\n",
    "    # Select top-k candidate label indices sorted by similarity descending\n",
    "    top_indices = topic_term_sims.argsort()[::-1][:top_k]\n",
    "\n",
    "    candidate_embeddings = cui_embeddings[top_indices]\n",
    "    candidate_defs = [cui_defs[i] for i in top_indices]\n",
    "    candidate_similarities = topic_term_sims[top_indices]\n",
    "\n",
    "    # Compute pairwise cosine similarity among top-k candidate labels\n",
    "    pairwise_sims = cosine_similarity(candidate_embeddings)\n",
    "    off_diag_mask = ~np.eye(pairwise_sims.shape[0], dtype=bool)\n",
    "    off_diagonal_sims = pairwise_sims[off_diag_mask]\n",
    "    topic_label_similarity = round(float(np.mean(off_diagonal_sims)), 2) if off_diagonal_sims.size > 0 else None\n",
    "\n",
    "    # Assign each entity to the closest candidate label based on similarity\n",
    "    entity_assignments = []\n",
    "    entity_scores = []\n",
    "\n",
    "    if entities and len(meta[\"sentence_embeddings\"]) == len(entities):\n",
    "        # Normalize entity embeddings as well\n",
    "        entity_embeddings_normalized = normalize_vectors(meta[\"sentence_embeddings\"])\n",
    "\n",
    "        # Compute similarity matrix (entities x candidate labels)\n",
    "        sims_entities_candidates = np.dot(entity_embeddings_normalized, candidate_embeddings.T)\n",
    "\n",
    "        # Clip similarity scores\n",
    "        sims_entities_candidates = np.clip(sims_entities_candidates, -1.0, 1.0)\n",
    "\n",
    "        for i, ent in enumerate(entities):\n",
    "            best_idx = sims_entities_candidates[i].argmax()\n",
    "            best_score = float(sims_entities_candidates[i][best_idx])\n",
    "            assigned_label = candidate_defs[best_idx]\n",
    "            entity_assignments.append({\n",
    "                \"entity\": ent,\n",
    "                \"assigned_label_definition\": assigned_label,\n",
    "                \"sim_score\": round(best_score, 2)\n",
    "            })\n",
    "            entity_scores.append(best_score)\n",
    "\n",
    "        entity_label_score = round(float(np.mean(entity_scores)), 2) if entity_scores else None\n",
    "    else:\n",
    "        entity_label_score = None\n",
    "        entity_assignments = []\n",
    "\n",
    "    # Prepare topic-level label list with definition and similarity\n",
    "    topic_level_labels = [(candidate_defs[i], round(float(candidate_similarities[i]), 2)) for i in range(len(top_indices))]\n",
    "\n",
    "    rows.append({\n",
    "        \"topic_id\": topic_id,\n",
    "        \"entities\": entities,\n",
    "        \"topic_level_labels\": topic_level_labels,  # list of (definition, similarity)\n",
    "        \"topic_Avg_label_similarity\": topic_label_similarity,\n",
    "        \"entity_level_assignments\": entity_assignments,  # list of dicts {entity, assigned_label_definition, sim_score}\n",
    "        \"entity_Avg_label_score\": entity_label_score\n",
    "    })\n",
    "\n",
    "# === 3. Create pandas DataFrame and display ===\n",
    "df = pd.DataFrame(rows)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "277IL7Gz1bwJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
