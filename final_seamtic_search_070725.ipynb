{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGNQB_nx_UdV",
        "outputId": "623d1438-6690-4775-c308-6e3193a06c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting scann\n",
            "  Downloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.8)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading bertopic-0.17.0-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scann, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bertopic\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bertopic-0.17.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 scann-1.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.8)\n",
            "Requirement already satisfied: scann in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy sentence-transformers bertopic hdbscan nltk scann\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "!pip install sentence-transformers bertopic hdbscan umap-learn scann nltk datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # === IMPORTS ===\n",
        "# import os\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import nltk\n",
        "# import logging\n",
        "\n",
        "# from collections import defaultdict\n",
        "# from nltk.tokenize import sent_tokenize\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from bertopic import BERTopic\n",
        "# from bertopic.representation import KeyBERTInspired\n",
        "# from hdbscan import HDBSCAN\n",
        "# from umap import UMAP\n",
        "# import scann\n",
        "\n",
        "# # === ENVIRONMENT SETUP ===\n",
        "# SEED = 42\n",
        "# np.random.seed(SEED)\n",
        "# random.seed(SEED)\n",
        "# os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "# torch.manual_seed(SEED)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed_all(SEED)\n",
        "# torch.use_deterministic_algorithms(True)\n",
        "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "# logger = logging.getLogger(__name__)\n",
        "# nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# # === CLASS FOR TOPIC SEARCHER ===\n",
        "# class AllergyTopicSearcher:\n",
        "#     def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params, model_name=\"all-MiniLM-L6-v2\"):\n",
        "#         self.chunks = chunks\n",
        "#         self.entities_per_chunk = entities_per_chunk\n",
        "#         self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "#         self.umap_params = umap_params\n",
        "#         self.hdbscan_params = hdbscan_params\n",
        "\n",
        "#         self.topic_model = None\n",
        "#         self.topic_metadata = []\n",
        "#         self.topic_embeddings = None\n",
        "#         self.searcher = None\n",
        "\n",
        "#         self._prepare()\n",
        "\n",
        "#     def _prepare(self):\n",
        "#         entity_context_pairs = []\n",
        "\n",
        "#         for idx, ents in enumerate(self.entities_per_chunk):\n",
        "#             chunk = self.chunks[idx].lower()\n",
        "#             sentences = sent_tokenize(chunk)\n",
        "#             for ent in ents:\n",
        "#                 ent_lower = ent.lower()\n",
        "#                 for sent in sentences:\n",
        "#                     if ent_lower in sent:\n",
        "#                         entity_context_pairs.append((ent_lower, sent.strip()))\n",
        "#                         break\n",
        "\n",
        "#         if not entity_context_pairs:\n",
        "#             raise ValueError(\"No entity-context pairs extracted!\")\n",
        "\n",
        "#         # Embed entity-contexts\n",
        "#         contextual_texts = [f\"{ent}: {context}\" for ent, context in entity_context_pairs]\n",
        "#         contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=True)\n",
        "\n",
        "#         # Topic Modeling\n",
        "#         umap_model = UMAP(**self.umap_params)\n",
        "#         hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
        "\n",
        "#         self.topic_model = BERTopic(\n",
        "#             embedding_model=self.embedding_model,\n",
        "#             umap_model=umap_model,\n",
        "#             hdbscan_model=hdbscan_model,\n",
        "#             representation_model=KeyBERTInspired(),\n",
        "#             calculate_probabilities=True,\n",
        "#             verbose=False,\n",
        "#         )\n",
        "\n",
        "#         topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
        "\n",
        "#         # Metadata aggregation\n",
        "#         topic_to_contexts = defaultdict(list)\n",
        "#         topic_to_entities = defaultdict(set)\n",
        "#         topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "#         for i, topic in enumerate(topics):\n",
        "#             ent, context = entity_context_pairs[i]\n",
        "#             topic_to_contexts[topic].append(context)\n",
        "#             topic_to_entities[topic].add(ent)\n",
        "#             topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "#         topic_embeddings = []\n",
        "#         topic_metadata = []\n",
        "\n",
        "#         for topic_id in topic_to_contexts:\n",
        "#             embeddings = topic_to_embeddings[topic_id]\n",
        "#             mean_emb = np.mean(embeddings, axis=0)\n",
        "#             mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "#             topic_embeddings.append(mean_emb)\n",
        "#             topic_metadata.append({\n",
        "#                 \"topic_id\": topic_id,\n",
        "#                 \"entities\": list(topic_to_entities[topic_id]),\n",
        "#                 \"sentences\": topic_to_contexts[topic_id],\n",
        "#                 \"sentence_embeddings\": np.array(embeddings)\n",
        "#             })\n",
        "\n",
        "#         self.topic_embeddings = np.array(topic_embeddings)\n",
        "#         self.topic_metadata = topic_metadata\n",
        "\n",
        "#         if len(self.topic_embeddings) < 1:\n",
        "#             raise RuntimeError(\"No topic embeddings to index.\")\n",
        "\n",
        "#         num_clusters = min(len(self.topic_embeddings), 3)\n",
        "#         self.searcher = (\n",
        "#             scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "#             .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "#             .score_brute_force()\n",
        "#             .reorder(3)\n",
        "#             .build()\n",
        "#         )\n",
        "\n",
        "#     def search(self, query, top_k_topics=1, top_k_sents=1):\n",
        "#         query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "#         neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "#         results = []\n",
        "#         for idx in neighbors:\n",
        "#             meta = self.topic_metadata[idx]\n",
        "#             seen = set()\n",
        "#             unique_sentences = []\n",
        "#             unique_embeddings = []\n",
        "\n",
        "#             for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "#                 if sent not in seen:\n",
        "#                     seen.add(sent)\n",
        "#                     unique_sentences.append(sent)\n",
        "#                     unique_embeddings.append(emb)\n",
        "\n",
        "#             sent_embs = np.array(unique_embeddings)\n",
        "#             sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "#             sims = np.dot(sent_embs_norm, query_emb)\n",
        "#             top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "#             top_sents = [(unique_sentences[i], sims[i]) for i in top_indices]\n",
        "\n",
        "#             results.append({\n",
        "#                 \"topic_id\": meta[\"topic_id\"],\n",
        "#                 \"entities\": meta[\"entities\"],\n",
        "#                 \"sentences\": top_sents,\n",
        "#             })\n",
        "\n",
        "#         return results\n",
        "\n",
        "\n",
        "# # === DATASET ===\n",
        "# allergy_dataset = {\n",
        "#     \"name\": \"Allergy Dataset\",\n",
        "#     \"chunks\": [\n",
        "#         \"Patient has peanut allergy causing hives and swelling. Anaphylaxis noted once during a reaction.\",\n",
        "#         \"Allergic rhinitis, or hay fever, results from exposure to pollen, dust, or pet dander.\",\n",
        "#         \"Severe anaphylaxis symptoms require immediate treatment with epinephrine.\",\n",
        "#         \"Food allergies to milk and eggs can cause skin reactions like urticaria and eczema.\",\n",
        "#         \"Cold weather does not cause allergy symptoms in this patient.\"\n",
        "#     ],\n",
        "#     \"entities\": [\n",
        "#         [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "#         [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "#         [\"anaphylaxis\", \"epinephrine\", \"treatment\"],\n",
        "#         [\"food allergies\", \"milk\", \"eggs\", \"urticaria\", \"eczema\"],\n",
        "#         [\"cold weather\", \"allergy symptoms\"]\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "# # === BEST PARAMETERS ===\n",
        "# best_umap = {\"n_neighbors\": 5, \"n_components\": 5, \"min_dist\": 0.1, \"metric\": \"cosine\"}\n",
        "# best_hdbscan = {\"min_cluster_size\": 2, \"min_samples\": 1, \"metric\": \"euclidean\"}\n",
        "\n",
        "# # === INITIALIZE SEARCHER ===\n",
        "# print(\"Preparing Allergy Topic Searcher...\")\n",
        "# searcher = AllergyTopicSearcher(\n",
        "#     chunks=allergy_dataset[\"chunks\"],\n",
        "#     entities_per_chunk=allergy_dataset[\"entities\"],\n",
        "#     umap_params=best_umap,\n",
        "#     hdbscan_params=best_hdbscan,\n",
        "# )\n",
        "# print(\"✅ Model ready for querying.\")\n",
        "\n",
        "# # === QUERY LOOP ===\n",
        "# print(\"\\n=== Allergy Topic Search ===\")\n",
        "# while True:\n",
        "#     query = input(\"\\nEnter a query (or type 'exit' to quit): \").strip()\n",
        "#     if query.lower() in {\"exit\", \"quit\"}:\n",
        "#         print(\"Goodbye!\")\n",
        "#         break\n",
        "\n",
        "#     results = searcher.search(query, top_k_topics=1, top_k_sents=3)\n",
        "#     print(f\"\\n🔎 Top results for: '{query}'\")\n",
        "#     for res in results:\n",
        "#         print(f\"🧠 Topic ID: {res['topic_id']}\")\n",
        "#         print(f\"🔗 Related Entities: {', '.join(res['entities'])}\")\n",
        "#         for sent, _ in res[\"sentences\"]:\n",
        "#             print(f\"✓ {sent}\")\n"
      ],
      "metadata": {
        "id": "eAxcf07N_a9X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTS ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "# === ENVIRONMENT SETUP ===\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# === CLASS FOR TOPIC SEARCHER ===\n",
        "class AllergyTopicSearcher:\n",
        "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.chunks = chunks\n",
        "        self.entities_per_chunk = entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "        self.umap_params = umap_params\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        entity_context_pairs = []\n",
        "\n",
        "        for idx, ents in enumerate(self.entities_per_chunk):\n",
        "            chunk = self.chunks[idx].lower()\n",
        "            sentences = sent_tokenize(chunk)\n",
        "            for ent in ents:\n",
        "                ent_lower = ent.lower()\n",
        "                for sent in sentences:\n",
        "                    if ent_lower in sent:\n",
        "                        entity_context_pairs.append((ent_lower, sent.strip()))\n",
        "                        break\n",
        "\n",
        "        if not entity_context_pairs:\n",
        "            raise ValueError(\"No entity-context pairs extracted!\")\n",
        "\n",
        "        # Embed entity-contexts\n",
        "        contextual_texts = [f\"{ent}: {context}\" for ent, context in entity_context_pairs]\n",
        "        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=True)\n",
        "\n",
        "        # Topic Modeling\n",
        "        umap_model = UMAP(**self.umap_params)\n",
        "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
        "\n",
        "        self.topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
        "\n",
        "        # Metadata aggregation\n",
        "        topic_to_contexts = defaultdict(list)\n",
        "        topic_to_entities = defaultdict(set)\n",
        "        topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "        for i, topic in enumerate(topics):\n",
        "            ent, context = entity_context_pairs[i]\n",
        "            topic_to_contexts[topic].append(context)\n",
        "            topic_to_entities[topic].add(ent)\n",
        "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id in topic_to_contexts:\n",
        "            embeddings = topic_to_embeddings[topic_id]\n",
        "            mean_emb = np.mean(embeddings, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": list(topic_to_entities[topic_id]),\n",
        "                \"sentences\": topic_to_contexts[topic_id],\n",
        "                \"sentence_embeddings\": np.array(embeddings)\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "        # === Added print: topics and their entities ===\n",
        "        print(\"\\n=== Topics and Associated Entities ===\")\n",
        "        for meta in self.topic_metadata:\n",
        "            print(f\"Topic ID: {meta['topic_id']}, Entities: {', '.join(meta['entities'])}\")\n",
        "\n",
        "        if len(self.topic_embeddings) < 1:\n",
        "            raise RuntimeError(\"No topic embeddings to index.\")\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 3)\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=1):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        results = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            seen = set()\n",
        "            unique_sentences = []\n",
        "            unique_embeddings = []\n",
        "\n",
        "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "                if sent not in seen:\n",
        "                    seen.add(sent)\n",
        "                    unique_sentences.append(sent)\n",
        "                    unique_embeddings.append(emb)\n",
        "\n",
        "            sent_embs = np.array(unique_embeddings)\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            top_sents = [(unique_sentences[i], sims[i]) for i in top_indices]\n",
        "\n",
        "            results.append({\n",
        "                \"topic_id\": meta[\"topic_id\"],\n",
        "                \"entities\": meta[\"entities\"],\n",
        "                \"sentences\": top_sents,\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# === DATASET ===\n",
        "allergy_dataset = {\n",
        "    \"name\": \"Allergy Dataset\",\n",
        "    \"chunks\": [\n",
        "        \"Patient has peanut allergy causing hives and swelling. Anaphylaxis noted once during a reaction.\",\n",
        "        \"Allergic rhinitis, or hay fever, results from exposure to pollen, dust, or pet dander.\",\n",
        "        \"Severe anaphylaxis symptoms require immediate treatment with epinephrine.\",\n",
        "        \"Food allergies to milk and eggs can cause skin reactions like urticaria and eczema.\",\n",
        "        \"Cold weather does not cause allergy symptoms in this patient.\"\n",
        "    ],\n",
        "    \"entities\": [\n",
        "        [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "        [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "        [\"anaphylaxis\", \"epinephrine\", \"treatment\"],\n",
        "        [\"food allergies\", \"milk\", \"eggs\", \"urticaria\", \"eczema\"],\n",
        "        [\"cold weather\", \"allergy symptoms\"]\n",
        "    ]\n",
        "}\n",
        "\n",
        "# === BEST PARAMETERS ===\n",
        "best_umap = {\"n_neighbors\": 5, \"n_components\": 5, \"min_dist\": 0.1, \"metric\": \"cosine\"}\n",
        "best_hdbscan = {\"min_cluster_size\": 2, \"min_samples\": 1, \"metric\": \"euclidean\"}\n",
        "\n",
        "# === INITIALIZE SEARCHER ===\n",
        "print(\"Preparing Allergy Topic Searcher...\")\n",
        "searcher = AllergyTopicSearcher(\n",
        "    chunks=allergy_dataset[\"chunks\"],\n",
        "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
        "    umap_params=best_umap,\n",
        "    hdbscan_params=best_hdbscan,\n",
        ")\n",
        "print(\"✅ Model ready for querying.\")\n",
        "\n",
        "# === QUERY LOOP ===\n",
        "print(\"\\n=== Allergy Topic Search ===\")\n",
        "while True:\n",
        "    query = input(\"\\nEnter a query (or type 'exit' to quit): \").strip()\n",
        "    if query.lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    results = searcher.search(query, top_k_topics=1, top_k_sents=3)\n",
        "    print(f\"\\n🔎 Top results for: '{query}'\")\n",
        "    for res in results:\n",
        "        print(f\"🧠 Topic ID: {res['topic_id']}\")\n",
        "        print(f\"🔗 Related Entities: {', '.join(res['entities'])}\")\n",
        "        for sent, _ in res[\"sentences\"]:\n",
        "            print(f\"✓ {sent}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdDY5WrNJvR6",
        "outputId": "4e07af5c-509e-4e28-a8b3-36a1e7b70fa7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing Allergy Topic Searcher...\n",
            "\n",
            "=== Topics and Associated Entities ===\n",
            "Topic ID: 3, Entities: swelling, peanut allergy, hives\n",
            "Topic ID: 2, Entities: treatment, anaphylaxis, epinephrine\n",
            "Topic ID: 0, Entities: allergic rhinitis, pollen, dust, allergy symptoms, cold weather, pet dander, hay fever\n",
            "Topic ID: 1, Entities: eggs, eczema, food allergies, urticaria, milk\n",
            "✅ Model ready for querying.\n",
            "\n",
            "=== Allergy Topic Search ===\n",
            "\n",
            "Enter a query (or type 'exit' to quit): food\n",
            "\n",
            "🔎 Top results for: 'food'\n",
            "🧠 Topic ID: 1\n",
            "🔗 Related Entities: eggs, eczema, food allergies, urticaria, milk\n",
            "✓ food allergies to milk and eggs can cause skin reactions like urticaria and eczema.\n",
            "\n",
            "Enter a query (or type 'exit' to quit): symptoms \n",
            "\n",
            "🔎 Top results for: 'symptoms'\n",
            "🧠 Topic ID: 0\n",
            "🔗 Related Entities: allergic rhinitis, pollen, dust, allergy symptoms, cold weather, pet dander, hay fever\n",
            "✓ cold weather does not cause allergy symptoms in this patient.\n",
            "✓ allergic rhinitis, or hay fever, results from exposure to pollen, dust, or pet dander.\n",
            "\n",
            "Enter a query (or type 'exit' to quit): causes of allergy\n",
            "\n",
            "🔎 Top results for: 'causes of allergy'\n",
            "🧠 Topic ID: 1\n",
            "🔗 Related Entities: eggs, eczema, food allergies, urticaria, milk\n",
            "✓ food allergies to milk and eggs can cause skin reactions like urticaria and eczema.\n",
            "\n",
            "Enter a query (or type 'exit' to quit): exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}