{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGNQB_nx_UdV",
        "outputId": "623d1438-6690-4775-c308-6e3193a06c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting scann\n",
            "  Downloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.8)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading bertopic-0.17.0-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scann, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bertopic\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bertopic-0.17.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 scann-1.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.8)\n",
            "Requirement already satisfied: scann in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy sentence-transformers bertopic hdbscan nltk scann\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "!pip install sentence-transformers bertopic hdbscan umap-learn scann nltk datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # === IMPORTS ===\n",
        "# import os\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import nltk\n",
        "# import logging\n",
        "\n",
        "# from collections import defaultdict\n",
        "# from nltk.tokenize import sent_tokenize\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from bertopic import BERTopic\n",
        "# from bertopic.representation import KeyBERTInspired\n",
        "# from hdbscan import HDBSCAN\n",
        "# from umap import UMAP\n",
        "# import scann\n",
        "\n",
        "# # === ENVIRONMENT SETUP ===\n",
        "# SEED = 42\n",
        "# np.random.seed(SEED)\n",
        "# random.seed(SEED)\n",
        "# os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "# torch.manual_seed(SEED)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed_all(SEED)\n",
        "# torch.use_deterministic_algorithms(True)\n",
        "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "# logger = logging.getLogger(__name__)\n",
        "# nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# # === CLASS FOR TOPIC SEARCHER ===\n",
        "# class AllergyTopicSearcher:\n",
        "#     def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params, model_name=\"all-MiniLM-L6-v2\"):\n",
        "#         self.chunks = chunks\n",
        "#         self.entities_per_chunk = entities_per_chunk\n",
        "#         self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "#         self.umap_params = umap_params\n",
        "#         self.hdbscan_params = hdbscan_params\n",
        "\n",
        "#         self.topic_model = None\n",
        "#         self.topic_metadata = []\n",
        "#         self.topic_embeddings = None\n",
        "#         self.searcher = None\n",
        "\n",
        "#         self._prepare()\n",
        "\n",
        "#     def _prepare(self):\n",
        "#         entity_context_pairs = []\n",
        "\n",
        "#         for idx, ents in enumerate(self.entities_per_chunk):\n",
        "#             chunk = self.chunks[idx].lower()\n",
        "#             sentences = sent_tokenize(chunk)\n",
        "#             for ent in ents:\n",
        "#                 ent_lower = ent.lower()\n",
        "#                 for sent in sentences:\n",
        "#                     if ent_lower in sent:\n",
        "#                         entity_context_pairs.append((ent_lower, sent.strip()))\n",
        "#                         break\n",
        "\n",
        "#         if not entity_context_pairs:\n",
        "#             raise ValueError(\"No entity-context pairs extracted!\")\n",
        "\n",
        "#         # Embed entity-contexts\n",
        "#         contextual_texts = [f\"{ent}: {context}\" for ent, context in entity_context_pairs]\n",
        "#         contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=True)\n",
        "\n",
        "#         # Topic Modeling\n",
        "#         umap_model = UMAP(**self.umap_params)\n",
        "#         hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
        "\n",
        "#         self.topic_model = BERTopic(\n",
        "#             embedding_model=self.embedding_model,\n",
        "#             umap_model=umap_model,\n",
        "#             hdbscan_model=hdbscan_model,\n",
        "#             representation_model=KeyBERTInspired(),\n",
        "#             calculate_probabilities=True,\n",
        "#             verbose=False,\n",
        "#         )\n",
        "\n",
        "#         topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
        "\n",
        "#         # Metadata aggregation\n",
        "#         topic_to_contexts = defaultdict(list)\n",
        "#         topic_to_entities = defaultdict(set)\n",
        "#         topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "#         for i, topic in enumerate(topics):\n",
        "#             ent, context = entity_context_pairs[i]\n",
        "#             topic_to_contexts[topic].append(context)\n",
        "#             topic_to_entities[topic].add(ent)\n",
        "#             topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "#         topic_embeddings = []\n",
        "#         topic_metadata = []\n",
        "\n",
        "#         for topic_id in topic_to_contexts:\n",
        "#             embeddings = topic_to_embeddings[topic_id]\n",
        "#             mean_emb = np.mean(embeddings, axis=0)\n",
        "#             mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "#             topic_embeddings.append(mean_emb)\n",
        "#             topic_metadata.append({\n",
        "#                 \"topic_id\": topic_id,\n",
        "#                 \"entities\": list(topic_to_entities[topic_id]),\n",
        "#                 \"sentences\": topic_to_contexts[topic_id],\n",
        "#                 \"sentence_embeddings\": np.array(embeddings)\n",
        "#             })\n",
        "\n",
        "#         self.topic_embeddings = np.array(topic_embeddings)\n",
        "#         self.topic_metadata = topic_metadata\n",
        "\n",
        "#         if len(self.topic_embeddings) < 1:\n",
        "#             raise RuntimeError(\"No topic embeddings to index.\")\n",
        "\n",
        "#         num_clusters = min(len(self.topic_embeddings), 3)\n",
        "#         self.searcher = (\n",
        "#             scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "#             .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "#             .score_brute_force()\n",
        "#             .reorder(3)\n",
        "#             .build()\n",
        "#         )\n",
        "\n",
        "#     def search(self, query, top_k_topics=1, top_k_sents=1):\n",
        "#         query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "#         neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "#         results = []\n",
        "#         for idx in neighbors:\n",
        "#             meta = self.topic_metadata[idx]\n",
        "#             seen = set()\n",
        "#             unique_sentences = []\n",
        "#             unique_embeddings = []\n",
        "\n",
        "#             for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "#                 if sent not in seen:\n",
        "#                     seen.add(sent)\n",
        "#                     unique_sentences.append(sent)\n",
        "#                     unique_embeddings.append(emb)\n",
        "\n",
        "#             sent_embs = np.array(unique_embeddings)\n",
        "#             sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "#             sims = np.dot(sent_embs_norm, query_emb)\n",
        "#             top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "#             top_sents = [(unique_sentences[i], sims[i]) for i in top_indices]\n",
        "\n",
        "#             results.append({\n",
        "#                 \"topic_id\": meta[\"topic_id\"],\n",
        "#                 \"entities\": meta[\"entities\"],\n",
        "#                 \"sentences\": top_sents,\n",
        "#             })\n",
        "\n",
        "#         return results\n",
        "\n",
        "\n",
        "# # === DATASET ===\n",
        "# allergy_dataset = {\n",
        "#     \"name\": \"Allergy Dataset\",\n",
        "#     \"chunks\": [\n",
        "#         \"Patient has peanut allergy causing hives and swelling. Anaphylaxis noted once during a reaction.\",\n",
        "#         \"Allergic rhinitis, or hay fever, results from exposure to pollen, dust, or pet dander.\",\n",
        "#         \"Severe anaphylaxis symptoms require immediate treatment with epinephrine.\",\n",
        "#         \"Food allergies to milk and eggs can cause skin reactions like urticaria and eczema.\",\n",
        "#         \"Cold weather does not cause allergy symptoms in this patient.\"\n",
        "#     ],\n",
        "#     \"entities\": [\n",
        "#         [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "#         [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "#         [\"anaphylaxis\", \"epinephrine\", \"treatment\"],\n",
        "#         [\"food allergies\", \"milk\", \"eggs\", \"urticaria\", \"eczema\"],\n",
        "#         [\"cold weather\", \"allergy symptoms\"]\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "# # === BEST PARAMETERS ===\n",
        "# best_umap = {\"n_neighbors\": 5, \"n_components\": 5, \"min_dist\": 0.1, \"metric\": \"cosine\"}\n",
        "# best_hdbscan = {\"min_cluster_size\": 2, \"min_samples\": 1, \"metric\": \"euclidean\"}\n",
        "\n",
        "# # === INITIALIZE SEARCHER ===\n",
        "# print(\"Preparing Allergy Topic Searcher...\")\n",
        "# searcher = AllergyTopicSearcher(\n",
        "#     chunks=allergy_dataset[\"chunks\"],\n",
        "#     entities_per_chunk=allergy_dataset[\"entities\"],\n",
        "#     umap_params=best_umap,\n",
        "#     hdbscan_params=best_hdbscan,\n",
        "# )\n",
        "# print(\"âœ… Model ready for querying.\")\n",
        "\n",
        "# # === QUERY LOOP ===\n",
        "# print(\"\\n=== Allergy Topic Search ===\")\n",
        "# while True:\n",
        "#     query = input(\"\\nEnter a query (or type 'exit' to quit): \").strip()\n",
        "#     if query.lower() in {\"exit\", \"quit\"}:\n",
        "#         print(\"Goodbye!\")\n",
        "#         break\n",
        "\n",
        "#     results = searcher.search(query, top_k_topics=1, top_k_sents=3)\n",
        "#     print(f\"\\nğŸ” Top results for: '{query}'\")\n",
        "#     for res in results:\n",
        "#         print(f\"ğŸ§  Topic ID: {res['topic_id']}\")\n",
        "#         print(f\"ğŸ”— Related Entities: {', '.join(res['entities'])}\")\n",
        "#         for sent, _ in res[\"sentences\"]:\n",
        "#             print(f\"âœ“ {sent}\")\n"
      ],
      "metadata": {
        "id": "eAxcf07N_a9X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTS ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "# === ENVIRONMENT SETUP ===\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# === CLASS FOR TOPIC SEARCHER ===\n",
        "class AllergyTopicSearcher:\n",
        "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.chunks = chunks\n",
        "        self.entities_per_chunk = entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "        self.umap_params = umap_params\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        entity_context_pairs = []\n",
        "\n",
        "        for idx, ents in enumerate(self.entities_per_chunk):\n",
        "            chunk = self.chunks[idx].lower()\n",
        "            sentences = sent_tokenize(chunk)\n",
        "            for ent in ents:\n",
        "                ent_lower = ent.lower()\n",
        "                for sent in sentences:\n",
        "                    if ent_lower in sent:\n",
        "                        entity_context_pairs.append((ent_lower, sent.strip()))\n",
        "                        break\n",
        "\n",
        "        if not entity_context_pairs:\n",
        "            raise ValueError(\"No entity-context pairs extracted!\")\n",
        "\n",
        "        # Embed entity-contexts\n",
        "        contextual_texts = [f\"{ent}: {context}\" for ent, context in entity_context_pairs]\n",
        "        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=True)\n",
        "\n",
        "        # Topic Modeling\n",
        "        umap_model = UMAP(**self.umap_params)\n",
        "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
        "\n",
        "        self.topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
        "\n",
        "        # Metadata aggregation\n",
        "        topic_to_contexts = defaultdict(list)\n",
        "        topic_to_entities = defaultdict(set)\n",
        "        topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "        for i, topic in enumerate(topics):\n",
        "            ent, context = entity_context_pairs[i]\n",
        "            topic_to_contexts[topic].append(context)\n",
        "            topic_to_entities[topic].add(ent)\n",
        "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id in topic_to_contexts:\n",
        "            embeddings = topic_to_embeddings[topic_id]\n",
        "            mean_emb = np.mean(embeddings, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": list(topic_to_entities[topic_id]),\n",
        "                \"sentences\": topic_to_contexts[topic_id],\n",
        "                \"sentence_embeddings\": np.array(embeddings)\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "        # === Added print: topics and their entities ===\n",
        "        print(\"\\n=== Topics and Associated Entities ===\")\n",
        "        for meta in self.topic_metadata:\n",
        "            print(f\"Topic ID: {meta['topic_id']}, Entities: {', '.join(meta['entities'])}\")\n",
        "\n",
        "        if len(self.topic_embeddings) < 1:\n",
        "            raise RuntimeError(\"No topic embeddings to index.\")\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 3)\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=1):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        results = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            seen = set()\n",
        "            unique_sentences = []\n",
        "            unique_embeddings = []\n",
        "\n",
        "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "                if sent not in seen:\n",
        "                    seen.add(sent)\n",
        "                    unique_sentences.append(sent)\n",
        "                    unique_embeddings.append(emb)\n",
        "\n",
        "            sent_embs = np.array(unique_embeddings)\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            top_sents = [(unique_sentences[i], sims[i]) for i in top_indices]\n",
        "\n",
        "            results.append({\n",
        "                \"topic_id\": meta[\"topic_id\"],\n",
        "                \"entities\": meta[\"entities\"],\n",
        "                \"sentences\": top_sents,\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# === DATASET ===\n",
        "allergy_dataset = {\n",
        "    \"name\": \"Allergy Dataset\",\n",
        "    \"chunks\": [\n",
        "        \"Patient has peanut allergy causing hives and swelling. Anaphylaxis noted once during a reaction.\",\n",
        "        \"Allergic rhinitis, or hay fever, results from exposure to pollen, dust, or pet dander.\",\n",
        "        \"Severe anaphylaxis symptoms require immediate treatment with epinephrine.\",\n",
        "        \"Food allergies to milk and eggs can cause skin reactions like urticaria and eczema.\",\n",
        "        \"Cold weather does not cause allergy symptoms in this patient.\"\n",
        "    ],\n",
        "    \"entities\": [\n",
        "        [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "        [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "        [\"anaphylaxis\", \"epinephrine\", \"treatment\"],\n",
        "        [\"food allergies\", \"milk\", \"eggs\", \"urticaria\", \"eczema\"],\n",
        "        [\"cold weather\", \"allergy symptoms\"]\n",
        "    ]\n",
        "}\n",
        "\n",
        "# === BEST PARAMETERS ===\n",
        "best_umap = {\"n_neighbors\": 5, \"n_components\": 5, \"min_dist\": 0.1, \"metric\": \"cosine\"}\n",
        "best_hdbscan = {\"min_cluster_size\": 2, \"min_samples\": 1, \"metric\": \"euclidean\"}\n",
        "\n",
        "# === INITIALIZE SEARCHER ===\n",
        "print(\"Preparing Allergy Topic Searcher...\")\n",
        "searcher = AllergyTopicSearcher(\n",
        "    chunks=allergy_dataset[\"chunks\"],\n",
        "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
        "    umap_params=best_umap,\n",
        "    hdbscan_params=best_hdbscan,\n",
        ")\n",
        "print(\"âœ… Model ready for querying.\")\n",
        "\n",
        "# === QUERY LOOP ===\n",
        "print(\"\\n=== Allergy Topic Search ===\")\n",
        "while True:\n",
        "    query = input(\"\\nEnter a query (or type 'exit' to quit): \").strip()\n",
        "    if query.lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    results = searcher.search(query, top_k_topics=1, top_k_sents=3)\n",
        "    print(f\"\\nğŸ” Top results for: '{query}'\")\n",
        "    for res in results:\n",
        "        print(f\"ğŸ§  Topic ID: {res['topic_id']}\")\n",
        "        print(f\"ğŸ”— Related Entities: {', '.join(res['entities'])}\")\n",
        "        for sent, _ in res[\"sentences\"]:\n",
        "            print(f\"âœ“ {sent}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdDY5WrNJvR6",
        "outputId": "4e07af5c-509e-4e28-a8b3-36a1e7b70fa7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing Allergy Topic Searcher...\n",
            "\n",
            "=== Topics and Associated Entities ===\n",
            "Topic ID: 3, Entities: swelling, peanut allergy, hives\n",
            "Topic ID: 2, Entities: treatment, anaphylaxis, epinephrine\n",
            "Topic ID: 0, Entities: allergic rhinitis, pollen, dust, allergy symptoms, cold weather, pet dander, hay fever\n",
            "Topic ID: 1, Entities: eggs, eczema, food allergies, urticaria, milk\n",
            "âœ… Model ready for querying.\n",
            "\n",
            "=== Allergy Topic Search ===\n",
            "\n",
            "Enter a query (or type 'exit' to quit): food\n",
            "\n",
            "ğŸ” Top results for: 'food'\n",
            "ğŸ§  Topic ID: 1\n",
            "ğŸ”— Related Entities: eggs, eczema, food allergies, urticaria, milk\n",
            "âœ“ food allergies to milk and eggs can cause skin reactions like urticaria and eczema.\n",
            "\n",
            "Enter a query (or type 'exit' to quit): symptoms \n",
            "\n",
            "ğŸ” Top results for: 'symptoms'\n",
            "ğŸ§  Topic ID: 0\n",
            "ğŸ”— Related Entities: allergic rhinitis, pollen, dust, allergy symptoms, cold weather, pet dander, hay fever\n",
            "âœ“ cold weather does not cause allergy symptoms in this patient.\n",
            "âœ“ allergic rhinitis, or hay fever, results from exposure to pollen, dust, or pet dander.\n",
            "\n",
            "Enter a query (or type 'exit' to quit): causes of allergy\n",
            "\n",
            "ğŸ” Top results for: 'causes of allergy'\n",
            "ğŸ§  Topic ID: 1\n",
            "ğŸ”— Related Entities: eggs, eczema, food allergies, urticaria, milk\n",
            "âœ“ food allergies to milk and eggs can cause skin reactions like urticaria and eczema.\n",
            "\n",
            "Enter a query (or type 'exit' to quit): exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}