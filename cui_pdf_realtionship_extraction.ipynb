{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5J8UH7f893d-"
   },
   "outputs": [],
   "source": [
    "# Google Cloud Document AI client\n",
    "!pip install google-cloud-documentai\n",
    "\n",
    "# Google Cloud BigQuery (for reading CUI vectors)\n",
    "!pip install pandas-gbq google-cloud-bigquery\n",
    "\n",
    "# Numpy\n",
    "!pip install numpy\n",
    "\n",
    "# Google GenAI SDK (for Vertex AI embeddings)\n",
    "!pip install google-genai\n",
    "\n",
    "# Optional but useful: pandas (for dataframe handling)\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB9nGIAKYxhZ"
   },
   "outputs": [],
   "source": [
    "#Updated code 1 woth 3 cui embedding: from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas_gbq\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "PROJECT_ID = \"YOUR_GCP_PROJECT\"\n",
    "LOCATION = \"global\"\n",
    "BQ_CUI_TABLE = \"your_project.your_dataset.cui_embeddings\"  # columns: cui STRING, embedding ARRAY<FLOAT64>\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
    "\n",
    "# ----------------------------\n",
    "# Vertex AI Client\n",
    "# ----------------------------\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "_client = genai.Client()\n",
    "\n",
    "def gemini_embed(texts: List[str], *, task_type: str = \"RETRIEVAL_DOCUMENT\") -> np.ndarray:\n",
    "    \"\"\"Batch-embed text with Vertex AI (Gemini) embeddings.\"\"\"\n",
    "    if not texts:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "    cfg = EmbedContentConfig(task_type=task_type)\n",
    "    resp = _client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=texts,\n",
    "        config=cfg,\n",
    "    )\n",
    "    return np.array([e.values for e in resp.embeddings], dtype=np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# BigQuery: load CUI embeddings\n",
    "# ----------------------------\n",
    "def load_cui_vectors_bq(table_fqn: str = BQ_CUI_TABLE) -> Dict[str, np.ndarray]:\n",
    "    df = pandas_gbq.read_gbq(f\"SELECT cui, embedding FROM `{table_fqn}`\", project_id=PROJECT_ID)\n",
    "    out: Dict[str, np.ndarray] = {}\n",
    "    for _, row in df.iterrows():\n",
    "        out[row[\"cui\"]] = np.array(row[\"embedding\"], dtype=np.float32)\n",
    "    return out\n",
    "\n",
    "CUI_VECTORS = load_cui_vectors_bq(BQ_CUI_TABLE)\n",
    "\n",
    "# ----------------------------\n",
    "# Dummy UMLS linker â€” replace with your logic\n",
    "# ----------------------------\n",
    "def umls_link(text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return e.g. [{\"cui\": \"C0001449\", \"score\": 0.97},\n",
    "                  {\"cui\": \"C0002336\", \"score\": 0.88},\n",
    "                  {\"cui\": \"C0005555\", \"score\": 0.77}]\n",
    "    \"\"\"\n",
    "    return []\n",
    "\n",
    "# ----------------------------\n",
    "# Date handling\n",
    "# ----------------------------\n",
    "DATE_REGEX = re.compile(r\"\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})(?:[ ,;]+(\\d{1,2}[:.]\\d{2}))?\\b\")\n",
    "\n",
    "def norm_date(s: str) -> Optional[str]:\n",
    "    s = s.strip().replace('.', ':').replace(',', ' ')\n",
    "    try:\n",
    "        for fmt in (\"%m/%d/%Y\", \"%m/%d/%y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "            try:\n",
    "                return datetime.strptime(s.split()[0], fmt).strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        m = DATE_REGEX.search(s)\n",
    "        if m:\n",
    "            raw = m.group(1)\n",
    "            for fmt in (\"%m/%d/%Y\", \"%m/%d/%y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "                try:\n",
    "                    return datetime.strptime(raw, fmt).strftime(\"%Y-%m-%d\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Entity class (UPDATED)\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Entity:\n",
    "    entity_id: str\n",
    "    page: int\n",
    "    text: str\n",
    "    kind: str                          # \"date\" | \"kv\" | \"section\" | \"other\"\n",
    "    section_title: Optional[str]\n",
    "    value_norm_date: Optional[str]\n",
    "\n",
    "    # NEW: store multiple CUIs + embeddings\n",
    "    cui_candidates: List[str] = field(default_factory=list)\n",
    "    cui_scores: List[float] = field(default_factory=list)\n",
    "    embeddings: List[np.ndarray] = field(default_factory=list)\n",
    "\n",
    "    # Backward compatibility: best single values\n",
    "    cui: Optional[str] = None\n",
    "    cui_score: Optional[float] = None\n",
    "    emb: Optional[np.ndarray] = None\n",
    "\n",
    "# ----------------------------\n",
    "# Parse DocAI entities\n",
    "# ----------------------------\n",
    "def parse_docai_entities(docai: Dict) -> List[Entity]:\n",
    "    entities: List[Entity] = []\n",
    "    pages = docai.get(\"document\", {}).get(\"pages\", [])\n",
    "    for p_idx, page in enumerate(pages):\n",
    "        # KVs\n",
    "        for ff in page.get(\"formFields\", []) or []:\n",
    "            name = ff.get(\"fieldName\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ff.get(\"fieldName\", {}).get(\"content\", \"\")\n",
    "            value = ff.get(\"fieldValue\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or ff.get(\"fieldValue\", {}).get(\"content\", \"\")\n",
    "            kv_text = f\"{name.strip()}: {value.strip()}\".strip(\": \")\n",
    "            entities.append(Entity(\n",
    "                entity_id=f\"kv:{p_idx}:{len(entities)}\",\n",
    "                page=p_idx, text=kv_text, kind=\"kv\",\n",
    "                section_title=None, value_norm_date=norm_date(kv_text)\n",
    "            ))\n",
    "            if DATE_REGEX.search(kv_text or \"\"):\n",
    "                entities.append(Entity(\n",
    "                    entity_id=f\"date:{p_idx}:{len(entities)}\",\n",
    "                    page=p_idx, text=kv_text, kind=\"date\",\n",
    "                    section_title=None, value_norm_date=norm_date(kv_text)\n",
    "                ))\n",
    "        # Headings + paragraphs\n",
    "        heading = None\n",
    "        for line in page.get(\"lines\", []) or []:\n",
    "            t = line.get(\"layout\", {}).get(\"textAnchor\", {}).get(\"content\", \"\") or line.get(\"layout\", {}).get(\"content\", \"\")\n",
    "            if not t:\n",
    "                continue\n",
    "            if t.endswith(\":\") or t.isupper():\n",
    "                heading = t.strip().strip(\":\")\n",
    "            else:\n",
    "                entities.append(Entity(\n",
    "                    entity_id=f\"sec:{p_idx}:{len(entities)}\",\n",
    "                    page=p_idx,\n",
    "                    text=f\"{(heading or '').upper()}: {t}\",\n",
    "                    kind=\"section\",\n",
    "                    section_title=(heading or \"\").upper(),\n",
    "                    value_norm_date=norm_date(t)\n",
    "                ))\n",
    "                for m in DATE_REGEX.finditer(t or \"\"):\n",
    "                    entities.append(Entity(\n",
    "                        entity_id=f\"datei:{p_idx}:{len(entities)}\",\n",
    "                        page=p_idx, text=t, kind=\"date\",\n",
    "                        section_title=(heading or \"\").upper(),\n",
    "                        value_norm_date=norm_date(m.group(0))\n",
    "                    ))\n",
    "    return entities\n",
    "\n",
    "# ----------------------------\n",
    "# Attach CUIs and embeddings (UPDATED)\n",
    "# ----------------------------\n",
    "def attach_cuis_and_embeddings_local(entities: List[Entity], topk: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Attach top-k CUIs and their embeddings (if available) to entities.\n",
    "    Fallback to text embedding where needed.\n",
    "    \"\"\"\n",
    "    to_embed_texts = []\n",
    "    text_entity_refs = []\n",
    "\n",
    "    for e in entities:\n",
    "        cand = umls_link(e.text)\n",
    "\n",
    "        if cand:\n",
    "            # pick top-k best matches\n",
    "            top_matches = sorted(cand, key=lambda x: x.get(\"score\", 0.0), reverse=True)[:topk]\n",
    "\n",
    "            e.cui_candidates = [m[\"cui\"] for m in top_matches]\n",
    "            e.cui_scores = [m.get(\"score\", 0.0) for m in top_matches]\n",
    "\n",
    "            # Add embeddings if available\n",
    "            for cui, score in zip(e.cui_candidates, e.cui_scores):\n",
    "                if cui in CUI_VECTORS:\n",
    "                    e.embeddings.append(CUI_VECTORS[cui])\n",
    "\n",
    "            # assign best candidate for backward use\n",
    "            if e.cui_candidates:\n",
    "                e.cui = e.cui_candidates[0]\n",
    "                e.cui_score = e.cui_scores[0]\n",
    "                if e.embeddings:\n",
    "                    e.emb = e.embeddings[0]\n",
    "\n",
    "        # If no CUI match or no precomputed embedding, fall back to text\n",
    "        if not e.embeddings:\n",
    "            to_embed_texts.append(f\"[ENTITY:{e.kind}] {e.text[:500]}\")\n",
    "            text_entity_refs.append(e)\n",
    "\n",
    "    if to_embed_texts:\n",
    "        embs = gemini_embed(to_embed_texts)\n",
    "        for ent, vec in zip(text_entity_refs, embs):\n",
    "            ent.embeddings.append(vec)\n",
    "            ent.emb = vec  # fallback single emb\n",
    "\n",
    "# ----------------------------\n",
    "# Cosine similarity + graph building\n",
    "# ----------------------------\n",
    "def _cos(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    da = float(np.linalg.norm(a) + 1e-8)\n",
    "    db = float(np.linalg.norm(b) + 1e-8)\n",
    "    return float(np.dot(a, b) / (da * db))\n",
    "\n",
    "def build_edges(entities: List[Entity], sim_threshold: float=0.65) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    ids = [e.entity_id for e in entities if e.emb is not None]\n",
    "    by_id = {e.entity_id: e for e in entities}\n",
    "    adj: Dict[str, List[Tuple[str, float]]] = {i: [] for i in ids}\n",
    "    for i in range(len(ids)):\n",
    "        ei = by_id[ids[i]]\n",
    "        for j in range(i+1, len(ids)):\n",
    "            ej = by_id[ids[j]]\n",
    "            s = _cos(ei.emb, ej.emb)\n",
    "            if s >= sim_threshold:\n",
    "                adj[ei.entity_id].append((ej.entity_id, s))\n",
    "                adj[ej.entity_id].append((ei.entity_id, s))\n",
    "    return adj\n",
    "\n",
    "# ----------------------------\n",
    "# Query utilities\n",
    "# ----------------------------\n",
    "def _embed_query_vector(query_text: str) -> np.ndarray:\n",
    "    cand = umls_link(query_text)\n",
    "    if cand:\n",
    "        best = max(cand, key=lambda x: x.get(\"score\", 0.0))\n",
    "        cui = best[\"cui\"]\n",
    "        if cui in CUI_VECTORS:\n",
    "            return CUI_VECTORS[cui]\n",
    "    qvec = gemini_embed([f\"[QUERY] {query_text[:800]}\"])\n",
    "    return qvec[0]\n",
    "\n",
    "def _is_date_like(q: str) -> Optional[str]:\n",
    "    return norm_date(q)\n",
    "\n",
    "def _nearest_neighbors(vec: np.ndarray, entities: List[Entity], topk: int = 10) -> List[Tuple[Entity, float]]:\n",
    "    cands = [(e, _cos(vec, e.emb)) for e in entities if e.emb is not None]\n",
    "    cands.sort(key=lambda x: -x[1])\n",
    "    return cands[:topk]\n",
    "\n",
    "def build_model_local(docai: Dict, min_edge: float = 0.60):\n",
    "    ents = parse_docai_entities(docai)\n",
    "    attach_cuis_and_embeddings_local(ents)\n",
    "    adj = build_edges(ents, sim_threshold=min_edge)\n",
    "    by_id = {e.entity_id: e for e in ents}\n",
    "    return ents, adj, by_id\n",
    "\n",
    "def query_any_local(docai: Dict, query_text: str,\n",
    "                    min_direct: float = 0.60, min_indirect: float = 0.45, decay: float = 0.8,\n",
    "                    topk: int = 8):\n",
    "    ents, adj, by_id = build_model_local(docai, min_edge=min_direct)\n",
    "\n",
    "    q_iso = _is_date_like(query_text)\n",
    "    q_vec = _embed_query_vector(query_text)\n",
    "\n",
    "    nn = _nearest_neighbors(q_vec, ents, topk=topk)\n",
    "\n",
    "    date_anchors = []\n",
    "    if q_iso:\n",
    "        date_anchors = [e for e in ents if e.kind == \"date\" and e.value_norm_date == q_iso]\n",
    "        for e in date_anchors:\n",
    "            nn.insert(0, (e, 1.0))\n",
    "        seen = set()\n",
    "        nn = [(e, s) for e, s in nn if not (e.entity_id in seen or seen.add(e.entity_id))]\n",
    "\n",
    "    direct = []\n",
    "    kept = set()\n",
    "    for e, score in nn:\n",
    "        if score < min_direct:\n",
    "            continue\n",
    "        direct.append({\n",
    "            \"entity_id\": e.entity_id,\n",
    "            \"kind\": e.kind,\n",
    "            \"section_title\": e.section_title,\n",
    "            \"cui\": e.cui,\n",
    "            \"cui_candidates\": e.cui_candidates,\n",
    "            \"cui_scores\": e.cui_scores,\n",
    "            \"score\": round(float(score), 3),\n",
    "            \"evidence\": e.text[:240]\n",
    "        })\n",
    "        kept.add(e.entity_id)\n",
    "        if len(direct) >= topk:\n",
    "            break\n",
    "\n",
    "    indirect = []\n",
    "    start_nodes = date_anchors if date_anchors else [by_id[d[\"entity_id\"]] for d in direct[:3]]\n",
    "    for src in start_nodes:\n",
    "        for mid, s1 in adj.get(src.entity_id, []):\n",
    "            if s1 < min_indirect:\n",
    "                continue\n",
    "            for tgt, s2 in adj.get(mid, []):\n",
    "                if tgt in kept:\n",
    "                    continue\n",
    "                ind_score = s1 * s2 * decay\n",
    "                if ind_score >= min_indirect:\n",
    "                    tEnt = by_id[tgt]\n",
    "                    indirect.append({\n",
    "                        \"target_entity_id\": tgt,\n",
    "                        \"kind\": tEnt.kind,\n",
    "                        \"section_title\": tEnt.section_title,\n",
    "                        \"score\": round(float(ind_score), 3),\n",
    "                        \"path\": [\n",
    "                            {\"from\": src.entity_id, \"to\": mid, \"score\": round(float(s1), 3)},\n",
    "                            {\"from\": mid, \"to\": tgt, \"score\": round(float(s2), 3)}\n",
    "                        ],\n",
    "                        \"evidence\": tEnt.text[:240]\n",
    "                    })\n",
    "                    kept.add(tgt)\n",
    "\n",
    "    direct.sort(key=lambda x: -x[\"score\"])\n",
    "    indirect.sort(key=lambda x: -x[\"score\"])\n",
    "    return {\n",
    "        \"query\": query_text,\n",
    "        \"query_iso_date\": q_iso,\n",
    "        \"direct\": direct,\n",
    "        \"indirect\": indirect\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
