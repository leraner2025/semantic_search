# ============================
# CODE 1: CUI Graph (Local Context Only) â€” BQ has {cui, embedding}
# ============================
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Any
import numpy as np
import re
from datetime import datetime
import os

# ----------------------------
# CONFIG
# ----------------------------
PROJECT_ID = "YOUR_GCP_PROJECT"
LOCATION = "global"
BQ_CUI_TABLE = "your_project.your_dataset.cui_embeddings"  # columns: cui STRING, embedding ARRAY<FLOAT64>

# ----------------------------
# Vertex AI Embeddings (official Vertex routing via google-genai)
# ----------------------------
os.environ["GOOGLE_CLOUD_PROJECT"] = PROJECT_ID
os.environ["GOOGLE_CLOUD_LOCATION"] = LOCATION
os.environ["GOOGLE_GENAI_USE_VERTEXAI"] = "True"

from google import genai
from google.genai.types import EmbedContentConfig
_client = genai.Client()

def gemini_embed(texts: List[str], *, task_type: str = "RETRIEVAL_DOCUMENT") -> np.ndarray:
    """Batch-embed text with Vertex AI (Gemini) embeddings."""
    if not texts:
        return np.zeros((0, 0), dtype=np.float32)
    cfg = EmbedContentConfig(task_type=task_type)
    resp = _client.models.embed_content(
        model="gemini-embedding-001",
        contents=texts,
        config=cfg,
    )
    return np.array([e.values for e in resp.embeddings], dtype=np.float32)

# ----------------------------
# BigQuery: load CUI embeddings (two columns only)
# ----------------------------
import pandas_gbq

def load_cui_vectors_bq(table_fqn: str = BQ_CUI_TABLE) -> Dict[str, np.ndarray]:
    """Load {cui, embedding} from BigQuery into an in-memory dict."""
    df = pandas_gbq.read_gbq(f"SELECT cui, embedding FROM `{table_fqn}`", project_id=PROJECT_ID)
    out: Dict[str, np.ndarray] = {}
    for _, row in df.iterrows():
        out[row["cui"]] = np.array(row["embedding"], dtype=np.float32)
    return out

CUI_VECTORS = load_cui_vectors_bq(BQ_CUI_TABLE)  # {cui: np.ndarray}

# ----------------------------
# UMLS linker (plug your own)
# ----------------------------
def umls_link(text: str) -> List[Dict[str, Any]]:
    """Return e.g. [{"cui": "C0001449", "score": 0.97}, ...] using your preferred UMLS linker."""
    return []

# ----------------------------
# DocAI parsing (lightweight)
# ----------------------------
DATE_REGEX = re.compile(r"\b(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})(?:[ ,;]+(\d{1,2}[:.]\d{2}))?\b")

def norm_date(s: str) -> Optional[str]:
    s = s.strip().replace('.', ':').replace(',', ' ')
    try:
        for fmt in ("%m/%d/%Y", "%m/%d/%y", "%m-%d-%Y", "%m-%d-%y"):
            try:
                return datetime.strptime(s.split()[0], fmt).strftime("%Y-%m-%d")
            except Exception:
                pass
        m = DATE_REGEX.search(s)
        if m:
            raw = m.group(1)
            for fmt in ("%m/%d/%Y", "%m/%d/%y", "%m-%d-%Y", "%m-%d-%y"):
                try:
                    return datetime.strptime(raw, fmt).strftime("%Y-%m-%d")
                except Exception:
                    pass
    except Exception:
        pass
    return None

@dataclass
class Entity:
    entity_id: str
    page: int
    text: str
    kind: str                  # "date" | "kv" | "section" | "other"
    section_title: Optional[str]
    value_norm_date: Optional[str]
    cui: Optional[str] = None
    cui_score: Optional[float] = None
    emb: Optional[np.ndarray] = None

def parse_docai_entities(docai: Dict) -> List[Entity]:
    """Parse a subset of DocAI JSON into Entity rows (KVs, sections, dates)."""
    entities: List[Entity] = []
    pages = docai.get("document", {}).get("pages", [])
    for p_idx, page in enumerate(pages):
        # KVs
        for ff in page.get("formFields", []) or []:
            name = ff.get("fieldName", {}).get("textAnchor", {}).get("content", "") or ff.get("fieldName", {}).get("content", "")
            value = ff.get("fieldValue", {}).get("textAnchor", {}).get("content", "") or ff.get("fieldValue", {}).get("content", "")
            kv_text = f"{name.strip()}: {value.strip()}".strip(": ")
            entities.append(Entity(
                entity_id=f"kv:{p_idx}:{len(entities)}",
                page=p_idx, text=kv_text, kind="kv",
                section_title=None, value_norm_date=norm_date(kv_text)
            ))
            if DATE_REGEX.search(kv_text or ""):
                entities.append(Entity(
                    entity_id=f"date:{p_idx}:{len(entities)}",
                    page=p_idx, text=kv_text, kind="date",
                    section_title=None, value_norm_date=norm_date(kv_text)
                ))
        # headings + paragraphs (simple)
        heading = None
        for line in page.get("lines", []) or []:
            t = line.get("layout", {}).get("textAnchor", {}).get("content", "") or line.get("layout", {}).get("content", "")
            if not t:
                continue
            if t.endswith(":") or t.isupper():
                heading = t.strip().strip(":")
            else:
                entities.append(Entity(
                    entity_id=f"sec:{p_idx}:{len(entities)}",
                    page=p_idx,
                    text=f"{(heading or '').upper()}: {t}",
                    kind="section",
                    section_title=(heading or "").upper(),
                    value_norm_date=norm_date(t)
                ))
                for m in DATE_REGEX.finditer(t or ""):
                    entities.append(Entity(
                        entity_id=f"datei:{p_idx}:{len(entities)}",
                        page=p_idx, text=t, kind="date",
                        section_title=(heading or "").upper(),
                        value_norm_date=norm_date(m.group(0))
                    ))
    return entities

# ----------------------------
# CUI mapping + embeddings
# ----------------------------
def attach_cuis_and_embeddings_local(entities: List[Entity]) -> None:
    texts, idx = [], []
    for e in entities:
        cand = umls_link(e.text)
        if cand:
            best = max(cand, key=lambda x: x.get("score", 0.0))
            e.cui, e.cui_score = best["cui"], best.get("score")
        # prefer precomputed CUI vector
        if e.cui and e.cui in CUI_VECTORS:
            e.emb = CUI_VECTORS[e.cui]
        else:
            # fallback: quick local text embedding
            texts.append(f"[ENTITY:{e.kind}] {e.text[:500]}")
            idx.append(e)
    if texts:
        embs = gemini_embed(texts)
        for ent, vec in zip(idx, embs):
            ent.emb = vec

# ----------------------------
# Graph + query helpers
# ----------------------------
def _cos(a: np.ndarray, b: np.ndarray) -> float:
    da = float(np.linalg.norm(a) + 1e-8)
    db = float(np.linalg.norm(b) + 1e-8)
    return float(np.dot(a, b) / (da * db))

def build_edges(entities: List[Entity], sim_threshold: float=0.65) -> Dict[str, List[Tuple[str, float]]]:
    ids = [e.entity_id for e in entities if e.emb is not None]
    by_id = {e.entity_id: e for e in entities}
    adj: Dict[str, List[Tuple[str, float]]] = {i: [] for i in ids}
    for i in range(len(ids)):
        ei = by_id[ids[i]]
        for j in range(i+1, len(ids)):
            ej = by_id[ids[j]]
            s = _cos(ei.emb, ej.emb)
            if s >= sim_threshold:
                adj[ei.entity_id].append((ej.entity_id, s))
                adj[ej.entity_id].append((ei.entity_id, s))
    return adj

# ----------------------------
# General multi-hop query function
# ----------------------------
def query_multi_hop(docai: Dict, query_text: str,
                    min_direct: float = 0.60, min_indirect: float = 0.45,
                    decay: float = 0.8, topk: int = 8, max_hops: int = 2):
    """
    Generic multi-hop query over the entity graph.

    Args:
        docai: DocAI parsed document dict.
        query_text: Query string.
        min_direct: minimum similarity for direct neighbors.
        min_indirect: minimum similarity for indirect neighbors.
        decay: decay factor applied per hop for indirect similarity.
        topk: number of direct neighbors to return.
        max_hops: max number of hops to traverse (1 = direct only).

    Returns:
        Dict with direct and indirect associations.
    """
    ents = parse_docai_entities(docai)
    attach_cuis_and_embeddings_local(ents)
    adj = build_edges(ents, sim_threshold=min_direct)
    by_id = {e.entity_id: e for e in ents}

    # Embed query
    def _embed_query_vector(query_text: str) -> np.ndarray:
        cand = umls_link(query_text)  # potential CUI mapping
        if cand:
            best = max(cand, key=lambda x: x.get("score", 0.0))
            cui = best["cui"]
            if cui in CUI_VECTORS:
                return CUI_VECTORS[cui]
        qvec = gemini_embed([f"[QUERY] {query_text[:800]}"])
        return qvec[0]

    q_vec = _embed_query_vector(query_text)

    # Find nearest neighbors (direct)
    cands = [(e, _cos(q_vec, e.emb)) for e in ents if e.emb is not None]
    cands.sort(key=lambda x: -x[1])
    direct = []
    kept = set()
    for e, score in cands:
        if score < min_direct:
            continue
        direct.append({
            "entity_id": e.entity_id,
            "kind": e.kind,
            "section_title": e.section_title,
            "cui": e.cui,
            "score": round(float(score), 3),
            "evidence": e.text[:240]
        })
        kept.add(e.entity_id)
        if len(direct) >= topk:
            break

    # Multi-hop indirect neighbors using BFS up to max_hops
    indirect = []
    if max_hops > 1:
        # Start from direct nodes
        frontier = [(e["entity_id"], 1.0) for e in direct]
        visited = set(kept)
        hop = 1

        while frontier and hop < max_hops:
            next_frontier = []
            for current_id, current_score in frontier:
                for nbr_id, sim_score in adj.get(current_id, []):
                    if nbr_id in visited:
                        continue
                    combined_score = current_score * sim_score * decay
                    if combined_score >= min_indirect:
                        e_tgt = by_id[nbr_id]
                        indirect.append({
                            "target_entity_id": nbr_id,
                            "kind": e_tgt.kind,
                            "section_title": e_tgt.section_title,
                            "score": round(float(combined_score), 3),
                            "hop": hop + 1,
                            "evidence": e_tgt.text[:240]
                        })
                        visited.add(nbr_id)
                        next_frontier.append((nbr_id, combined_score))
            frontier = next_frontier
            hop += 1

    direct.sort(key=lambda x: -x["score"])
    indirect.sort(key=lambda x: -x["score"])

    return {
        "query": query_text,
        "direct": direct,
        "indirect": indirect
    }
