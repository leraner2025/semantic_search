import subprocess
import requests
import numpy as np
import pandas as pd
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from google.cloud import bigquery
from vertexai.language_models import TextEmbeddingModel
import scann

# Initialize Vertex AI embedding model
gemini_model = TextEmbeddingModel.from_pretrained("gemini-embedding-001")

# GCP Auth Header Setup
headers = None
def gcp_update_header():
    global headers
    tmp = subprocess.run(['gcloud', 'auth', 'print-identity-token'], stdout=subprocess.PIPE, universal_newlines=True)
    if tmp.returncode != 0:
        raise Exception("Cannot get GCP access token")
    identity_token = tmp.stdout.strip()
    headers = {
        "Authorization": f"Bearer {identity_token}",
        "Content-Type": "application/json"
    }
gcp_update_header()

# NER API
def call_ner_api(text, url):
    payload = {"query_texts": [text], "top_k": 3}
    try:
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        data = response.json()
        cuis = data.get(text, [])
        print(f"NER API call successful. CUIs found: {len(cuis)}")
        return cuis
    except Exception as e:
        print(f"NER API error: {e}")
        return []

# Get embeddings from BigQuery
def get_cui_embeddings(client, project_id, dataset, embedding_table, cuis):
    if not cuis:
        return {}
    
    query = f"""
        SELECT CUI, Embedding
        FROM `{project_id}.{dataset}.{embedding_table}`
        WHERE CUI IN UNNEST(@cuis)
    """
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ArrayQueryParameter("cuis", "STRING", cuis)
        ]
    )
    results = client.query(query, job_config=job_config).result()
    embeddings = {row.CUI: row.Embedding for row in results}
    print(f"Retrieved embeddings for {len(embeddings)} CUIs.")
    return embeddings

# Clustering function using ScaNN with cosine similarity threshold
def cluster_embeddings(embeddings, distance_threshold=0.2):
    if len(embeddings) < 2:
        return [0] * len(embeddings)
    
    # Normalize embeddings for cosine similarity
    normed_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

    # Build ScaNN searcher for nearest neighbors (cosine similarity = 1 - angular distance)
    searcher = scann.scann_ops_pybind.builder(normed_embeddings, 10, "dot_product").tree(
        num_leaves=200, num_leaves_to_search=50, training_sample_size=25000).score_ah(
        2, anisotropic_quantization_threshold=0.2).reorder(100).build()
    
    # Find neighbors within distance threshold (converted for dot product)
    clusters = [-1] * len(embeddings)
    cluster_id = 0

    for i in range(len(embeddings)):
        if clusters[i] != -1:
            continue
        # Query neighbors (top 10, can adjust)
        neighbors, distances = searcher.search_batched(normed_embeddings[i:i+1], final_num_neighbors=10)
        neighbors = neighbors[0]
        distances = distances[0]

        # For cosine similarity, dot_product threshold corresponds to 1 - distance_threshold
        # So neighbors with dot_product >= 1 - distance_threshold are similar enough
        clusters[i] = cluster_id
        for n, dist in zip(neighbors[1:], distances[1:]):  # skip self (first neighbor)
            if dist >= 1 - distance_threshold and clusters[n] == -1:
                clusters[n] = cluster_id
        cluster_id += 1

    print(f"Clustering done: {len(set(clusters))} clusters found.")
    return clusters

# Pick primary and secondary CUIs within each cluster (same as your original)
def pick_most_informative_cuis(cluster_cuis, cluster_embeddings, similarity_threshold=0.9):
    sim_matrix = cosine_similarity(cluster_embeddings)
    n = len(cluster_cuis)
    if n == 1:
        # Only one CUI, no similarity to others, so keep it as primary
        return cluster_cuis, list(cluster_embeddings), [], []
    
    avg_similarities = sim_matrix.sum(axis=1) / (n - 1)

    to_remove = set()
    for i in range(n):
        for j in range(i + 1, n):
            if sim_matrix[i][j] >= similarity_threshold:
                if avg_similarities[i] >= avg_similarities[j]:
                    to_remove.add(j)
                else:
                    to_remove.add(i)

    final_indices = [i for i in range(n) if i not in to_remove]
    final_cuis = [cluster_cuis[i] for i in final_indices]
    final_embeddings = [cluster_embeddings[i] for i in final_indices]

    redundant_indices = [i for i in range(n) if i in to_remove]
    redundant_cuis = [cluster_cuis[i] for i in redundant_indices]
    redundant_embeddings = [cluster_embeddings[i] for i in redundant_indices]

    return final_cuis, final_embeddings, redundant_cuis, redundant_embeddings

# Select representatives from all clusters (same as original)
def select_representatives(cui_list, embeddings, text_embedding=None,
                           similarity_threshold=0.9,
                           min_similarity_to_text=0.3,
                           min_secondary_text_sim=0.25):
    labels = cluster_embeddings(embeddings)  # this calls the new ScaNN-based clustering
    cluster_map = defaultdict(list)

    for i, label in enumerate(labels):
        cluster_map[label].append((cui_list[i], embeddings[i]))

    final_cuis_with_flags = []

    for cluster in cluster_map.values():
        if not cluster:
            continue

        cluster_cuis = [cui for cui, _ in cluster]
        cluster_embs = np.array([embedding for _, embedding in cluster])

        primary_cuis, primary_embs, redundant_cuis, redundant_embs = pick_most_informative_cuis(
            cluster_cuis, cluster_embs, similarity_threshold
        )

        kept_primaries = []
        for cui, emb in zip(primary_cuis, primary_embs):
            if text_embedding is not None:
                sim = cosine_similarity([emb], [text_embedding])[0][0]
                if sim >= min_similarity_to_text:
                    kept_primaries.append((cui, emb))
                    final_cuis_with_flags.append((cui, "primary"))
            else:
                kept_primaries.append((cui, emb))
                final_cuis_with_flags.append((cui, "primary"))

        for cui, emb in zip(redundant_cuis, redundant_embs):
            if text_embedding is not None:
                sim_to_text = cosine_similarity([emb], [text_embedding])[0][0]
                if sim_to_text >= min_secondary_text_sim:
                    too_similar = False
                    for _, sel_emb in kept_primaries:
                        sim_to_sel = cosine_similarity([emb], [sel_emb])[0][0]
                        if sim_to_sel >= similarity_threshold:
                            too_similar = True
                            break
                    if not too_similar:
                        final_cuis_with_flags.append((cui, "secondary"))
            else:
                final_cuis_with_flags.append((cui, "secondary"))

    print(f"Selected {len(final_cuis_with_flags)} CUIs (primary + secondary).")
    return final_cuis_with_flags

# Validation function (optional)
def validate_with_gemini(text_embedding, cui_embeddings):
    results = []
    text_emb = np.array(text_embedding).reshape(1, -1)
    for cui, emb in cui_embeddings.items():
        cui_emb = np.array(emb).reshape(1, -1)
        sim = cosine_similarity(text_emb, cui_emb)[0][0]
        results.append((cui, sim))
    return results

# Full pipeline
def run_pipeline(text, project_id, dataset, embedding_table, ner_url):
    client = bigquery.Client()

    # Step 1: NER
    ner_cuis = call_ner_api(text, ner_url)
    if not ner_cuis:
        print("No CUIs from NER. Exiting.")
        return

    pd.DataFrame({"CUI": ner_cuis}).to_csv("ner_cuis.csv", index=False)
    print("Saved NER CUIs to 'ner_cuis.csv'")

    # Step 2: Embedding retrieval
    cui_embeddings = get_cui_embeddings(client, project_id, dataset, embedding_table, ner_cuis)
    if not cui_embeddings:
        print("No embeddings found. Exiting.")
        return

    pd.DataFrame({"CUI": list(cui_embeddings.keys())}).to_csv("cui_embeddings.csv", index=False)
    print("Saved CUI list to 'cui_embeddings.csv'")

    # Step 3: Get text embedding
    text_embedding = gemini_model.get_embeddings([text])[0].values

    # Step 4: Cluster and select CUIs using ScaNN
    cui_list = list(cui_embeddings.keys())
    embedding_matrix = np.array([cui_embeddings[cui] for cui in cui_list])
    selected_cuis_with_flags = select_representatives(cui_list, embedding_matrix, text_embedding)

    # Step 5: Save final CUIs
    df_final = pd.DataFrame(selected_cuis_with_flags, columns=["CUI", "Type"])
    df_final.to_csv("final_cuis.csv", index=False)
    print("Saved final CUIs to 'final_cuis.csv'")
    print(df_final)

# Example usage
if __name__ == "__main__":
    # Assign your GCP details here
    project_id = project_id
    dataset = dataset
    embedding_table = embedding_table
    ner_url = url

    text = "MRI of head"

    run_pipeline(text, project_id, dataset, embedding_table, ner_url)
