import asyncio
import aiohttp
import subprocess
import networkx as nx
import re
import logging
from pathlib import Path
import json
from typing import List, Dict, Any, Optional, Iterable

# ---------------- Config ----------------
URL = url1
BATCH_SIZE = 50
MAX_CONCURRENT_REQUESTS = 10
REQUEST_TIMEOUT = 60
MAX_RETRIES_PER_CUI = 2  
OUTPUT_DIR = Path("cui_results")
OUTPUT_DIR.mkdir(exist_ok=True)

BASE_CUI_RE = re.compile(r"^(C\d{7})(?:-\d+)?$")

# ---------------- Logging ----------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

# ---------------- Helpers ----------------
def normalize_cui(cui: str) -> Optional[str]:
    m = BASE_CUI_RE.match(cui)
    return m.group(1) if m else None

def validate_cui_list(cuis: Iterable[str]) -> List[str]:
    cuis = list(cuis)
    invalid = [c for c in cuis if not isinstance(c, str) or not BASE_CUI_RE.match(c)]
    if invalid:
        raise ValueError(f"Invalid CUIs: {invalid[:5]}")
    return cuis

def validate_str_list(value: Optional[List[str]]) -> List[str]:
    if value is None:
        return []
    if not all(isinstance(x, str) for x in value):
        raise ValueError("limit_context must be list[str]")
    return value

# ---------------- Auth ----------------
def get_identity_headers() -> Dict[str, str]:
    proc = subprocess.run(["gcloud", "auth", "print-identity-token"],
                          stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if proc.returncode != 0:
        raise RuntimeError(proc.stderr)
    return {"Authorization": f"Bearer {proc.stdout.strip()}", "Content-Type": "application/json"}

HEADERS = get_identity_headers()

# ---------------- Async CUI Client ----------------
class AsyncCUIClient:
    def __init__(self, url: str, headers: Dict[str, str], max_concurrent: int):
        self.url = url
        self.headers = headers
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def fetch_cui(self, session: aiohttp.ClientSession, cui: str, limit_context: List[str]):
        payload = {"cuis": [cui], "cross_context": False}
        if limit_context:
            payload["limit_context"] = limit_context
        try:
            async with self.semaphore:
                async with session.post(f"{self.url}/subnet/", json=payload, headers=self.headers, timeout=REQUEST_TIMEOUT) as resp:
                    resp.raise_for_status()
                    data = await resp.json()
                    nodes, edges = data.get("output", ([], []))
                    nodes = [normalize_cui(n) for n in nodes if normalize_cui(n)]
                    edges = [(normalize_cui(s), normalize_cui(t)) for s, t in edges if normalize_cui(s) and normalize_cui(t)]
                    if normalize_cui(cui) not in nodes:
                        nodes.append(normalize_cui(cui))
                    return nodes, edges, True
        except Exception as e:
            logger.warning(f"CUI {cui} failed: {e}")
            return [], [], False  # indicate failure

    async def fetch_batch(self, batch: List[str], limit_context: List[str]):
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)) as session:
            tasks = [self.fetch_cui(session, cui, limit_context) for cui in batch]
            results = await asyncio.gather(*tasks)
            all_nodes, all_edges = [], []
            failed_cuis = []
            for nodes, edges, success in results:
                all_nodes.extend(nodes)
                all_edges.extend(edges)
                if not success:
                    failed_cuis.extend([c for c in batch if normalize_cui(c) not in nodes])
            return all_nodes, all_edges, failed_cuis

# ---------------- Pipeline ----------------
async def run_pipeline(target_cuis: List[str], limit_context: Optional[List[str]] = None):
    target_cuis = validate_cui_list(target_cuis)
    limit_context = validate_str_list(limit_context)
    client = AsyncCUIClient(URL, HEADERS, MAX_CONCURRENT_REQUESTS)

    batches = [target_cuis[i:i+BATCH_SIZE] for i in range(0, len(target_cuis), BATCH_SIZE)]
    G = nx.DiGraph()
    final_results: Dict[str, Dict[str, Any]] = {}
    output_file = OUTPUT_DIR / "full_results.json"

    failed_cuis_total = []

    # ---------------- Process batches ----------------
    for batch_index, batch in enumerate(batches):
        nodes, edges, failed_cuis = await client.fetch_batch(batch, limit_context)
        G.add_nodes_from(nodes)
        G.add_edges_from(edges)
        failed_cuis_total.extend(failed_cuis)

        # Incremental ancestors & descendants
        for cui in batch:
            n_cui = normalize_cui(cui)
            if not n_cui:
                continue
            ancestors = nx.ancestors(G, n_cui) or {n_cui}
            descendants = nx.descendants(G, n_cui) or {n_cui}
            final_results[n_cui] = {
                "ancestors": sorted(ancestors),
                "descendants": sorted(descendants),
                "total_ancestor_count": len(ancestors),
                "total_descendant_count": len(descendants),
            }

        # Save progress
        with open(output_file, "w") as f:
            json.dump(final_results, f)
        logger.info(f"Saved results after batch {batch_index+1} ({batch[0]}-{batch[-1]})")

    # ---------------- Retry failed CUIs ----------------
    for retry in range(MAX_RETRIES_PER_CUI):
        if not failed_cuis_total:
            break
        logger.info(f"Retry attempt {retry+1} for {len(failed_cuis_total)} failed CUIs")
        new_failed = []
        batches = [failed_cuis_total[i:i+BATCH_SIZE] for i in range(0, len(failed_cuis_total), BATCH_SIZE)]
        failed_cuis_total = []
        for batch in batches:
            nodes, edges, failed_cuis = await client.fetch_batch(batch, limit_context)
            G.add_nodes_from(nodes)
            G.add_edges_from(edges)
            failed_cuis_total.extend(failed_cuis)
            # Update incremental results
            for cui in batch:
                n_cui = normalize_cui(cui)
                if not n_cui:
                    continue
                ancestors = nx.ancestors(G, n_cui) or {n_cui}
                descendants = nx.descendants(G, n_cui) or {n_cui}
                final_results[n_cui] = {
                    "ancestors": sorted(ancestors),
                    "descendants": sorted(descendants),
                    "total_ancestor_count": len(ancestors),
                    "total_descendant_count": len(descendants),
                }
            # Save after each retry batch
            with open(output_file, "w") as f:
                json.dump(final_results, f)
        if failed_cuis_total:
            logger.warning(f"After retry {retry+1}, {len(failed_cuis_total)} CUIs still failed")

    logger.info("Pipeline completed successfully.")
    if failed_cuis_total:
        logger.error(f"CUIs that permanently failed: {failed_cuis_total}")

# ---------------- Run ----------------
async def main():
    target_cuis = ['C0003261', 'C0003264', 'C0003313', 'C0003314', 'C0004368', 'C0005532', 'C0005903', 'C0009528', 'C0009546', 'C0009547',
                  'C0018129', 'C0018133', 'C0019627', 'C0019699', 'C0020523', 'C0020964', 'C0020966', 'C0020967', 'C0020969', 'C0021055',
                  'C0021080', 'C0027651', 'C0035203', 'C0037088', 'C0040735', 'C0085416', 'C0152036', 'C0178498', 'C0178575', 'C0178709', 
                  'C0206120', 'C0239998', 'C0282558', 'C0301896', 'C0312535', 'C0338370', 'C0427349', 'C0428033', 'C0428047', 'C0457936', 
                  'C0458064', 'C0458065', 'C0458066', 'C0458067', 'C0458068', 'C0458069', 'C0458070', 'C0458071', 'C0458072', 'C0458073',
                  'C0458074', 'C0481430', 'C0517595', 'C0517597', 'C0517599', 'C0518656', 'C0518883', 'C0520990', 'C0582172', 
                  'C0588457', 'C0596117', 'C0596177', 'C0596829', 'C0597140', 'C0678209', 'C0678656', 'C0678852', 'C0678885', 'C0700205', 
                  'C0749722', 'C0920352', 'C1135927', 'C1140093', 'C1140118', 'C1140162', 'C1140168', 'C1262477', 'C1274019',
                  'C1277676', 'C1286235', 'C1287242', 'C1305430', 'C1319220', 'C1319289', 'C1320403', 'C1320991', 'C1443394',
                  'C1513731', 'C1553844', 'C1553931', 'C1609987', 'C1699089', 'C1817756', 'C1820730', 'C1820731', 'C1820732', 'C1820733', 
                  'C1820734', 'C1820735', 'C1820736', 'C2711893', 'C3714634', 'C4716274', 'C5234945', 'C5699729', 'C5788868', 
                  'C5788869', 'C5788870', 'C5788871', 'C5788908', 'C5788909', 'C5788910', 'C5788911', 'C5788912', 'C5788913', 'C5788914',
                  'C5788915', 'C5788916', 'C5788917', 'C5788918', 'C5788919', 'C5788920', 'C5788921', 'C5788922', 'C5788923', 'C5788924',
                  'C5788925', 'C5788926', 'C5788927', 'C5788928', 'C5788929', 'C5788930', 'C5817380'] 
    limit_context = ['LNC.Empty', 'SNOMEDCT_US.Empty', 'SNOMEDCT_US.isa',
        'ICD10.Empty', 'ICD10AM.Empty', 'ICD10CM.Empty',
        'ICD10PCS.Empty', 'CCSR_ICD10PCS.Empty']
    await run_pipeline(target_cuis, limit_context)

await main()
