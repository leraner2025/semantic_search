# ============================================================
# STANDARD LIBRARIES
# ============================================================
import logging
import time
import threading
import subprocess
import re
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple, Optional, Set, Iterable, Any
from collections import defaultdict, deque

# ============================================================
# THIRD PARTY
# ============================================================
import numpy as np
import requests
import networkx as nx
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from google.cloud import bigquery
from sklearn.cluster import AgglomerativeClustering

# -------------------------
# Logging
# -------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# -------------------------
# CUI Extraction API Client
# -------------------------
class CUIAPIClient:
    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 5):
        self.api_base_url = api_base_url.rstrip("/")
        self.timeout = timeout
        self.top_k = top_k
        self.session = requests.Session()

    def _refresh_token(self):
        with self._token_lock:
            now = time.time()
            if self._cached_token and now < self._token_expiry:
                return self._cached_token

            result = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                universal_newlines=True,
            )
            token = result.stdout.strip()
            self._cached_token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            self._token_expiry = now + 3300
            return self._cached_token

    def extract_cuis_batch(self, texts: List[str]) -> Set[str]:
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = self._refresh_token()

        response = self.session.post(
            self.api_base_url,
            json=payload,
            headers=headers,
            timeout=self.timeout
        )
        response.raise_for_status()
        data = response.json()

        cuis = set()
        for t in texts:
            cuis.update(map(str, data.get(t, [])))
        logger.info(f"Extracted {len(cuis)} CUIs")
        return cuis

# -------------------------
# Filter CUIs by allowed vocabularies
# -------------------------
def filter_allowed_cuis(cuis: Set[str], project_id: str, dataset_id: str) -> List[str]:
    if not cuis:
        return []
    try:
        client = bigquery.Client(project=project_id)
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN ('ICD10', 'ICD9', 'SNOMEDCT_US', 'LOINC')
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))]
        )
        df = client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        allowed_cuis = df['CUI'].tolist()
        logger.info(f"{len(allowed_cuis)} CUIs allowed after SAB filtering")
        return allowed_cuis
    except Exception as e:
        logger.error(f"Failed to filter CUIs by SAB: {str(e)}")
        return []

# -------------------------
# Group by Semantic Type
# -------------------------
def group_cuis_by_semantic_type(client: bigquery.Client, project_id: str,
                                dataset_id: str, cuis: List[str], batch_size=5000) -> Dict[str, List[str]]:
    groups = defaultdict(list)
    for i in range(0, len(cuis), batch_size):
        batch = cuis[i:i + batch_size]
        query = f"""
        SELECT CUI, TUI
        FROM `{project_id}.{dataset_id}.MRSTY`
        WHERE CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", batch)]
        )
        df = client.query(query, job_config=job_config).result().to_dataframe()
        for _, row in df.iterrows():
            groups[row["TUI"]].append(row["CUI"])
    logger.info(f"Grouped into {len(groups)} semantic types")
    return groups

# -------------------------
# Subnet API for hierarchy
# -------------------------
class SubnetAPIClient:
    def __init__(self, subnet_url: str):
        self.url = subnet_url.rstrip("/")

    def _get_token(self):
        result = subprocess.run(
            ["gcloud", "auth", "print-identity-token"],
            stdout=subprocess.PIPE,
            universal_newlines=True,
        )
        token = result.stdout.strip()
        return {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    def get_subnet(self, cuis: List[str]) -> Dict:
        headers = self._get_token()
        payload = {"cuis": cuis, "cross_context": False}
        response = requests.post(f"{self.url}/subnet/", json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        nodes, edges = response.json().get("output")
        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(cuis)
        for parent, child in edges:
            parent_to_children[parent].append(child)
            child_to_parents[child].append(parent)
            all_cuis.update([parent, child])
        return {"child_to_parents": dict(child_to_parents),
                "parent_to_children": dict(parent_to_children),
                "all_cuis": all_cuis}

# -------------------------
# Compute IC scores
# -------------------------
def compute_ic_scores(hierarchy: Dict) -> Dict[str, float]:
    parent_to_children = hierarchy["parent_to_children"]
    all_cuis = hierarchy["all_cuis"]
    total = len(all_cuis)
    descendant_counts = {}

    def dfs(cui):
        if cui in descendant_counts:
            return descendant_counts[cui]
        count = 0
        for ch in parent_to_children.get(cui, []):
            count += 1 + dfs(ch)
        descendant_counts[cui] = count
        return count

    for cui in all_cuis:
        dfs(cui)

    ic_scores = {cui: -np.log((descendant_counts[cui] + 1) / total) for cui in all_cuis}
    return ic_scores

# -------------------------
# Rollup LIA + MIA
# -------------------------
def rollup_cuis(cuis: List[str], hierarchy: Dict, ic_scores: Dict[str, float], ic_threshold: float):
    child_to_parents = hierarchy["child_to_parents"]
    parent_to_children = hierarchy["parent_to_children"]
    rolled = set()

    for cui in cuis:
        # LIA: Least Informative Ancestor
        queue = deque([cui])
        visited = set()
        while queue:
            cur = queue.popleft()
            for p in child_to_parents.get(cur, []):
                if p not in visited:
                    visited.add(p)
                    queue.append(p)
        lia_candidates = [c for c in visited if ic_scores.get(c, 0) >= ic_threshold]

        # MIA: Most Informative Descendants
        queue = deque([cui])
        visited = set()
        while queue:
            cur = queue.popleft()
            for ch in parent_to_children.get(cur, []):
                if ch not in visited:
                    visited.add(ch)
                    queue.append(ch)
        mia_candidates = [c for c in visited if ic_scores.get(c, 0) >= ic_threshold]

        rolled.update(lia_candidates + mia_candidates + [cui])

    # Remove duplicates and ancestor/descendant overlaps
    final_set = set(rolled)
    for cui in list(final_set):
        descendants = set()
        queue = deque([cui])
        while queue:
            queue.extend(parent_to_children.get(queue.popleft(), []))
            descendants.update(queue)
        final_set -= descendants

    return list(final_set)

# -------------------------
# Cluster by embeddings & pick far-from-median representatives
# -------------------------
def cluster_and_select_diverse_representatives(grouped_cuis: Dict[str, List[str]],
                                               embeddings: Dict[str, np.ndarray],
                                               similarity_threshold: float = 0.88,
                                               distance_from_median_threshold: float = 0.5) -> List[str]:
    final_cuis = []
    for tui, cuis in grouped_cuis.items():
        valid_cuis = [c for c in cuis if c in embeddings]
        if not valid_cuis:
            continue
        vectors = np.vstack([embeddings[c] for c in valid_cuis])

        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=1 - similarity_threshold,
            metric="cosine",
            linkage="average"
        )
        labels = clustering.fit_predict(vectors)

        for label in np.unique(labels):
            cluster_indices = np.where(labels == label)[0]
            cluster_cuis = [valid_cuis[i] for i in cluster_indices]
            cluster_vectors = vectors[cluster_indices]

            median_vector = np.median(cluster_vectors, axis=0)
            norm = np.linalg.norm(cluster_vectors, axis=1) * np.linalg.norm(median_vector)
            norm[norm == 0] = 1e-10
            cosine_sim = np.dot(cluster_vectors, median_vector) / norm
            cosine_dist = 1 - cosine_sim

            selected = [cluster_cuis[i] for i, dist in enumerate(cosine_dist)
                        if dist >= distance_from_median_threshold]
            if not selected:
                selected = [cluster_cuis[0]]

            final_cuis.extend(selected)

    return sorted(set(final_cuis))

# -------------------------
# Main pipeline
# -------------------------
def main():
    PROJECT_ID = project_id
    DATASET_ID = dataset
    EMBEDDING_TABLE = embedding_table
    CUI_API_URL = url
    SUBNET_API_URL = subnet

    texts = ["Tumor size impacts symptoms"]

    # Step 1: Extract CUIs
    extractor = CUIAPIClient(CUI_API_URL)
    raw_cuis = extractor.extract_cuis_batch(texts)

    # Step 2: Filter by allowed vocabularies
    filtered_cuis = filter_allowed_cuis(raw_cuis, PROJECT_ID, DATASET_ID)
    logger.info(f"Filtered CUIs count: {len(filtered_cuis)}")
    if not filtered_cuis:
        return

    # Step 3: Group by semantic type
    client = bigquery.Client(project=PROJECT_ID)
    grouped_cuis = group_cuis_by_semantic_type(client, PROJECT_ID, DATASET_ID, filtered_cuis)

    # Step 4: Build hierarchy & compute IC
    subnet_client = SubnetAPIClient(SUBNET_API_URL)
    hierarchy = subnet_client.get_subnet(filtered_cuis)
    ic_scores = compute_ic_scores(hierarchy)
    ic_threshold = np.percentile(list(ic_scores.values()), 50)  # configurable

    # Step 5-7: LIA + MIA + deduplication
    rolled_cuis = rollup_cuis(filtered_cuis, hierarchy, ic_scores, ic_threshold)

    # Step 8: Load embeddings (pseudo: replace with your embeddings loader)
    embeddings = {}  # {CUI: np.ndarray}
    # TODO: Load only for rolled_cuis from your BigQuery embeddings table

    # Step 9-10: Cluster & select diverse representatives
    final_cuis = cluster_and_select_diverse_representatives({ "all": rolled_cuis }, embeddings)

    logger.info(f"Final Reduced CUIs ({len(final_cuis)}): {final_cuis}")

if __name__ == "__main__":
    main()
-------------------------------------------------------
Pipeline Outline

NER → CUIs
 CUI extraction API for the input texts.

Filter by allowed vocabularies
    Use MRCONSO table to keep only ICD, SNOMED, LOINC.

Group by semantic type
    Use MRSTY to group CUIs by their semantic type (TUI).
    
use Subnetwork API to get the cui relationship (ancessots and decendents)

for each group, Compute IC scores
    Build hierarchy from above Subnet API → compute Information Content (IC) for each CUI.

Select LIA (Least Informative Ancestors)

    For each cluster: find most general ancestors in hierarchy.

    Keep only those with IC ≥ threshold.

Select MIA (Most Informative Ancestors)

    For each cluster: find most specific descendants.

Keep only those with IC ≥ threshold.

    Combine LIA + MIA selections

Remove duplicates and CUIs that are ancestors/descendants of each other to reduce redundancy.

Fetch embeddings

    Load embeddings only for remaining CUIs.

Cluster by embeddings (semantic clustering)

    Agglomerative clustering using cosine distance.

Select representatives per cluster

    Pick CUIs based on distance within cluster (far apart = keep, close = remove).
