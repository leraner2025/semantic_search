

# ============================================================
# LOGGING
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ============================================================
# ---------------- CODE 1 : SUBNET API CLIENT ----------------
# ============================================================

CUI_RE = re.compile(r"^C\d{7}(?:-\d+)?$")

class AugmentCUI:
    """
    Subnet API client (fully inlined from Code 1)
    """

    def __init__(self, url: str):
        self.url = url.rstrip("/")

    def get_token(self) -> Dict[str, str]:
        try:
            result = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True,
            )
            if result.returncode != 0:
                raise RuntimeError(result.stderr)

            token = result.stdout.strip()
            return {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
            }
        except Exception as e:
            raise RuntimeError(f"Failed to retrieve identity token: {e}")

    def get_subnet(
        self,
        input_cuis: Iterable[str],
        limit_context: Optional[List[str]] = None,
        cross_context: bool = False,
    ) -> Any:

        payload = {
            "cuis": list(input_cuis),
            "cross_context": cross_context,
        }
        if limit_context is not None:
            payload["limit_context"] = limit_context

        headers = self.get_token()
        response = requests.post(
            f"{self.url}/subnet/",
            headers=headers,
            json=payload,
            timeout=60,
        )
        response.raise_for_status()
        return response.json().get("output")


# ============================================================
# ---------------- MRCONSO FILTER ----------------
# ============================================================
def filter_allowed_cuis(
    cuis: Set[str],
    project_id: str,
    dataset_id: str,
) -> List[str]:
    if not cuis:
        return []

    try:
        client = bigquery.Client(project=project_id)
        query = f"""
        SELECT DISTINCT CUI
        FROM `{project_id}.{dataset_id}.MRCONSO`
        WHERE CUI IN UNNEST(@cuis)
          AND SAB IN (
            'ICD10','ICD10CM','ICD10PCS','ICD9CM',
            'SNOMEDCT_US','LOINC'
          )
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ArrayQueryParameter("cuis", "STRING", list(cuis))
            ]
        )
        df = client.query(query, job_config=job_config).result(timeout=60).to_dataframe()
        return df["CUI"].tolist()

    except Exception as e:
        logger.error(f"SAB filtering failed: {e}")
        return []


# ============================================================
# ---------------- REDUCTION STATS ----------------
# ============================================================
@dataclass
class ReductionStats:
    initial_count: int
    after_ic_rollup: int
    final_count: int
    ic_rollup_reduction_pct: float
    semantic_clustering_reduction_pct: float
    total_reduction_pct: float
    processing_time: float
    ic_threshold_used: float
    hierarchy_size: int
    api_call_time: float = 0.0

    def to_dict(self):
        return asdict(self)


# ============================================================
# ---------------- CUI EXTRACTION API ----------------
# ============================================================
class CUIAPIClient:

    _token_lock = threading.Lock()
    _cached_token = None
    _token_expiry = 0

    def __init__(self, api_base_url: str, timeout: int = 60, top_k: int = 3):
        self.api_base_url = api_base_url.rstrip("/")
        self.timeout = timeout
        self.top_k = top_k

        self.session = requests.Session()
        adapter = HTTPAdapter(
            max_retries=Retry(
                total=3,
                backoff_factor=1,
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=["POST"],
            )
        )
        self.session.mount("https://", adapter)
        self._refresh_token()

    def _refresh_token(self, force=False):
        with self._token_lock:
            now = time.time()
            if not force and self._cached_token and now < self._token_expiry:
                return self._cached_token

            result = subprocess.run(
                ["gcloud", "auth", "print-identity-token"],
                stdout=subprocess.PIPE,
                universal_newlines=True,
            )
            token = result.stdout.strip()
            self._cached_token = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
            }
            self._token_expiry = now + 3300
            return self._cached_token

    def extract_cuis_batch(self, texts: List[str]) -> Set[str]:
        payload = {"query_texts": texts, "top_k": self.top_k}
        headers = self._refresh_token()

        response = self.session.post(
            self.api_base_url,
            json=payload,
            headers=headers,
            timeout=self.timeout,
        )
        response.raise_for_status()
        data = response.json()

        cuis = set()
        for t in texts:
            cuis.update(map(str, data.get(t, [])))
        return cuis


# ============================================================
# ---------------- SUBNET-BASED REDUCER ----------------
# ============================================================
class EnhancedCUIReducer:

    def __init__(
        self,
        project_id: str,
        dataset_id: str,
        subnet_api_url: str,
        cui_embeddings_table: str,
    ):
        self.project_id = project_id
        self.dataset_id = dataset_id

        self.client = bigquery.Client(project=project_id)
        self.cui_embeddings_table = cui_embeddings_table

        self.subnet_client = AugmentCUI(subnet_api_url)

        self._hierarchy_cache = None
        self._ic_scores_cache = None

    def _build_hierarchy_safe(self, cuis: List[str]) -> Dict:
        if self._hierarchy_cache:
            return self._hierarchy_cache

        child_to_parents = defaultdict(list)
        parent_to_children = defaultdict(list)
        all_cuis = set(cuis)

        logger.info("Fetching hierarchy via subnet API...")
        subnet = self.subnet_client.get_subnet(cuis)
        nodes, edges = subnet

        for parent, child in edges:
            parent_to_children[parent].append(child)
            child_to_parents[child].append(parent)
            all_cuis.update([parent, child])

        hierarchy = {
            "child_to_parents": dict(child_to_parents),
            "parent_to_children": dict(parent_to_children),
            "all_cuis": all_cuis,
        }

        self._hierarchy_cache = hierarchy
        return hierarchy

    def _compute_ic_scores_safe(self, hierarchy: Dict) -> Dict[str, float]:
        if self._ic_scores_cache:
            return self._ic_scores_cache

        parent_to_children = hierarchy["parent_to_children"]
        all_cuis = hierarchy["all_cuis"]
        total = len(all_cuis)

        descendant_counts = {}

        def dfs(cui):
            if cui in descendant_counts:
                return descendant_counts[cui]
            count = 0
            for ch in parent_to_children.get(cui, []):
                count += 1 + dfs(ch)
            descendant_counts[cui] = count
            return count

        for cui in all_cuis:
            dfs(cui)

        ic_scores = {
            cui: -np.log((descendant_counts[cui] + 1) / total)
            for cui in all_cuis
        }

        self._ic_scores_cache = ic_scores
        return ic_scores

    def _semantic_rollup(self, cuis, hierarchy, ic_scores, threshold):
        child_to_parents = hierarchy["child_to_parents"]
        rolled = {}

        for cui in cuis:
            visited = set()
            queue = deque([cui])
            candidates = [cui]

            while queue:
                cur = queue.popleft()
                for p in child_to_parents.get(cur, []):
                    if p not in visited:
                        visited.add(p)
                        candidates.append(p)
                        queue.append(p)

            valid = [c for c in candidates if ic_scores.get(c, 0) >= threshold]
            rolled[cui] = min(valid, key=lambda c: ic_scores[c]) if valid else cui

        return list(set(rolled.values()))

    def _semantic_clustering(self, cuis, ic_scores, threshold):
        if len(cuis) <= 1:
            return cuis

        query = f"""
        SELECT REF_CUI as cui, REF_Embedding as embedding
        FROM `{self.project_id}.{self.dataset_id}.{self.cui_embeddings_table}`
        WHERE REF_CUI IN UNNEST(@cuis)
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[bigquery.ArrayQueryParameter("cuis", "STRING", cuis)]
        )
        df = self.client.query(query, job_config=job_config).result().to_dataframe()

        if df.empty:
            return cuis

        embeddings = np.vstack(df["embedding"])
        labels = AgglomerativeClustering(
            n_clusters=None,
            metric="cosine",
            linkage="average",
            distance_threshold=1 - threshold,
        ).fit_predict(embeddings)

        final = []
        for label in np.unique(labels):
            cluster = df["cui"][labels == label].tolist()
            final.append(min(cluster, key=lambda c: ic_scores.get(c, float("inf"))))

        return final

    def reduce(self, cuis, ic_percentile=50, semantic_threshold=0.88):
        start = time.time()

        hierarchy = self._build_hierarchy_safe(cuis)
        ic_scores = self._compute_ic_scores_safe(hierarchy)
        threshold = np.percentile(list(ic_scores.values()), ic_percentile)

        rolled = self._semantic_rollup(cuis, hierarchy, ic_scores, threshold)
        final = self._semantic_clustering(rolled, ic_scores, semantic_threshold)

        stats = ReductionStats(
            initial_count=len(cuis),
            after_ic_rollup=len(rolled),
            final_count=len(final),
            ic_rollup_reduction_pct=(1 - len(rolled) / len(cuis)) * 100,
            semantic_clustering_reduction_pct=(1 - len(final) / len(rolled)) * 100,
            total_reduction_pct=(1 - len(final) / len(cuis)) * 100,
            processing_time=time.time() - start,
            ic_threshold_used=threshold,
            hierarchy_size=len(hierarchy["all_cuis"]),
        )

        return final, stats


# ============================================================
# ---------------- MAIN ----------------
# ============================================================
def main():
    # -------- CONFIG --------
    PROJECT_ID = project_id
    DATASET_ID = dataset
    CUI_API_URL = url
    SUBNET_API_URL = subnet
    EMBEDDING_TABLE = embedding_table

    TEXTS = ["Tumor size impacts symptoms"]

    # -------- EXTRACT CUIS --------
    extractor = CUIAPIClient(CUI_API_URL)
    extracted_cuis = extractor.extract_cuis_batch(TEXTS)
    logger.info(f"Extracted {len(extracted_cuis)} CUIs")

    # -------- FILTER --------
    filtered_cuis = filter_allowed_cuis(extracted_cuis, PROJECT_ID, DATASET_ID)
    logger.info(f"{len(filtered_cuis)} CUIs after filtering")

    if not filtered_cuis:
        logger.warning("No CUIs to reduce")
        return

    # -------- REDUCE --------
    reducer = EnhancedCUIReducer(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        subnet_api_url=SUBNET_API_URL,
        cui_embeddings_table=EMBEDDING_TABLE,
    )

    reduced_cuis, stats = reducer.reduce(filtered_cuis)

    # -------- OUTPUT --------
    print("\nInitial CUIs:", filtered_cuis)
    print("\nReduced CUIs:", reduced_cuis)
    print("\nStats:")
    for k, v in stats.to_dict().items():
        print(f"  {k}: {v}")


# ============================================================
# ENTRY POINT
# ============================================================
if __name__ == "__main__":
    main()
