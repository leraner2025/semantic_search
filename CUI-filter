import requests
import csv
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from google.cloud import bigquery, aiplatform
from google.cloud import storage, bigquery, bigquery_storage
import vertexai
from vertexai.language_models import TextEmbeddingModel
import subprocess
import time

# Initialize Vertex AI
aiplatform.init(project=project_id, location="us-central1")

GEMINI_EMBEDDING_MODEL = "gemini-embedding-001"
gemini_model = TextEmbeddingModel.from_pretrained(GEMINI_EMBEDDING_MODEL)


# GCP Auth Header Setup

headers = None

def gcp_update_header():
    global headers
    tmp = subprocess.run(['gcloud', 'auth', 'print-identity-token'], stdout=subprocess.PIPE, universal_newlines=True)
    if tmp.returncode != 0:
        raise Exception("Cannot get GCP access token")
    identity_token = tmp.stdout.strip()
    headers = {
        "Authorization": f"Bearer {identity_token}",
        "Content-Type": "application/json"
    }

gcp_update_header()

# API Call: NER CUI Extraction

def call_ner_api(text):
    url = url
    payload = {
        "query_texts": [text],
        "top_k": 3
    }

    try:
        response = requests.post(url, json=payload, headers=headers)
        print(f" Raw response status: {response.status_code}")
        response.raise_for_status()
        data = response.json()

        # Count CUIs for the specific input text
        cui_count = len(data.get(text, []))
        print(f" API call successful. CUIs returned: {cui_count}")

        return data
    except requests.exceptions.RequestException as e:
        print(f"Request failed: {e}")
    except json.JSONDecodeError as e:
        print(f" JSON decode error: {e}")
    return {}

def timed(label):
    def wrapper(func):
        def inner(*args, **kwargs):
            start = time.time()
            result = func(*args, **kwargs)
            print(f"{label} took {time.time() - start:.2f} sec")
            return result
        return inner
    return wrapper

# Step 1: Extract CUIs from NER response
@timed("Step 1 - CUI extraction")
def extract_cuis(ner_response, input_text, max_cuis=1):
    cuis = list(set(ner_response.get(input_text, [])))
    cuis = cuis[:max_cuis]
    print(f"CUIs extracted: {len(cuis)}")
    return cuis

# Step 2: Group CUIs by ancestry
@timed("Step 2 - Ancestry grouping")
def get_ref_cuis_and_group_nodes(client, project_id, dataset, parent_child_table, ner_cuis):
    if not ner_cuis:
        return {}
    cuis_str = ",".join([f"'{c}'" for c in ner_cuis])
    query = f"""
    WITH input_nodes AS (
        SELECT node_cui FROM UNNEST([{cuis_str}]) AS node_cui
    )
    SELECT REF_CUI, node_cui
    FROM `{project_id}.{dataset}.{parent_child_table}`,
    UNNEST(NODE_CUI_LIST) AS node_cui
    WHERE node_cui IN (SELECT node_cui FROM input_nodes)
    """
    results = client.query(query).result()
    grouped = {}
    for row in results:
        grouped.setdefault(row.REF_CUI, []).append(row.node_cui)
    return grouped

@timed("Step 3 - Depth-layered filtering")
def get_deepest_per_layer(client, project_id, dataset, parent_child_table, groups):
    all_nodes = [c for nodes in groups.values() for c in nodes]
    if not all_nodes:
        return []

    cuis_str = ",".join([f"'{c}'" for c in all_nodes])

    query = f"""
    WITH RECURSIVE hierarchy AS (
        SELECT node_cui, REF_CUI, 1 AS depth
        FROM `{project_id}.{dataset}.{parent_child_table}`
        WHERE REF_CUI IS NULL AND node_cui IN ({cuis_str})

        UNION ALL

        SELECT child.node_cui, child.REF_CUI, parent.depth + 1
        FROM `{project_id}.{dataset}.{parent_child_table}` child
        JOIN hierarchy parent ON child.REF_CUI = parent.node_cui
        WHERE child.node_cui IN ({cuis_str})
    )
    SELECT node_cui, MAX(depth) AS depth
    FROM hierarchy
    GROUP BY node_cui
    """

    results = client.query(query).result()
    depths = {row.node_cui: row.depth for row in results}

    # Group CUIs by depth layer
    from collections import defaultdict
    layered_nodes = defaultdict(list)
    for cui, depth in depths.items():
        layered_nodes[depth].append(cui)

    # Select deepest CUIs per layer
    final_cuis = []
    for layer in sorted(layered_nodes.keys()):
        layer_cuis = layered_nodes[layer]
        final_cuis.extend(layer_cuis)

    final_cuis = list(set(final_cuis))
    print(f"CUIs selected across layers: {len(final_cuis)}")
    return final_cuis

# Step 4: Retrieve embeddings
@timed("Step 4 - Embedding retrieval")
def get_cui_embeddings(client, project_id, dataset, embedding_table, cuis):
    if not cuis:
        return {}
    cuis_str = ",".join([f"'{c}'" for c in cuis])
    query = f"""
    SELECT REF_CUI, REF_Embedding
    FROM `{project_id}.{dataset}.{embedding_table}`
    WHERE REF_CUI IN UNNEST([{cuis_str}])
    """
    results = client.query(query).result()
    return {row.REF_CUI: row.REF_Embedding for row in results}

# Step 5: Compute cosine similarity
@timed("Step 5 - Similarity scoring")
def compute_similarity(note_embedding, cui_embeddings, threshold=0.5):
    if not cui_embeddings:
        return []
    note_vec = np.array(note_embedding).reshape(1, -1)
    cui_ids = list(cui_embeddings.keys())
    cui_matrix = np.array([cui_embeddings[cui] for cui in cui_ids])
    scores = cosine_similarity(note_vec, cui_matrix)[0]
    filtered = [cui for cui, score in zip(cui_ids, scores) if score >= threshold]
    print(f"CUIs above threshold ({threshold}): {len(filtered)}")
    return filtered

# Step 6: Export to CSV
def export_to_csv(cui_list, filename="final_cuis.csv"):
    df = pd.DataFrame({"CUI": cui_list})
    df.to_csv(filename, index=False)
    print(f" Exported {len(cui_list)} CUIs to {filename}")

# Step 8:pipeline
def run_pipeline(text, note_embedding, project_id, dataset, parent_child_table, embedding_table, export_csv=True, write_bq=False, bq_table_name=None):
    client = bigquery.Client()

    ner_response = call_ner_api(text)
    ner_cuis = extract_cuis(ner_response, text)
    groups = get_ref_cuis_and_group_nodes(client, project_id, dataset, parent_child_table, ner_cuis)
    deepest_cuis = get_deepest_per_layer(client, project_id, dataset, parent_child_table, groups)
    cui_embeddings = get_cui_embeddings(client, project_id, dataset, embedding_table, deepest_cuis)
    final_cuis = compute_similarity(note_embedding, cui_embeddings, threshold=0.5)

    if export_csv:
        export_to_csv(final_cuis)

    # if write_bq and bq_table_name:
    #     write_to_bigquery(client, final_cuis, project_id, dataset, bq_table_name)

    return final_cuis

# Example usage
if __name__ == "__main__":
    text = "mris for 6 months"
    note_embedding = np.random.rand(3072).tolist()
    project_id = project_id
    dataset = dataset
    parent_child_table = parent_child_table
    embedding_table = embedding_table
    # bq_table_name = bq_table_name

    final_cuis = run_pipeline(
        text,
        note_embedding,
        project_id,
        dataset,
        parent_child_table,
        embedding_table,
        export_csv=True,
        write_bq=True,
        # bq_table_name=bq_table_name
    )

    print(f"\n Final CUIs selected: {len(final_cuis)}")
