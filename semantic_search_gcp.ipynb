{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldooZ-eZLqxj"
      },
      "outputs": [],
      "source": [
        "# === Install dependencies\n",
        "!pip install google-cloud-bigquery pandas scann hdbscan umap-learn nltk sentence-transformers bertopic --quiet\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "from google.cloud import bigquery\n",
        "import ast\n",
        "import json\n",
        "\n",
        "# === Set random seed for reproducibility ===\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "# === Setup logging ===\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# === Download tokenizer ===\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# === Function to read and normalize BigQuery data ===\n",
        "def load_data_from_bigquery(project_id, dataset_id, table_id):\n",
        "    client = bigquery.Client()\n",
        "    full_table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT chunk, entities\n",
        "    FROM `{full_table_path}`\n",
        "    \"\"\"\n",
        "    df = client.query(query).to_dataframe()\n",
        "\n",
        "    def parse_entities(val):\n",
        "        if isinstance(val, list):\n",
        "            return val\n",
        "        elif isinstance(val, str):\n",
        "            val = val.strip()\n",
        "            try:\n",
        "                # Try JSON first (e.g., '[\"a\", \"b\"]')\n",
        "                parsed = json.loads(val)\n",
        "                if isinstance(parsed, list):\n",
        "                    return parsed\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "\n",
        "            try:\n",
        "                # Try Python literal (e.g., \"['a', 'b']\")\n",
        "                parsed = ast.literal_eval(val)\n",
        "                if isinstance(parsed, list):\n",
        "                    return parsed\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Fallback: comma-separated string\n",
        "            return [e.strip() for e in val.split(',') if e.strip()]\n",
        "\n",
        "        return []\n",
        "\n",
        "    df['entities'] = df['entities'].apply(parse_entities)\n",
        "\n",
        "    # Final output format\n",
        "    chunks = df['chunk'].tolist()\n",
        "    manual_entities_per_chunk = df['entities'].tolist()\n",
        "    return chunks, manual_entities_per_chunk\n",
        "\n",
        "\n",
        "# === AllergyTopicSearcher class ===\n",
        "class AllergyTopicSearcher:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, model_name=\"emilyalsentzer/Bio_ClinicalBERT\"):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model_name = model_name\n",
        "\n",
        "        self.embedding_model = None\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        try:\n",
        "            logger.info(\"Loading embedding model...\")\n",
        "            self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "            entity_to_chunk = defaultdict(list)\n",
        "            all_entities = []\n",
        "            for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "                for ent in ents:\n",
        "                    ent_lower = ent.lower()\n",
        "                    all_entities.append(ent_lower)\n",
        "                    entity_to_chunk[ent_lower].append(idx)\n",
        "\n",
        "            unique_entities = sorted(set(all_entities))\n",
        "            logger.info(f\"Unique entities found: {len(unique_entities)}\")\n",
        "\n",
        "            entity_embeddings = self.embedding_model.encode(unique_entities, normalize_embeddings=True)\n",
        "\n",
        "            umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=SEED)\n",
        "            hdbscan_model = HDBSCAN(min_cluster_size=2, min_samples=1, metric='euclidean', prediction_data=True)\n",
        "\n",
        "            self.topic_model = BERTopic(\n",
        "                embedding_model=self.embedding_model,\n",
        "                umap_model=umap_model,\n",
        "                hdbscan_model=hdbscan_model,\n",
        "                representation_model=KeyBERTInspired(),\n",
        "                calculate_probabilities=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            topics, _ = self.topic_model.fit_transform(unique_entities, embeddings=entity_embeddings)\n",
        "\n",
        "            topic_to_entities = defaultdict(list)\n",
        "            for ent, topic in zip(unique_entities, topics):\n",
        "                topic_to_entities[topic].append(ent)\n",
        "\n",
        "            topic_contexts = defaultdict(list)\n",
        "            for topic, entities in topic_to_entities.items():\n",
        "                for ent in entities:\n",
        "                    for chunk_id in entity_to_chunk[ent]:\n",
        "                        for sent in sent_tokenize(self.chunks[chunk_id]):\n",
        "                            if ent in sent.lower():\n",
        "                                topic_contexts[topic].append(sent)\n",
        "\n",
        "            for topic in topic_contexts:\n",
        "                topic_contexts[topic] = list(set(topic_contexts[topic]))\n",
        "\n",
        "            topic_embeddings = []\n",
        "            topic_metadata = []\n",
        "\n",
        "            for topic_id, sentences in topic_contexts.items():\n",
        "                if not sentences:\n",
        "                    continue\n",
        "                sent_embs = self.embedding_model.encode(sentences, normalize_embeddings=True)\n",
        "                mean_emb = np.mean(sent_embs, axis=0)\n",
        "                mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "                topic_embeddings.append(mean_emb)\n",
        "                topic_metadata.append({\n",
        "                    \"topic_id\": topic_id,\n",
        "                    \"entities\": topic_to_entities[topic_id],\n",
        "                    \"sentences\": sentences,\n",
        "                    \"sentence_embeddings\": sent_embs\n",
        "                })\n",
        "\n",
        "            self.topic_embeddings = np.array(topic_embeddings)\n",
        "\n",
        "            if len(self.topic_embeddings) == 0:\n",
        "                raise ValueError(\"No topic embeddings generated.\")\n",
        "\n",
        "            num_clusters = min(len(self.topic_embeddings), 5)\n",
        "            self.searcher = (\n",
        "                scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "                .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "                .score_brute_force()\n",
        "                .reorder(3)\n",
        "                .build()\n",
        "            )\n",
        "\n",
        "            self.topic_metadata = topic_metadata\n",
        "            logger.info(\"Preparation complete. Ready to search.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during preparation: {e}\")\n",
        "            raise\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=3):\n",
        "        if self.searcher is None or not self.topic_metadata:\n",
        "            raise RuntimeError(\"Searcher not initialized. Call _prepare() first.\")\n",
        "\n",
        "        print(f\"\\nüîé Query: '{query}'\\n{'=' * 60}\")\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        for rank, (idx, score) in enumerate(zip(neighbors, scores), 1):\n",
        "            meta = self.topic_metadata[idx]\n",
        "            print(f\"\\nRank {rank} [Score: {score:.4f}]\")\n",
        "            print(f\"  Topic ID   : {meta['topic_id']}\")\n",
        "            print(f\"  Entities   : {meta['entities']}\")\n",
        "\n",
        "            sents = meta[\"sentences\"]\n",
        "            sent_embs = meta[\"sentence_embeddings\"]\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "\n",
        "            print(f\"  Top {top_k_sents} Relevant Sentences:\")\n",
        "            for i in top_indices:\n",
        "                print(f\"    - {sents[i]} (score: {sims[i]:.4f})\")\n",
        "\n",
        "\n",
        "# === Entry point ===\n",
        "if __name__ == \"__main__\":\n",
        "    # üîÅ Replace with your actual values\n",
        "    project_id = \"your_project\"\n",
        "    dataset_id = \"your_dataset\"\n",
        "    table_id = \"chunk_entities_table\"\n",
        "\n",
        "    chunks, manual_entities_per_chunk = load_data_from_bigquery(project_id, dataset_id, table_id)\n",
        "\n",
        "    searcher = AllergyTopicSearcher(chunks, manual_entities_per_chunk)\n",
        "\n",
        "    # Test it with a query\n",
        "    searcher.search(\"What are skin allergy symptoms?\")\n"
      ],
      "metadata": {
        "id": "NwNwUkeHLxo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-- Step 1: Create the table with ARRAY<STRING> for entities\n",
        "CREATE TABLE `your_project.your_dataset.chunk_entities_table` (\n",
        "  chunk STRING,\n",
        "  entities ARRAY<STRING>\n",
        ");\n",
        "\n",
        "-- Step 2: Insert example chunks and associated entities\n",
        "INSERT INTO `your_project.your_dataset.chunk_entities_table` (chunk, entities)\n",
        "VALUES\n",
        "(\n",
        "  \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\",\n",
        "  [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"]\n",
        "),\n",
        "(\n",
        "  \"Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\",\n",
        "  [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"]\n",
        "),\n",
        "(\n",
        "  \"Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\",\n",
        "  [\"anaphylaxis\", \"allergic reaction\"]\n",
        "),\n",
        "(\n",
        "  \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\",\n",
        "  [\"food allergies\", \"milk\", \"eggs\"]\n",
        "),\n",
        "(\n",
        "  \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\",\n",
        "  [\"urticaria\", \"hives\", \"eczema\", \"allergies\"]\n",
        "),\n",
        "(\n",
        "  \"He walks in cold weather but has no allergy symptoms or reactions.\",\n",
        "  [\"cold weather\", \"allergy symptoms\", \"reactions\"]\n",
        ");\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#recomended bigquery schema:\n",
        "\n",
        "| Column Name | Type                                        | Description                                              |\n",
        "| ----------- | ------------------------------------------- | -------------------------------------------------------- |\n",
        "| `chunk`     | `STRING`                                    | The full chunk of text (a paragraph or document snippet) |\n",
        "| `entities`  | `ARRAY<STRING>` (preferred) **or** `STRING` | The list of entities found in the chunk                  |\n"
      ],
      "metadata": {
        "id": "eLEOaYpLOpqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4JaPE7oMNwvu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}