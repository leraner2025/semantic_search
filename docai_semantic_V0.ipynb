{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGSXicukh_ox"
   },
   "outputs": [],
   "source": [
    "#using SCANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIo_6PmFz23F"
   },
   "outputs": [],
   "source": [
    "#Install dependency\n",
    "!pip install google-cloud-documentai sentence-transformers scann numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBgH0rdGiWyw"
   },
   "outputs": [],
   "source": [
    "#including the paprargah,form fields,tables and entities:\n",
    "import re\n",
    "import json\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import scann\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "project_id = \"your-gcp-project-id\"\n",
    "location = \"us\"  # or 'eu'\n",
    "processor_id = \"your-processor-id\"\n",
    "gcs_input_uri = \"gs://your-bucket/your-file.pdf\"\n",
    "output_json_path = \"extracted_document.json\"\n",
    "\n",
    "# === SETUP CLIENT ===\n",
    "client = documentai.DocumentProcessorServiceClient()\n",
    "name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "# === HELPER FUNCTION: get text from text_anchor ===\n",
    "def get_text(text_anchor):\n",
    "    if not text_anchor or not text_anchor.text_segments:\n",
    "        return \"\"\n",
    "    text = \"\"\n",
    "    for segment in text_anchor.text_segments:\n",
    "        start = segment.start_index if segment.start_index else 0\n",
    "        end = segment.end_index if segment.end_index else 0\n",
    "        text += document.text[start:end]\n",
    "    return text.strip()\n",
    "\n",
    "# === HELPER FUNCTION: auto detect headers and extract sections ===\n",
    "def auto_extract_sections(page_text):\n",
    "    lines = page_text.splitlines()\n",
    "    sections = {}\n",
    "    current_header = None\n",
    "    current_content_lines = []\n",
    "\n",
    "    def save_section():\n",
    "        if current_header:\n",
    "            content = \"\\n\".join(current_content_lines).strip()\n",
    "            sections[current_header] = content\n",
    "\n",
    "    header_pattern = re.compile(r\".+:$\")  # line ends with colon\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        is_header = False\n",
    "        if header_pattern.match(stripped_line):\n",
    "            is_header = True\n",
    "            header_text = stripped_line[:-1].strip()  # remove colon\n",
    "        else:\n",
    "            words = stripped_line.split()\n",
    "            if 0 < len(words) <= 5 and all(word[0].isupper() for word in words):\n",
    "                is_header = True\n",
    "                header_text = stripped_line\n",
    "\n",
    "        if is_header:\n",
    "            save_section()\n",
    "            current_header = header_text\n",
    "            current_content_lines = []\n",
    "        else:\n",
    "            if current_header:\n",
    "                current_content_lines.append(stripped_line)\n",
    "    save_section()\n",
    "    return sections\n",
    "\n",
    "# === CONFIGURE REQUEST ===\n",
    "gcs_document = documentai.GcsDocument(gcs_uri=gcs_input_uri, mime_type=\"application/pdf\")\n",
    "input_config = documentai.BatchDocumentsInputConfig(gcs_documents=documentai.GcsDocuments(documents=[gcs_document]))\n",
    "\n",
    "request = documentai.ProcessRequest(\n",
    "    name=name,\n",
    "    input_documents=input_config\n",
    ")\n",
    "\n",
    "# === PROCESS DOCUMENT ===\n",
    "result = client.process_document(request=request)\n",
    "document = result.document\n",
    "\n",
    "# === BUILD OUTPUT DATA ===\n",
    "output_data = {\n",
    "    \"text\": document.text,\n",
    "    \"pages\": []\n",
    "}\n",
    "\n",
    "for page in document.pages:\n",
    "    page_text = get_text(page.layout.text_anchor)\n",
    "    page_data = {\n",
    "        \"page_number\": page.page_number,\n",
    "        \"form_fields\": [],\n",
    "        \"tables\": [],\n",
    "        \"entities\": [],\n",
    "        \"text_snippet\": page_text,\n",
    "        \"sections\": auto_extract_sections(page_text)\n",
    "    }\n",
    "\n",
    "    # Form fields\n",
    "    for field in page.form_fields:\n",
    "        field_name = get_text(field.field_name.text_anchor)\n",
    "        field_value = get_text(field.field_value.text_anchor)\n",
    "        page_data[\"form_fields\"].append({\n",
    "            \"field_name\": field_name,\n",
    "            \"field_value\": field_value\n",
    "        })\n",
    "\n",
    "    # Tables\n",
    "    for table in page.tables:\n",
    "        table_data = []\n",
    "        for row in table.header_rows + table.body_rows:\n",
    "            row_data = [get_text(cell.layout.text_anchor) for cell in row.cells]\n",
    "            table_data.append(row_data)\n",
    "        page_data[\"tables\"].append(table_data)\n",
    "\n",
    "    # Entities (if available)\n",
    "    for entity in getattr(document, \"entities\", []):\n",
    "        entity_text = get_text(entity.text_anchor)\n",
    "        entity_type = entity.type_ if hasattr(entity, \"type_\") else \"UNKNOWN\"\n",
    "        page_data[\"entities\"].append({\n",
    "            \"entity_type\": entity_type,\n",
    "            \"text\": entity_text\n",
    "        })\n",
    "\n",
    "    output_data[\"pages\"].append(page_data)\n",
    "\n",
    "# === SAVE TO JSON FILE ===\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Extracted data saved to: {output_json_path}\")\n",
    "\n",
    "# === SEMANTIC SEARCH SETUP ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "corpus = []\n",
    "texts = []\n",
    "\n",
    "for page in output_data.get(\"pages\", []):\n",
    "    # Sections (header + content)\n",
    "    for header, content in page.get(\"sections\", {}).items():\n",
    "        combined_text = f\"{header}: {content}\"\n",
    "        corpus.append({\n",
    "            \"type\": \"section\",\n",
    "            \"page\": page[\"page_number\"],\n",
    "            \"header\": header,\n",
    "            \"content\": content\n",
    "        })\n",
    "        texts.append(combined_text)\n",
    "\n",
    "    # Form fields\n",
    "    for field in page.get(\"form_fields\", []):\n",
    "        combined_text = f\"{field['field_name']}: {field['field_value']}\"\n",
    "        corpus.append({\n",
    "            \"type\": \"form_field\",\n",
    "            \"page\": page[\"page_number\"],\n",
    "            \"field_name\": field[\"field_name\"],\n",
    "            \"field_value\": field[\"field_value\"]\n",
    "        })\n",
    "        texts.append(combined_text)\n",
    "\n",
    "    # Tables (each row as a searchable text chunk)\n",
    "    for table_idx, table in enumerate(page.get(\"tables\", [])):\n",
    "        for row_idx, row in enumerate(table):\n",
    "            row_text = \" | \".join(row)\n",
    "            corpus.append({\n",
    "                \"type\": \"table_row\",\n",
    "                \"page\": page[\"page_number\"],\n",
    "                \"table_index\": table_idx,\n",
    "                \"row_index\": row_idx,\n",
    "                \"text\": row_text\n",
    "            })\n",
    "            texts.append(row_text)\n",
    "\n",
    "    # Entities\n",
    "    for entity in page.get(\"entities\", []):\n",
    "        entity_text = entity[\"text\"]\n",
    "        entity_type = entity[\"entity_type\"]\n",
    "        combined_text = f\"{entity_type}: {entity_text}\"\n",
    "        corpus.append({\n",
    "            \"type\": \"entity\",\n",
    "            \"page\": page[\"page_number\"],\n",
    "            \"entity_type\": entity_type,\n",
    "            \"text\": entity_text\n",
    "        })\n",
    "        texts.append(combined_text)\n",
    "\n",
    "# Generate embeddings for entire corpus\n",
    "corpus_embeddings = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Build SCANN index\n",
    "searcher = scann.scann_ops_pybind.builder(corpus_embeddings, 10, \"dot_product\").tree(\n",
    "    num_leaves=200, num_leaves_to_search=20, training_sample_size=250000).score_ah(\n",
    "    2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
    "\n",
    "# === SEARCH FUNCTION ===\n",
    "def semantic_search_scann(query, top_k=5):\n",
    "    query_embedding = model.encode(query, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    neighbors, distances = searcher.search(query_embedding, final_num_neighbors=top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx, dist in zip(neighbors, distances):\n",
    "        entry = corpus[idx]\n",
    "        results.append({\n",
    "            \"score\": float(dist),\n",
    "            \"metadata\": entry\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# === INTERACTIVE SEARCH ===\n",
    "while True:\n",
    "    search_term = input(\"\\nEnter your search query (or type 'exit' to quit): \").strip()\n",
    "    if search_term.lower() == \"exit\":\n",
    "        print(\"Exiting search.\")\n",
    "        break\n",
    "    if not search_term:\n",
    "        print(\"Please enter a non-empty query.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nSemantic search results for query: '{search_term}':\\n\")\n",
    "\n",
    "    results = semantic_search_scann(search_term, top_k=5)\n",
    "    if not results:\n",
    "        print(\"No results found.\\n\")\n",
    "        continue\n",
    "\n",
    "    for res in results:\n",
    "        md = res[\"metadata\"]\n",
    "        score = res[\"score\"]\n",
    "        if md[\"type\"] == \"section\":\n",
    "            print(f\"[Page {md['page']}] Section Header: {md['header']} (Score: {score:.3f})\")\n",
    "            print(f\"Content:\\n{md['content']}\\n\")\n",
    "        elif md[\"type\"] == \"form_field\":\n",
    "            print(f\"[Page {md['page']}] Form Field: {md['field_name']} (Score: {score:.3f})\")\n",
    "            print(f\"Value: {md['field_value']}\\n\")\n",
    "        elif md[\"type\"] == \"table_row\":\n",
    "            print(f\"[Page {md['page']}] Table Row (Table {md['table_index']} Row {md['row_index']}) (Score: {score:.3f})\")\n",
    "            print(f\"Text: {md['text']}\\n\")\n",
    "        elif md[\"type\"] == \"entity\":\n",
    "            print(f\"[Page {md['page']}] Entity ({md['entity_type']}) (Score: {score:.3f})\")\n",
    "            print(f\"Text: {md['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4k5jTG_0BFN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEhSqb9TxB-G"
   },
   "outputs": [],
   "source": [
    "#Using FAISS instead of SCANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD_FMjCNz8pu"
   },
   "outputs": [],
   "source": [
    "#Install dependency\n",
    "!pip install --upgrade google-cloud-documentai sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxWxqhFN2KmO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "project_id = \"your-gcp-project-id\"\n",
    "location = \"us\"\n",
    "processor_id = \"your-processor-id\"\n",
    "gcs_input_uri = \"gs://your-bucket/your-file.pdf\"\n",
    "output_json_path = \"extracted_document.json\"\n",
    "\n",
    "client = documentai.DocumentProcessorServiceClient()\n",
    "name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "def get_text(text_anchor):\n",
    "    if not text_anchor or not text_anchor.text_segments:\n",
    "        return \"\"\n",
    "    text = \"\"\n",
    "    for segment in text_anchor.text_segments:\n",
    "        start = segment.start_index or 0\n",
    "        end = segment.end_index or 0\n",
    "        text += document.text[start:end]\n",
    "    return text.strip()\n",
    "\n",
    "def auto_extract_sections(page_text):\n",
    "    lines = page_text.splitlines()\n",
    "    sections = {}\n",
    "    current_header = None\n",
    "    current_content_lines = []\n",
    "\n",
    "    def save_section():\n",
    "        if current_header:\n",
    "            content = \"\\n\".join(current_content_lines).strip()\n",
    "            sections[current_header] = content\n",
    "\n",
    "    header_pattern = re.compile(r\".+:$\")\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        is_header = False\n",
    "        if header_pattern.match(stripped_line):\n",
    "            is_header = True\n",
    "            header_text = stripped_line[:-1].strip()\n",
    "        else:\n",
    "            words = stripped_line.split()\n",
    "            if 0 < len(words) <= 5 and all(word[0].isupper() for word in words):\n",
    "                is_header = True\n",
    "                header_text = stripped_line\n",
    "\n",
    "        if is_header:\n",
    "            save_section()\n",
    "            current_header = header_text\n",
    "            current_content_lines = []\n",
    "        else:\n",
    "            if current_header:\n",
    "                current_content_lines.append(stripped_line)\n",
    "    save_section()\n",
    "    return sections\n",
    "\n",
    "# === PROCESS DOCUMENT ===\n",
    "gcs_document = documentai.GcsDocument(gcs_uri=gcs_input_uri, mime_type=\"application/pdf\")\n",
    "input_config = documentai.BatchDocumentsInputConfig(gcs_documents=documentai.GcsDocuments(documents=[gcs_document]))\n",
    "request = documentai.ProcessRequest(name=name, input_documents=input_config)\n",
    "\n",
    "result = client.process_document(request=request)\n",
    "document = result.document\n",
    "\n",
    "# === EXTRACT DATA ===\n",
    "output_data = {\"text\": document.text, \"pages\": []}\n",
    "\n",
    "for page in document.pages:\n",
    "    page_text = get_text(page.layout.text_anchor)\n",
    "    page_data = {\n",
    "        \"page_number\": page.page_number,\n",
    "        \"form_fields\": [],\n",
    "        \"tables\": [],\n",
    "        \"entities\": [],\n",
    "        \"text_snippet\": page_text,\n",
    "        \"sections\": auto_extract_sections(page_text),\n",
    "    }\n",
    "\n",
    "    for field in page.form_fields:\n",
    "        field_name = get_text(field.field_name.text_anchor)\n",
    "        field_value = get_text(field.field_value.text_anchor)\n",
    "        page_data[\"form_fields\"].append({\"field_name\": field_name, \"field_value\": field_value})\n",
    "\n",
    "    for table in page.tables:\n",
    "        table_data = []\n",
    "        for row in table.header_rows + table.body_rows:\n",
    "            row_data = [get_text(cell.layout.text_anchor) for cell in row.cells]\n",
    "            table_data.append(row_data)\n",
    "        page_data[\"tables\"].append(table_data)\n",
    "\n",
    "    for entity in getattr(document, \"entities\", []):\n",
    "        entity_text = get_text(entity.text_anchor)\n",
    "        entity_type = getattr(entity, \"type_\", \"UNKNOWN\")\n",
    "        page_data[\"entities\"].append({\"entity_type\": entity_type, \"text\": entity_text})\n",
    "\n",
    "    output_data[\"pages\"].append(page_data)\n",
    "\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Extracted data saved to: {output_json_path}\")\n",
    "\n",
    "# === SEMANTIC SEARCH SETUP (FAISS) ===\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "corpus = []\n",
    "texts = []\n",
    "\n",
    "for page in output_data[\"pages\"]:\n",
    "    for header, content in page.get(\"sections\", {}).items():\n",
    "        combined_text = f\"{header}: {content}\"\n",
    "        corpus.append({\"type\": \"section\", \"page\": page[\"page_number\"], \"header\": header, \"content\": content})\n",
    "        texts.append(combined_text)\n",
    "\n",
    "    for field in page.get(\"form_fields\", []):\n",
    "        combined_text = f\"{field['field_name']}: {field['field_value']}\"\n",
    "        corpus.append({\n",
    "            \"type\": \"form_field\", \"page\": page[\"page_number\"],\n",
    "            \"field_name\": field[\"field_name\"], \"field_value\": field[\"field_value\"]\n",
    "        })\n",
    "        texts.append(combined_text)\n",
    "\n",
    "    for table_idx, table in enumerate(page.get(\"tables\", [])):\n",
    "        for row_idx, row in enumerate(table):\n",
    "            row_text = \" | \".join(row)\n",
    "            corpus.append({\n",
    "                \"type\": \"table_row\", \"page\": page[\"page_number\"],\n",
    "                \"table_index\": table_idx, \"row_index\": row_idx, \"text\": row_text\n",
    "            })\n",
    "            texts.append(row_text)\n",
    "\n",
    "    for entity in page.get(\"entities\", []):\n",
    "        entity_text = entity[\"text\"]\n",
    "        entity_type = entity[\"entity_type\"]\n",
    "        combined_text = f\"{entity_type}: {entity_text}\"\n",
    "        corpus.append({\n",
    "            \"type\": \"entity\", \"page\": page[\"page_number\"],\n",
    "            \"entity_type\": entity_type, \"text\": entity_text\n",
    "        })\n",
    "        texts.append(combined_text)\n",
    "\n",
    "corpus_embeddings = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "# === SEMANTIC SEARCH FUNCTION ===\n",
    "def semantic_search_faiss(query, top_k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        results.append({\"score\": float(score), \"metadata\": corpus[idx]})\n",
    "    return results\n",
    "\n",
    "# === INTERACTIVE SEARCH ===\n",
    "while True:\n",
    "    query = input(\"\\nEnter your search query (or type 'exit'): \").strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "    if not query:\n",
    "        print(\"Please enter a query.\")\n",
    "        continue\n",
    "\n",
    "    results = semantic_search_faiss(query)\n",
    "    print(f\"\\nSemantic search results for: '{query}'\\n\")\n",
    "    for res in results:\n",
    "        md = res[\"metadata\"]\n",
    "        score = res[\"score\"]\n",
    "        print(f\"üìÑ Page {md['page']} | Type: {md['type']} | Score: {score:.3f}\")\n",
    "        if md[\"type\"] == \"section\":\n",
    "            print(f\"Header: {md['header']}\\n{md['content']}\")\n",
    "        elif md[\"type\"] == \"form_field\":\n",
    "            print(f\"{md['field_name']}: {md['field_value']}\")\n",
    "        elif md[\"type\"] == \"table_row\":\n",
    "            print(f\"Table Row: {md['text']}\")\n",
    "        elif md[\"type\"] == \"entity\":\n",
    "            print(f\"Entity ({md['entity_type']}): {md['text']}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26wf7KbkvQfu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjcbyybxvZ6p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZWb7gAEvZ-C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UupzFZa0va9a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4MFRSmqvbA0"
   },
   "outputs": [],
   "source": [
    "#OCR based solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxnyrLOCvbEi"
   },
   "outputs": [],
   "source": [
    "# !pip install pytesseract pdf2image pillow sentence-transformers faiss-cpu\n",
    "# !apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k19FxP0nvQi6"
   },
   "outputs": [],
   "source": [
    "# import pytesseract\n",
    "# from pdf2image import convert_from_path\n",
    "# from PIL import Image\n",
    "# import re\n",
    "# import json\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import numpy as np\n",
    "# import faiss\n",
    "\n",
    "# # === CONFIGURATION ===\n",
    "# pdf_path = \"test2.pdf\"\n",
    "# output_json_path = \"ocr_extracted.json\"\n",
    "# # tesseract_cmd_path = None  # Example: \"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "# # if tesseract_cmd_path:\n",
    "# #     pytesseract.pytesseract.tesseract_cmd = tesseract_cmd_path\n",
    "\n",
    "# # === OCR + SECTION PARSING ===\n",
    "\n",
    "# def extract_sections(text):\n",
    "#     lines = text.split(\"\\n\")\n",
    "#     sections = {}\n",
    "#     current_header = None\n",
    "#     buffer = []\n",
    "\n",
    "#     def flush():\n",
    "#         if current_header and buffer:\n",
    "#             sections[current_header] = \"\\n\".join(buffer).strip()\n",
    "\n",
    "#     header_pattern = re.compile(r\"^[A-Z][A-Za-z0-9\\s]{0,40}:?$\")\n",
    "\n",
    "#     for line in lines:\n",
    "#         line = line.strip()\n",
    "#         if not line:\n",
    "#             continue\n",
    "\n",
    "#         if header_pattern.match(line) and len(line) < 50:\n",
    "#             flush()\n",
    "#             current_header = line.rstrip(\":\").strip()\n",
    "#             buffer = []\n",
    "#         else:\n",
    "#             if current_header:\n",
    "#                 buffer.append(line)\n",
    "\n",
    "#     flush()\n",
    "#     return sections\n",
    "\n",
    "# def extract_form_fields(text):\n",
    "#     form_fields = []\n",
    "#     for line in text.splitlines():\n",
    "#         if \":\" in line and len(line) < 100:\n",
    "#             parts = line.split(\":\", 1)\n",
    "#             key = parts[0].strip()\n",
    "#             val = parts[1].strip()\n",
    "#             if key and val:\n",
    "#                 form_fields.append({\"field_name\": key, \"field_value\": val})\n",
    "#     return form_fields\n",
    "\n",
    "# def extract_entities(text):\n",
    "#     entities = []\n",
    "#     date_pattern = r\"\\b(?:\\d{1,2}[/-])?\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{4}-\\d{2}-\\d{2}\\b\"\n",
    "#     name_pattern = r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\"\n",
    "\n",
    "#     for match in re.findall(date_pattern, text):\n",
    "#         entities.append({\"entity_type\": \"Date\", \"text\": match})\n",
    "\n",
    "#     for match in re.findall(name_pattern, text):\n",
    "#         entities.append({\"entity_type\": \"Person\", \"text\": match})\n",
    "\n",
    "#     return entities\n",
    "\n",
    "# def extract_table_lines(text):\n",
    "#     tables = []\n",
    "#     lines = [line.strip() for line in text.splitlines() if \"|\" in line and line.count(\"|\") >= 2]\n",
    "#     for line in lines:\n",
    "#         cells = [cell.strip() for cell in line.split(\"|\") if cell.strip()]\n",
    "#         if cells:\n",
    "#             tables.append(cells)\n",
    "#     return tables\n",
    "\n",
    "# # === OCR PDF ===\n",
    "# pages = convert_from_path(pdf_path, dpi=300)\n",
    "# output_data = {\"pages\": []}\n",
    "\n",
    "# for i, image in enumerate(pages):\n",
    "#     text = pytesseract.image_to_string(image, lang=\"eng\")\n",
    "#     sections = extract_sections(text)\n",
    "#     form_fields = extract_form_fields(text)\n",
    "#     entities = extract_entities(text)\n",
    "#     tables = extract_table_lines(text)\n",
    "\n",
    "#     output_data[\"pages\"].append({\n",
    "#         \"page_number\": i + 1,\n",
    "#         \"text\": text,\n",
    "#         \"sections\": sections,\n",
    "#         \"form_fields\": form_fields,\n",
    "#         \"entities\": entities,\n",
    "#         \"tables\": tables\n",
    "#     })\n",
    "\n",
    "# # === SAVE TO JSON ===\n",
    "# with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"‚úÖ OCR and extraction complete. Saved to: {output_json_path}\")\n",
    "\n",
    "# # === SEMANTIC SEARCH INDEXING ===\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# corpus = []\n",
    "# texts = []\n",
    "\n",
    "# for page in output_data[\"pages\"]:\n",
    "#     # Sections\n",
    "#     for header, content in page.get(\"sections\", {}).items():\n",
    "#         full = f\"{header}: {content}\"\n",
    "#         corpus.append({\n",
    "#             \"type\": \"section\",\n",
    "#             \"page\": page[\"page_number\"],\n",
    "#             \"header\": header,\n",
    "#             \"content\": content\n",
    "#         })\n",
    "#         texts.append(full)\n",
    "\n",
    "#     # Form fields\n",
    "#     for field in page.get(\"form_fields\", []):\n",
    "#         full = f\"{field['field_name']}: {field['field_value']}\"\n",
    "#         corpus.append({\n",
    "#             \"type\": \"form_field\",\n",
    "#             \"page\": page[\"page_number\"],\n",
    "#             \"field_name\": field[\"field_name\"],\n",
    "#             \"field_value\": field[\"field_value\"]\n",
    "#         })\n",
    "#         texts.append(full)\n",
    "\n",
    "#     # Tables\n",
    "#     for idx, row in enumerate(page.get(\"tables\", [])):\n",
    "#         row_text = \" | \".join(row)\n",
    "#         corpus.append({\n",
    "#             \"type\": \"table_row\",\n",
    "#             \"page\": page[\"page_number\"],\n",
    "#             \"row_index\": idx,\n",
    "#             \"text\": row_text\n",
    "#         })\n",
    "#         texts.append(row_text)\n",
    "\n",
    "#     # Entities\n",
    "#     for entity in page.get(\"entities\", []):\n",
    "#         corpus.append({\n",
    "#             \"type\": \"entity\",\n",
    "#             \"page\": page[\"page_number\"],\n",
    "#             \"entity_type\": entity[\"entity_type\"],\n",
    "#             \"text\": entity[\"text\"]\n",
    "#         })\n",
    "#         texts.append(entity[\"text\"])\n",
    "\n",
    "# # === BUILD FAISS INDEX ===\n",
    "# embeddings = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "# embedding_dim = embeddings.shape[1]\n",
    "# index = faiss.IndexFlatIP(embedding_dim)  # Using Inner Product for cosine similarity with normalized vectors\n",
    "# index.add(embeddings)\n",
    "\n",
    "# # === INTERACTIVE SEARCH ===\n",
    "# while True:\n",
    "#     query = input(\"\\nüîç Enter search query (or 'exit'): \").strip()\n",
    "#     if query.lower() == \"exit\":\n",
    "#         break\n",
    "#     if not query:\n",
    "#         print(\"Please enter a valid query.\")\n",
    "#         continue\n",
    "\n",
    "#     query_embedding = model.encode(query, convert_to_numpy=True, normalize_embeddings=True).reshape(1, -1)\n",
    "#     distances, neighbors = index.search(query_embedding, 5)  # Top 5 results\n",
    "\n",
    "#     print(f\"\\nüîé Top Results for: '{query}'\\n\")\n",
    "#     for idx, dist in zip(neighbors[0], distances[0]):\n",
    "#         entry = corpus[idx]\n",
    "#         print(f\"üìÑ Page {entry['page']} | Type: {entry['type']} | Score: {dist:.3f}\")\n",
    "#         if entry[\"type\"] == \"section\":\n",
    "#             print(f\"Header: {entry['header']}\\n{entry['content']}\")\n",
    "#         elif entry[\"type\"] == \"form_field\":\n",
    "#             print(f\"{entry['field_name']}: {entry['field_value']}\")\n",
    "#         elif entry[\"type\"] == \"table_row\":\n",
    "#             print(f\"Table Row: {entry['text']}\")\n",
    "#         elif entry[\"type\"] == \"entity\":\n",
    "#             print(f\"Entity ({entry['entity_type']}): {entry['text']}\")\n",
    "#         print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1V6O7qwvQmy"
   },
   "outputs": [],
   "source": [
    "# #expected json ouptut taken for search;\n",
    "# {\n",
    "#     \"text\": \"Full raw text from the PDF document...\",\n",
    "#     \"pages\": [\n",
    "#         {\n",
    "#             \"page_number\": 1,\n",
    "#             \"form_fields\": [\n",
    "#                 {\n",
    "#                     \"field_name\": \"Patient Name\",\n",
    "#                     \"field_value\": \"John Doe\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"field_name\": \"Admission Date\",\n",
    "#                     \"field_value\": \"2023-08-20\"\n",
    "#                 }\n",
    "#             ],\n",
    "#             \"tables\": [\n",
    "#                 [\n",
    "#                     [\"Test\", \"Result\", \"Normal Range\"],\n",
    "#                     [\"Hemoglobin\", \"13.5\", \"13-17\"]\n",
    "#                 ]\n",
    "#             ],\n",
    "#             \"text_snippet\": \"History:\\nPatient admitted with chest pain.\\nTreatment:\\nAspirin given.\\n...\",\n",
    "#             \"sections\": {\n",
    "#                 \"History\": \"Patient admitted with chest pain.\",\n",
    "#                 \"Treatment\": \"Aspirin given.\",\n",
    "#                 \"Medication\": \"None reported.\",\n",
    "#                 \"Admission Date\": \"2023-08-20\"\n",
    "#             }\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "#search result:\n",
    "\n",
    "# [\n",
    "#   {\n",
    "#     \"page\": 1,\n",
    "#     \"section_header\": \"Treatment\",\n",
    "#     \"section_text\": \"Aspirin given.\"\n",
    "#   }\n",
    "# ]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
