{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9cc55rpEqma",
        "outputId": "cc356e46-7b4f-4351-afd6-8b3ca5fe5c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scann in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.8)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install numpy sentence-transformers bertopic hdbscan nltk scann\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Shared Imports & Setup ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "# Download punkt tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# === Code 1 ===\n",
        "class AllergyTopicSearcherModel1:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, model_name=\"emilyalsentzer/Bio_ClinicalBERT\"):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model_name = model_name\n",
        "\n",
        "        self.embedding_model = None\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "        entity_to_chunk = defaultdict(list)\n",
        "        all_entities = []\n",
        "        for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "            for ent in ents:\n",
        "                ent_lower = ent.lower()\n",
        "                all_entities.append(ent_lower)\n",
        "                entity_to_chunk[ent_lower].append(idx)\n",
        "\n",
        "        unique_entities = sorted(set(all_entities))\n",
        "        entity_embeddings = self.embedding_model.encode(unique_entities, normalize_embeddings=True)\n",
        "\n",
        "        umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=SEED)\n",
        "        hdbscan_model = HDBSCAN(min_cluster_size=2, min_samples=1, metric='euclidean',\n",
        "                                prediction_data=True)\n",
        "\n",
        "        self.topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        topics, _ = self.topic_model.fit_transform(unique_entities, embeddings=entity_embeddings)\n",
        "\n",
        "        topic_to_entities = defaultdict(list)\n",
        "        for ent, topic in zip(unique_entities, topics):\n",
        "            topic_to_entities[topic].append(ent)\n",
        "\n",
        "        topic_contexts = defaultdict(list)\n",
        "        for topic, entities in topic_to_entities.items():\n",
        "            for ent in entities:\n",
        "                for chunk_id in entity_to_chunk[ent]:\n",
        "                    for sent in sent_tokenize(self.chunks[chunk_id]):\n",
        "                        if ent in sent.lower():\n",
        "                            topic_contexts[topic].append(sent)\n",
        "\n",
        "        for topic in topic_contexts:\n",
        "            topic_contexts[topic] = list(set(topic_contexts[topic]))\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id, sentences in topic_contexts.items():\n",
        "            if not sentences:\n",
        "                continue\n",
        "            sent_embs = self.embedding_model.encode(sentences, normalize_embeddings=True)\n",
        "            mean_emb = np.mean(sent_embs, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": topic_to_entities[topic_id],\n",
        "                \"sentences\": sentences,\n",
        "                \"sentence_embeddings\": sent_embs\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 5)\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=2):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        output = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            sents = meta[\"sentences\"]\n",
        "            sent_embs = meta[\"sentence_embeddings\"]\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            output += [sents[i] for i in top_indices]\n",
        "        return output\n",
        "\n",
        "\n",
        "# === Code 2 ===\n",
        "class AllergyTopicSearcherModel2:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, model_name=\"emilyalsentzer/Bio_ClinicalBERT\"):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model_name = model_name\n",
        "\n",
        "        self.embedding_model = None\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "        entity_context_pairs = []\n",
        "        entity_to_chunk = defaultdict(list)\n",
        "\n",
        "        for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "            chunk = self.chunks[idx].lower()\n",
        "            sentences = sent_tokenize(chunk)\n",
        "            for ent in ents:\n",
        "                ent_lower = ent.lower()\n",
        "                for sent in sentences:\n",
        "                    if ent_lower in sent:\n",
        "                        entity_context_pairs.append((ent_lower, sent.strip()))\n",
        "                        entity_to_chunk[ent_lower].append(idx)\n",
        "                        break\n",
        "\n",
        "        contextual_texts = [f\"{ent}: {context}\" for ent, context in entity_context_pairs]\n",
        "        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=True)\n",
        "\n",
        "        umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=SEED)\n",
        "        hdbscan_model = HDBSCAN(min_cluster_size=2, min_samples=1, metric='euclidean', prediction_data=True)\n",
        "\n",
        "        self.topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
        "\n",
        "        topic_to_contexts = defaultdict(list)\n",
        "        topic_to_entities = defaultdict(set)\n",
        "        topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "        for i, topic in enumerate(topics):\n",
        "            ent, context = entity_context_pairs[i]\n",
        "            topic_to_contexts[topic].append(context)\n",
        "            topic_to_entities[topic].add(ent)\n",
        "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id in topic_to_contexts:\n",
        "            embeddings = topic_to_embeddings[topic_id]\n",
        "            mean_emb = np.mean(embeddings, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": list(topic_to_entities[topic_id]),\n",
        "                \"sentences\": topic_to_contexts[topic_id],\n",
        "                \"sentence_embeddings\": np.array(topic_to_embeddings[topic_id])\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 5)\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=2):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        output = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            seen = set()\n",
        "            sents = []\n",
        "            embs = []\n",
        "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "                if sent not in seen:\n",
        "                    seen.add(sent)\n",
        "                    sents.append(sent)\n",
        "                    embs.append(emb)\n",
        "            sent_embs = np.array(embs)\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            output += [sents[i] for i in top_indices]\n",
        "        return output\n",
        "\n",
        "\n",
        "# === Evaluation ===\n",
        "chunks = [\n",
        "    \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\",\n",
        "    \"Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\",\n",
        "    \"Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\",\n",
        "    \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\",\n",
        "    \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\",\n",
        "    \"He walks in cold weather but has no allergy symptoms or reactions.\"\n",
        "]\n",
        "\n",
        "manual_entities_per_chunk = [\n",
        "    [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "    [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "    [\"anaphylaxis\", \"allergic reaction\"],\n",
        "    [\"food allergies\", \"milk\", \"eggs\"],\n",
        "    [\"urticaria\", \"hives\", \"eczema\", \"allergies\"],\n",
        "    [\"cold weather\", \"allergy symptoms\", \"reactions\"]\n",
        "]\n",
        "\n",
        "queries = [\n",
        "    \"peanut allergy\",\n",
        "    \"symptoms of anaphylaxis\",\n",
        "    \"hay fever\",\n",
        "    \"eczema treatment\",\n",
        "    \"allergic reaction to milk\",\n",
        "    \"signs of food allergy\",\n",
        "    \"urticaria causes\",\n",
        "    \"pet dander allergies\",\n",
        "    \"cold weather allergy\",\n",
        "    \"hives and swelling\"\n",
        "]\n",
        "\n",
        "print(\"üîß Initializing Model 1...\")\n",
        "m1 = AllergyTopicSearcherModel1(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"üîß Initializing Model 2...\")\n",
        "m2 = AllergyTopicSearcherModel2(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"\\n\\nüîç Starting Evaluation\")\n",
        "for q in queries:\n",
        "    res1 = m1.search(q, top_k_sents=2)\n",
        "    res2 = m2.search(q, top_k_sents=2)\n",
        "\n",
        "    print(f\"\\n\\nüîé Query: {q}\")\n",
        "    print(\"-\" * 90)\n",
        "    print(\"üìò Model 1:\")\n",
        "    for i, r in enumerate(res1, 1):\n",
        "        print(f\"  {i}. {r}\")\n",
        "    print(\"üìô Model 2:\")\n",
        "    for i, r in enumerate(res2, 1):\n",
        "        print(f\"  {i}. {r}\")\n",
        "    print(\"=\" * 90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQFi_GscE-F7",
        "outputId": "c21e5574-8a79-416f-e279-400e37043553"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name emilyalsentzer/Bio_ClinicalBERT. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Model 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name emilyalsentzer/Bio_ClinicalBERT. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Model 2...\n",
            "\n",
            "\n",
            "üîç Starting Evaluation\n",
            "\n",
            "\n",
            "üîé Query: peanut allergy\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Peanut allergy is one of the most common causes of severe allergic reactions.\n",
            "üìô Model 2:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: symptoms of anaphylaxis\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. Symptoms can include hives, swelling, and anaphylaxis.\n",
            "  2. Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\n",
            "üìô Model 2:\n",
            "  1. symptoms can include hives, swelling, and anaphylaxis.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: hay fever\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Symptoms can include hives, swelling, and anaphylaxis.\n",
            "üìô Model 2:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: eczema treatment\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. Skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "  2. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "üìô Model 2:\n",
            "  1. skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: allergic reaction to milk\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Peanut allergy is one of the most common causes of severe allergic reactions.\n",
            "üìô Model 2:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: signs of food allergy\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Peanut allergy is one of the most common causes of severe allergic reactions.\n",
            "üìô Model 2:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: urticaria causes\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. Skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "  2. Symptoms can include hives, swelling, and anaphylaxis.\n",
            "üìô Model 2:\n",
            "  1. skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: pet dander allergies\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\n",
            "üìô Model 2:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: cold weather allergy\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\n",
            "üìô Model 2:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "üîé Query: hives and swelling\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1:\n",
            "  1. Symptoms can include hives, swelling, and anaphylaxis.\n",
            "  2. Skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "üìô Model 2:\n",
            "  1. symptoms can include hives, swelling, and anaphylaxis.\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Shared Imports & Setup ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "# Download punkt tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# === Data ===\n",
        "chunks = [\n",
        "    \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\",\n",
        "    \"Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\",\n",
        "    \"Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\",\n",
        "    \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\",\n",
        "    \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\",\n",
        "    \"He walks in cold weather but has no allergy symptoms or reactions.\"\n",
        "]\n",
        "\n",
        "manual_entities_per_chunk = [\n",
        "    [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "    [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "    [\"anaphylaxis\", \"allergic reaction\"],\n",
        "    [\"food allergies\", \"milk\", \"eggs\"],\n",
        "    [\"urticaria\", \"hives\", \"eczema\", \"allergies\"],\n",
        "    [\"cold weather\", \"allergy symptoms\", \"reactions\"]\n",
        "]\n",
        "\n",
        "queries = [\n",
        "    \"peanut allergy\",\n",
        "    \"symptoms of anaphylaxis\",\n",
        "    \"hay fever\",\n",
        "    \"eczema treatment\",\n",
        "    \"allergic reaction to milk\",\n",
        "    \"signs of food allergy\",\n",
        "    \"urticaria causes\",\n",
        "    \"pet dander allergies\",\n",
        "    \"cold weather allergy\",\n",
        "    \"hives and swelling\"\n",
        "]\n",
        "\n",
        "# Ground truth answers (2 outputs each query)\n",
        "ground_truth = {\n",
        "    \"peanut allergy\": [\n",
        "        \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\",\n",
        "        \"Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\"\n",
        "    ],\n",
        "    \"symptoms of anaphylaxis\": [\n",
        "        \"Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\",\n",
        "        \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\"\n",
        "    ],\n",
        "    \"hay fever\": [\n",
        "        \"Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\",\n",
        "        \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\"\n",
        "    ],\n",
        "    \"eczema treatment\": [\n",
        "        \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\",\n",
        "        \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\"\n",
        "    ],\n",
        "    \"allergic reaction to milk\": [\n",
        "        \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\",\n",
        "        \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\"\n",
        "    ],\n",
        "    \"signs of food allergy\": [\n",
        "        \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\",\n",
        "        \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\"\n",
        "    ],\n",
        "    \"urticaria causes\": [\n",
        "        \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\",\n",
        "        \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\"\n",
        "    ],\n",
        "    \"pet dander allergies\": [\n",
        "        \"Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\",\n",
        "        \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\"\n",
        "    ],\n",
        "    \"cold weather allergy\": [\n",
        "        \"He walks in cold weather but has no allergy symptoms or reactions.\",\n",
        "        \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\"\n",
        "    ],\n",
        "    \"hives and swelling\": [\n",
        "        \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\",\n",
        "        \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# === Helper functions to compute metrics ===\n",
        "def normalize_text(text):\n",
        "    return text.lower().strip()\n",
        "\n",
        "def precision_recall_f1(preds, truths):\n",
        "    preds_norm = set(normalize_text(p) for p in preds)\n",
        "    truths_norm = set(normalize_text(t) for t in truths)\n",
        "    tp = len(preds_norm.intersection(truths_norm))\n",
        "    precision = tp / len(preds_norm) if preds_norm else 0.0\n",
        "    recall = tp / len(truths_norm) if truths_norm else 0.0\n",
        "    if precision + recall == 0:\n",
        "        f1 = 0.0\n",
        "    else:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "# === Initialize Models ===\n",
        "print(\"üîß Initializing Model 1...\")\n",
        "m1 = AllergyTopicSearcherModel1(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"üîß Initializing Model 2...\")\n",
        "m2 = AllergyTopicSearcherModel2(chunks, manual_entities_per_chunk)\n",
        "\n",
        "# === Evaluation ===\n",
        "print(\"\\n\\nüîç Starting Evaluation\\n\")\n",
        "\n",
        "metrics_m1 = []\n",
        "metrics_m2 = []\n",
        "\n",
        "for q in queries:\n",
        "    gt_answers = ground_truth[q]\n",
        "    res1 = m1.search(q, top_k_sents=2)\n",
        "    res2 = m2.search(q, top_k_sents=2)\n",
        "\n",
        "    prec1, rec1, f1_1 = precision_recall_f1(res1, gt_answers)\n",
        "    prec2, rec2, f1_2 = precision_recall_f1(res2, gt_answers)\n",
        "\n",
        "    metrics_m1.append((prec1, rec1, f1_1))\n",
        "    metrics_m2.append((prec2, rec2, f1_2))\n",
        "\n",
        "    print(f\"üîé Query: {q}\")\n",
        "    print(\"-\" * 90)\n",
        "    print(\"üìò Model 1 Results:\")\n",
        "    for i, r in enumerate(res1, 1):\n",
        "        print(f\"  {i}. {r}\")\n",
        "    print(f\"  Precision: {prec1:.3f} | Recall: {rec1:.3f} | F1: {f1_1:.3f}\")\n",
        "    print(\"üìô Model 2 Results:\")\n",
        "    for i, r in enumerate(res2, 1):\n",
        "        print(f\"  {i}. {r}\")\n",
        "    print(f\"  Precision: {prec2:.3f} | Recall: {rec2:.3f} | F1: {f1_2:.3f}\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "# Aggregate overall metrics\n",
        "def aggregate_metrics(metrics):\n",
        "    precs, recs, f1s = zip(*metrics)\n",
        "    return np.mean(precs), np.mean(recs), np.mean(f1s)\n",
        "\n",
        "avg_prec_m1, avg_rec_m1, avg_f1_m1 = aggregate_metrics(metrics_m1)\n",
        "avg_prec_m2, avg_rec_m2, avg_f1_m2 = aggregate_metrics(metrics_m2)\n",
        "\n",
        "print(\"\\n\\n=== Overall Evaluation ===\")\n",
        "print(f\"Model 1 - Precision@2: {avg_prec_m1:.3f}, Recall@2: {avg_rec_m1:.3f}, F1@2: {avg_f1_m1:.3f}\")\n",
        "print(f\"Model 2 - Precision@2: {avg_prec_m2:.3f}, Recall@2: {avg_rec_m2:.3f}, F1@2: {avg_f1_m2:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT2VPRnzISEV",
        "outputId": "bd29344c-62c6-4b5e-8202-b5484d48c180"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name emilyalsentzer/Bio_ClinicalBERT. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Model 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name emilyalsentzer/Bio_ClinicalBERT. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Model 2...\n",
            "\n",
            "\n",
            "üîç Starting Evaluation\n",
            "\n",
            "üîé Query: peanut allergy\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Peanut allergy is one of the most common causes of severe allergic reactions.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "üìô Model 2 Results:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: symptoms of anaphylaxis\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. Symptoms can include hives, swelling, and anaphylaxis.\n",
            "  2. Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\n",
            "  Precision: 0.500 | Recall: 0.500 | F1: 0.500\n",
            "üìô Model 2 Results:\n",
            "  1. symptoms can include hives, swelling, and anaphylaxis.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: hay fever\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Symptoms can include hives, swelling, and anaphylaxis.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "üìô Model 2 Results:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: eczema treatment\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. Skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "  2. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  Precision: 0.500 | Recall: 0.500 | F1: 0.500\n",
            "üìô Model 2 Results:\n",
            "  1. skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "  Precision: 1.000 | Recall: 0.500 | F1: 0.667\n",
            "==========================================================================================\n",
            "üîé Query: allergic reaction to milk\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Peanut allergy is one of the most common causes of severe allergic reactions.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "üìô Model 2 Results:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: signs of food allergy\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Peanut allergy is one of the most common causes of severe allergic reactions.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "üìô Model 2 Results:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: urticaria causes\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. Skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "  2. Symptoms can include hives, swelling, and anaphylaxis.\n",
            "  Precision: 0.500 | Recall: 0.500 | F1: 0.500\n",
            "üìô Model 2 Results:\n",
            "  1. skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "  Precision: 1.000 | Recall: 0.500 | F1: 0.667\n",
            "==========================================================================================\n",
            "üîé Query: pet dander allergies\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\n",
            "  Precision: 0.500 | Recall: 0.500 | F1: 0.500\n",
            "üìô Model 2 Results:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: cold weather allergy\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. He walks in cold weather but has no allergy symptoms or reactions.\n",
            "  2. Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\n",
            "  Precision: 0.500 | Recall: 0.500 | F1: 0.500\n",
            "üìô Model 2 Results:\n",
            "  1. he walks in cold weather but has no allergy symptoms or reactions.\n",
            "  Precision: 1.000 | Recall: 0.500 | F1: 0.667\n",
            "==========================================================================================\n",
            "üîé Query: hives and swelling\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. Symptoms can include hives, swelling, and anaphylaxis.\n",
            "  2. Skin reactions like urticaria (hives) and eczema are often signs of allergies.\n",
            "  Precision: 0.500 | Recall: 0.500 | F1: 0.500\n",
            "üìô Model 2 Results:\n",
            "  1. symptoms can include hives, swelling, and anaphylaxis.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "=== Overall Evaluation ===\n",
            "Model 1 - Precision@2: 0.300, Recall@2: 0.300, F1@2: 0.300\n",
            "Model 2 - Precision@2: 0.300, Recall@2: 0.150, F1@2: 0.200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2nYRlhO4Uaan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Shared Imports & Setup ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# === Model 1 ===\n",
        "class AllergyTopicSearcherModel1:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        entity_to_chunk = defaultdict(list)\n",
        "        all_entities = []\n",
        "        for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "            for ent in ents:\n",
        "                e = ent.lower()\n",
        "                all_entities.append(e)\n",
        "                entity_to_chunk[e].append(idx)\n",
        "\n",
        "        unique_entities = sorted(set(all_entities))\n",
        "        entity_embeddings = self.embedding_model.encode(unique_entities, normalize_embeddings=True)\n",
        "\n",
        "        umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=SEED)\n",
        "        hdbscan_model = HDBSCAN(min_cluster_size=2, min_samples=1, metric='euclidean', prediction_data=True)\n",
        "\n",
        "        topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False\n",
        "        )\n",
        "        topics, _ = topic_model.fit_transform(unique_entities, embeddings=entity_embeddings)\n",
        "\n",
        "        topic_to_entities = defaultdict(list)\n",
        "        for ent, t in zip(unique_entities, topics):\n",
        "            topic_to_entities[t].append(ent)\n",
        "\n",
        "        topic_metadata = []\n",
        "        topic_embeddings = []\n",
        "        for t, ents in topic_to_entities.items():\n",
        "            sents = []\n",
        "            for ent in ents:\n",
        "                for idx in entity_to_chunk[ent]:\n",
        "                    for sent in sent_tokenize(self.chunks[idx].lower()):\n",
        "                        if ent in sent:\n",
        "                            sents.append(sent)\n",
        "            sents = list(set(sents))\n",
        "            if not sents:\n",
        "                continue\n",
        "            sent_embs = self.embedding_model.encode(sents, normalize_embeddings=True)\n",
        "            mean_emb = np.mean(sent_embs, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb)\n",
        "\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": t,\n",
        "                \"entities\": ents,\n",
        "                \"sentences\": sents,\n",
        "                \"sentence_embeddings\": sent_embs\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.vstack(topic_embeddings)\n",
        "        self.topic_metadata = topic_metadata\n",
        "        self.searcher = scann.scann_ops_pybind.builder(\n",
        "            self.topic_embeddings, 3, \"dot_product\"\n",
        "        ).tree(num_leaves=min(5, len(self.topic_embeddings)), num_leaves_to_search=2)\\\n",
        "         .score_brute_force().reorder(3).build()\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=2):\n",
        "        q_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, _ = self.searcher.search(q_emb, final_num_neighbors=top_k_topics)\n",
        "        output = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            sent_embs = meta[\"sentence_embeddings\"]\n",
        "            sims = np.dot(sent_embs, q_emb)\n",
        "            for i in sims.argsort()[::-1][:top_k_sents]:\n",
        "                output.append(meta[\"sentences\"][i])\n",
        "        return output\n",
        "\n",
        "# === Model 2 ===\n",
        "class AllergyTopicSearcherModel2:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        pairs = []\n",
        "        for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "            for ent in ents:\n",
        "                for sent in sent_tokenize(self.chunks[idx].lower()):\n",
        "                    if ent.lower() in sent:\n",
        "                        pairs.append((ent.lower(), sent.strip()))\n",
        "                        break\n",
        "\n",
        "        texts = [f\"{ent}: {sent}\" for ent, sent in pairs]\n",
        "        embeddings = self.embedding_model.encode(texts, normalize_embeddings=True)\n",
        "\n",
        "        umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=SEED)\n",
        "        hdbscan_model = HDBSCAN(min_cluster_size=2, min_samples=1, metric='euclidean', prediction_data=True)\n",
        "\n",
        "        topic_model = BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            representation_model=KeyBERTInspired(),\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False\n",
        "        )\n",
        "        topics, _ = topic_model.fit_transform(texts, embeddings=embeddings)\n",
        "\n",
        "        topic_metadata = defaultdict(lambda: {\"ents\": [], \"sents\": [], \"embs\": []})\n",
        "        for i, t in enumerate(topics):\n",
        "            ent, sent = pairs[i]\n",
        "            topic_metadata[t][\"ents\"].append(ent)\n",
        "            topic_metadata[t][\"sents\"].append(sent)\n",
        "            topic_metadata[t][\"embs\"].append(embeddings[i])\n",
        "\n",
        "        self.topic_metadata = []\n",
        "        topic_embeddings = []\n",
        "        for t, d in topic_metadata.items():\n",
        "            embs = np.vstack(d[\"embs\"])\n",
        "            mean_emb = np.mean(embs, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb)\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            self.topic_metadata.append({\n",
        "                \"topic_id\": t,\n",
        "                \"entities\": list(set(d[\"ents\"])),\n",
        "                \"sentences\": d[\"sents\"],\n",
        "                \"sentence_embeddings\": embs\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.vstack(topic_embeddings)\n",
        "        self.searcher = scann.scann_ops_pybind.builder(\n",
        "            self.topic_embeddings, 3, \"dot_product\"\n",
        "        ).tree(num_leaves=min(5, len(self.topic_embeddings)), num_leaves_to_search=2)\\\n",
        "         .score_brute_force().reorder(3).build()\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=2):\n",
        "        q_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, _ = self.searcher.search(q_emb, final_num_neighbors=top_k_topics)\n",
        "        output = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            sent_embs = meta[\"sentence_embeddings\"]\n",
        "            sims = np.dot(sent_embs, q_emb)\n",
        "            for i in sims.argsort()[::-1][:top_k_sents]:\n",
        "                output.append(meta[\"sentences\"][i])\n",
        "        return output\n",
        "\n",
        "# === New Example Data ===\n",
        "chunks = [\n",
        "    \"60-year-old female with chronic kidney disease stage 3, presenting with fatigue and ankle swelling.\",\n",
        "    \"Medical history includes hypertension, hyperlipidemia, and gout. Medications: ACE inhibitor, statin.\",\n",
        "    \"Lab results: serum creatinine 2.1 mg/dL, eGFR 38 mL/min/1.73m2, uric acid 9.0 mg/dL.\",\n",
        "    \"Patient reports nocturia and decreased appetite. No chest pain or dyspnea.\",\n",
        "    \"Exam shows 2+ pitting edema in lower extremities. Blood pressure 145/90 mmHg.\",\n",
        "    \"Lifestyle: overweight, diet high in sodium, minimal exercise.\"\n",
        "]\n",
        "\n",
        "manual_entities_per_chunk = [\n",
        "    [\"kidney disease\", \"fatigue\", \"ankle swelling\"],\n",
        "    [\"hypertension\", \"hyperlipidemia\", \"gout\", \"ACE inhibitor\", \"statin\"],\n",
        "    [\"serum creatinine\", \"eGFR\", \"uric acid\"],\n",
        "    [\"nocturia\", \"decreased appetite\"],\n",
        "    [\"pitting edema\", \"blood pressure\"],\n",
        "    [\"overweight\", \"diet\", \"salt\", \"exercise\"]\n",
        "]\n",
        "\n",
        "queries = [\n",
        "    \"kidney function lab results\",\n",
        "    \"edema assessment\",\n",
        "    \"gout medication\",\n",
        "    \"blood pressure management\",\n",
        "    \"patient lifestyle\",\n",
        "]\n",
        "\n",
        "# === Evaluation ===\n",
        "print(\"üîß Initializing Model 1...\")\n",
        "m1 = AllergyTopicSearcherModel1(chunks, manual_entities_per_chunk)\n",
        "print(\"üîß Initializing Model 2...\")\n",
        "m2 = AllergyTopicSearcherModel2(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"\\nüîç Running Queries and Results\")\n",
        "for q in queries:\n",
        "    res1 = m1.search(q, top_k_sents=2)\n",
        "    res2 = m2.search(q, top_k_sents=2)\n",
        "    print(f\"\\nQuery: {q}\")\n",
        "    print(\"Model‚ÄØ1:\", res1)\n",
        "    print(\"Model‚ÄØ2:\", res2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojhUv5QHUasW",
        "outputId": "366aa782-d805-4afb-befe-815f7363ba57"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Model 1...\n",
            "üîß Initializing Model 2...\n",
            "\n",
            "üîç Running Queries and Results\n",
            "\n",
            "Query: kidney function lab results\n",
            "Model‚ÄØ1: ['lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.', 'patient reports nocturia and decreased appetite.']\n",
            "Model‚ÄØ2: ['lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.', 'lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.']\n",
            "\n",
            "Query: edema assessment\n",
            "Model‚ÄØ1: ['exam shows 2+ pitting edema in lower extremities.', 'lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.']\n",
            "Model‚ÄØ2: ['exam shows 2+ pitting edema in lower extremities.', 'lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.']\n",
            "\n",
            "Query: gout medication\n",
            "Model‚ÄØ1: ['medical history includes hypertension, hyperlipidemia, and gout.']\n",
            "Model‚ÄØ2: ['medical history includes hypertension, hyperlipidemia, and gout.', 'medical history includes hypertension, hyperlipidemia, and gout.']\n",
            "\n",
            "Query: blood pressure management\n",
            "Model‚ÄØ1: ['blood pressure 145/90 mmhg.', 'medical history includes hypertension, hyperlipidemia, and gout.']\n",
            "Model‚ÄØ2: ['blood pressure 145/90 mmhg.', 'medical history includes hypertension, hyperlipidemia, and gout.']\n",
            "\n",
            "Query: patient lifestyle\n",
            "Model‚ÄØ1: ['medical history includes hypertension, hyperlipidemia, and gout.']\n",
            "Model‚ÄØ2: ['medical history includes hypertension, hyperlipidemia, and gout.', 'medical history includes hypertension, hyperlipidemia, and gout.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Shared Imports & Setup ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "# Download punkt tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# === Data ===\n",
        "chunks = [\n",
        "    \"60-year-old female with chronic kidney disease stage 3, presenting with fatigue and ankle swelling.\",\n",
        "    \"Medical history includes hypertension, hyperlipidemia, and gout. Medications: ACE inhibitor, statin.\",\n",
        "    \"Lab results: serum creatinine 2.1 mg/dL, eGFR 38 mL/min/1.73m2, uric acid 9.0 mg/dL.\",\n",
        "    \"Patient reports nocturia and decreased appetite. No chest pain or dyspnea.\",\n",
        "    \"Exam shows 2+ pitting edema in lower extremities. Blood pressure 145/90 mmHg.\",\n",
        "    \"Lifestyle: overweight, diet high in sodium, minimal exercise.\"\n",
        "]\n",
        "\n",
        "queries = [\n",
        "    \"kidney function lab results\",\n",
        "    \"edema assessment\",\n",
        "    \"gout medication\",\n",
        "    \"blood pressure management\",\n",
        "    \"patient lifestyle\",\n",
        "]\n",
        "\n",
        "# If your models rely on manual entities per chunk, here is an example:\n",
        "manual_entities_per_chunk = [\n",
        "    [\"chronic kidney disease\", \"fatigue\", \"ankle swelling\"],\n",
        "    [\"hypertension\", \"hyperlipidemia\", \"gout\", \"ACE inhibitor\", \"statin\"],\n",
        "    [\"serum creatinine\", \"eGFR\", \"uric acid\"],\n",
        "    [\"nocturia\", \"decreased appetite\", \"chest pain\", \"dyspnea\"],\n",
        "    [\"pitting edema\", \"blood pressure\"],\n",
        "    [\"overweight\", \"diet high in sodium\", \"minimal exercise\"]\n",
        "]\n",
        "\n",
        "# Ground truth answers for evaluation (2 outputs per query)\n",
        "ground_truth = {\n",
        "    \"kidney function lab results\": [\n",
        "        \"Lab results: serum creatinine 2.1 mg/dL, eGFR 38 mL/min/1.73m2, uric acid 9.0 mg/dL.\",\n",
        "        \"60-year-old female with chronic kidney disease stage 3, presenting with fatigue and ankle swelling.\"\n",
        "    ],\n",
        "    \"edema assessment\": [\n",
        "        \"Exam shows 2+ pitting edema in lower extremities. Blood pressure 145/90 mmHg.\",\n",
        "        \"60-year-old female with chronic kidney disease stage 3, presenting with fatigue and ankle swelling.\"\n",
        "    ],\n",
        "    \"gout medication\": [\n",
        "        \"Medical history includes hypertension, hyperlipidemia, and gout. Medications: ACE inhibitor, statin.\",\n",
        "        \"Lab results: serum creatinine 2.1 mg/dL, eGFR 38 mL/min/1.73m2, uric acid 9.0 mg/dL.\"\n",
        "    ],\n",
        "    \"blood pressure management\": [\n",
        "        \"Exam shows 2+ pitting edema in lower extremities. Blood pressure 145/90 mmHg.\",\n",
        "        \"Medical history includes hypertension, hyperlipidemia, and gout. Medications: ACE inhibitor, statin.\"\n",
        "    ],\n",
        "    \"patient lifestyle\": [\n",
        "        \"Lifestyle: overweight, diet high in sodium, minimal exercise.\",\n",
        "        \"Patient reports nocturia and decreased appetite. No chest pain or dyspnea.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# === Helper functions to compute metrics ===\n",
        "def normalize_text(text):\n",
        "    return text.lower().strip()\n",
        "\n",
        "def precision_recall_f1(preds, truths):\n",
        "    preds_norm = set(normalize_text(p) for p in preds)\n",
        "    truths_norm = set(normalize_text(t) for t in truths)\n",
        "    tp = len(preds_norm.intersection(truths_norm))\n",
        "    precision = tp / len(preds_norm) if preds_norm else 0.0\n",
        "    recall = tp / len(truths_norm) if truths_norm else 0.0\n",
        "    if precision + recall == 0:\n",
        "        f1 = 0.0\n",
        "    else:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "# === Initialize Models ===\n",
        "print(\"üîß Initializing Model 1...\")\n",
        "m1 = AllergyTopicSearcherModel1(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"üîß Initializing Model 2...\")\n",
        "m2 = AllergyTopicSearcherModel2(chunks, manual_entities_per_chunk)\n",
        "\n",
        "# === Evaluation ===\n",
        "print(\"\\n\\nüîç Starting Evaluation\\n\")\n",
        "\n",
        "metrics_m1 = []\n",
        "metrics_m2 = []\n",
        "\n",
        "for q in queries:\n",
        "    gt_answers = ground_truth[q]\n",
        "    res1 = m1.search(q, top_k_sents=2)\n",
        "    res2 = m2.search(q, top_k_sents=2)\n",
        "\n",
        "    prec1, rec1, f1_1 = precision_recall_f1(res1, gt_answers)\n",
        "    prec2, rec2, f1_2 = precision_recall_f1(res2, gt_answers)\n",
        "\n",
        "    metrics_m1.append((prec1, rec1, f1_1))\n",
        "    metrics_m2.append((prec2, rec2, f1_2))\n",
        "\n",
        "    print(f\"üîé Query: {q}\")\n",
        "    print(\"-\" * 90)\n",
        "    print(\"üìò Model 1 Results:\")\n",
        "    for i, r in enumerate(res1, 1):\n",
        "        print(f\"  {i}. {r}\")\n",
        "    print(f\"  Precision: {prec1:.3f} | Recall: {rec1:.3f} | F1: {f1_1:.3f}\")\n",
        "    print(\"üìô Model 2 Results:\")\n",
        "    for i, r in enumerate(res2, 1):\n",
        "        print(f\"  {i}. {r}\")\n",
        "    print(f\"  Precision: {prec2:.3f} | Recall: {rec2:.3f} | F1: {f1_2:.3f}\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "# Aggregate overall metrics\n",
        "def aggregate_metrics(metrics):\n",
        "    precs, recs, f1s = zip(*metrics)\n",
        "    return np.mean(precs), np.mean(recs), np.mean(f1s)\n",
        "\n",
        "avg_prec_m1, avg_rec_m1, avg_f1_m1 = aggregate_metrics(metrics_m1)\n",
        "avg_prec_m2, avg_rec_m2, avg_f1_m2 = aggregate_metrics(metrics_m2)\n",
        "\n",
        "print(\"\\n\\n=== Overall Evaluation ===\")\n",
        "print(f\"Model 1 - Precision@2: {avg_prec_m1:.3f}, Recall@2: {avg_rec_m1:.3f}, F1@2: {avg_f1_m1:.3f}\")\n",
        "print(f\"Model 2 - Precision@2: {avg_prec_m2:.3f}, Recall@2: {avg_rec_m2:.3f}, F1@2: {avg_f1_m2:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qc5xjwjWEP_",
        "outputId": "d3998ad4-be59-49e1-81e5-d9ec42e58cf2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Model 1...\n",
            "üîß Initializing Model 2...\n",
            "\n",
            "\n",
            "üîç Starting Evaluation\n",
            "\n",
            "üîé Query: kidney function lab results\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.\n",
            "  2. patient reports nocturia and decreased appetite.\n",
            "  Precision: 0.500 | Recall: 0.500 | F1: 0.500\n",
            "üìô Model 2 Results:\n",
            "  1. lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.\n",
            "  2. lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.\n",
            "  Precision: 1.000 | Recall: 0.500 | F1: 0.667\n",
            "==========================================================================================\n",
            "üîé Query: edema assessment\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. exam shows 2+ pitting edema in lower extremities.\n",
            "  2. medical history includes hypertension, hyperlipidemia, and gout.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "üìô Model 2 Results:\n",
            "  1. exam shows 2+ pitting edema in lower extremities.\n",
            "  2. lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: gout medication\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. medical history includes hypertension, hyperlipidemia, and gout.\n",
            "  2. lab results: serum creatinine 2.1 mg/dl, egfr 38 ml/min/1.73m2, uric acid 9.0 mg/dl.\n",
            "  Precision: 0.500 | Recall: 0.500 | F1: 0.500\n",
            "üìô Model 2 Results:\n",
            "  1. medical history includes hypertension, hyperlipidemia, and gout.\n",
            "  2. medical history includes hypertension, hyperlipidemia, and gout.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: blood pressure management\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. blood pressure 145/90 mmhg.\n",
            "  2. medical history includes hypertension, hyperlipidemia, and gout.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "üìô Model 2 Results:\n",
            "  1. blood pressure 145/90 mmhg.\n",
            "  2. medical history includes hypertension, hyperlipidemia, and gout.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "==========================================================================================\n",
            "üîé Query: patient lifestyle\n",
            "------------------------------------------------------------------------------------------\n",
            "üìò Model 1 Results:\n",
            "  1. medical history includes hypertension, hyperlipidemia, and gout.\n",
            "  2. blood pressure 145/90 mmhg.\n",
            "  Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
            "üìô Model 2 Results:\n",
            "  1. lifestyle: overweight, diet high in sodium, minimal exercise.\n",
            "  2. lifestyle: overweight, diet high in sodium, minimal exercise.\n",
            "  Precision: 1.000 | Recall: 0.500 | F1: 0.667\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "=== Overall Evaluation ===\n",
            "Model 1 - Precision@2: 0.200, Recall@2: 0.200, F1@2: 0.200\n",
            "Model 2 - Precision@2: 0.400, Recall@2: 0.200, F1@2: 0.267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fJpmk9VLNw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Method                    | Type                  | Embeddings | Suitable for Short Texts | Notes                            |\n",
        "| ------------------------- | --------------------- | ---------- | ------------------------ | -------------------------------- |\n",
        "| LDA                       | Probabilistic         | No         | No                       | Classic baseline                 |\n",
        "| NMF                       | Matrix Factorization  | No         | No                       | Fast, interpretable              |\n",
        "| Neural Topic Models (VAE) | Neural Probabilistic  | Optional   | Yes                      | Powerful but complex             |\n",
        "| Top2Vec                   | Embedding Clustering  | Yes        | Yes                      | No predefined topic number       |\n",
        "| GSDMM                     | Probabilistic         | No         | Yes                      | Good for short texts             |\n",
        "| CTM                       | Contextualized Neural | Yes (BERT) | Yes                      | State-of-art for semantic topics |\n",
        "| LDA2Vec                   | Hybrid                | Yes        | No                       | Combines LDA & embeddings        |\n"
      ],
      "metadata": {
        "id": "J1RfXh42LMCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CTM method\n",
        "\n",
        "# Install or upgrade cython, which gensim sometimes needs\n",
        "!pip install --upgrade cython\n",
        "\n",
        "# Now install gensim (specific stable version recommended)\n",
        "!pip install gensim\n",
        "\n",
        "# Finally install contextualized-topic-models (which depends on gensim)\n",
        "!pip install contextualized-topic-models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZWVqli9K8HY",
        "outputId": "fbc7faa1-c8bc-4e79-a776-b824ab8463a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.1.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Collecting contextualized-topic-models\n",
            "  Using cached contextualized_topic_models-2.5.0-py2.py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.11/dist-packages (from contextualized-topic-models) (1.26.4)\n",
            "Requirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from contextualized-topic-models) (0.21.0+cu124)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from contextualized-topic-models) (2.6.0+cu124)\n",
            "Collecting gensim==4.2.0 (from contextualized-topic-models)\n",
            "  Using cached gensim-4.2.0.tar.gz (23.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentence-transformers>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from contextualized-topic-models) (4.1.0)\n",
            "Requirement already satisfied: wordcloud>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from contextualized-topic-models) (1.9.4)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from contextualized-topic-models) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.56.0 in /usr/local/lib/python3.11/dist-packages (from contextualized-topic-models) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from contextualized-topic-models) (1.13.1)\n",
            "Collecting ipywidgets==7.5.1 (from contextualized-topic-models)\n",
            "  Using cached ipywidgets-7.5.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ipython==8.10.0 (from contextualized-topic-models)\n",
            "  Using cached ipython-8.10.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.2.0->contextualized-topic-models) (7.3.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython==8.10.0->contextualized-topic-models) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython==8.10.0->contextualized-topic-models) (4.4.2)\n",
            "Collecting jedi>=0.16 (from ipython==8.10.0->contextualized-topic-models)\n",
            "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython==8.10.0->contextualized-topic-models) (0.1.7)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython==8.10.0->contextualized-topic-models) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /usr/local/lib/python3.11/dist-packages (from ipython==8.10.0->contextualized-topic-models) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython==8.10.0->contextualized-topic-models) (2.19.2)\n",
            "Collecting stack-data (from ipython==8.10.0->contextualized-topic-models)\n",
            "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.11/dist-packages (from ipython==8.10.0->contextualized-topic-models) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython==8.10.0->contextualized-topic-models) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==7.5.1->contextualized-topic-models) (6.17.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==7.5.1->contextualized-topic-models) (5.10.4)\n",
            "Collecting widgetsnbextension~=3.5.0 (from ipywidgets==7.5.1->contextualized-topic-models)\n",
            "  Using cached widgetsnbextension-3.5.2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython==8.10.0->contextualized-topic-models) (0.2.13)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (6.5.7)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (6.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython==8.10.0->contextualized-topic-models) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (4.3.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models) (3.2.3)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models) (4.24.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models) (0.26.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (25.1.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (0.2.0)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (1.3.1)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (1.5.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (1.4.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (1.16.0)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (4.14.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython==8.10.0->contextualized-topic-models) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models) (1.17.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.1.1->contextualized-topic-models) (4.53.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.1.1->contextualized-topic-models) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.1.1->contextualized-topic-models) (0.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.1.1->contextualized-topic-models) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.1.1->contextualized-topic-models) (1.1.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open>=1.8.1->gensim==4.2.0->contextualized-topic-models) (1.17.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->contextualized-topic-models) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->contextualized-topic-models) (1.3.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (2.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models) (2.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=2.1.1->contextualized-topic-models) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.1.1->contextualized-topic-models) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.1.1->contextualized-topic-models) (3.6.0)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython==8.10.0->contextualized-topic-models)\n",
            "  Using cached executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython==8.10.0->contextualized-topic-models)\n",
            "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack-data->ipython==8.10.0->contextualized-topic-models)\n",
            "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Using cached contextualized_topic_models-2.5.0-py2.py3-none-any.whl (36 kB)\n",
            "Using cached ipython-8.10.0-py3-none-any.whl (784 kB)\n",
            "Using cached ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n",
            "Using cached widgetsnbextension-3.5.2-py2.py3-none-any.whl (1.6 MB)\n",
            "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Using cached asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Using cached executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: gensim\n",
            "\u001b[33m  DEPRECATION: Building 'gensim' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gensim'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
            "Failed to build gensim\n",
            "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (gensim)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y build-essential python3-dev\n",
        "!pip install --upgrade pip setuptools wheel cython\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTaa0QLuMGtO",
        "outputId": "6f1daf4c-e5d0-4b8a-db63-d86c0aa9d4fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "python3-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "print(gensim.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "F60tyPy1MWMo",
        "outputId": "5c82b7e5-d8b2-450e-bf60-5a7d7d818e54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-690879376.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsi\u001b[0m  \u001b[0;31m# gamma function utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Shared Imports & Setup ===\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
        "from contextualized_topic_models.utils.data_preparation import TextHandler\n",
        "import scann\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "# Download punkt tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# === AllergyTopicSearcherModel1 with CTM instead of BERTopic ===\n",
        "class AllergyTopicSearcherModel1:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, model_name=\"emilyalsentzer/Bio_ClinicalBERT\", num_topics=6):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model_name = model_name\n",
        "        self.num_topics = num_topics\n",
        "\n",
        "        self.embedding_model = None\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        # SentenceTransformer model\n",
        "        self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "        entity_to_chunk = defaultdict(list)\n",
        "        all_entities = []\n",
        "        for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "            for ent in ents:\n",
        "                ent_lower = ent.lower()\n",
        "                all_entities.append(ent_lower)\n",
        "                entity_to_chunk[ent_lower].append(idx)\n",
        "\n",
        "        unique_entities = sorted(set(all_entities))\n",
        "        entity_embeddings = self.embedding_model.encode(unique_entities, normalize_embeddings=True)\n",
        "\n",
        "        # Prepare tokenized corpus for CTM\n",
        "        text_handler = TextHandler()\n",
        "        tokenized_entities = [text_handler.tokenize(ent) for ent in unique_entities]\n",
        "\n",
        "        # Create CTM Dataset: bag of words + contextual embeddings\n",
        "        ctm_dataset = CTMDataset(tokenized_entities, entity_embeddings)\n",
        "\n",
        "        # Initialize CTM model\n",
        "        self.topic_model = CombinedTM(\n",
        "            bow_size=ctm_dataset.get_vocab_size(),\n",
        "            contextual_size=entity_embeddings.shape[1],\n",
        "            n_components=self.num_topics,\n",
        "            num_epochs=300,\n",
        "            train_embeddings=False,\n",
        "            seed=SEED\n",
        "        )\n",
        "\n",
        "        # Train CTM on entities + embeddings\n",
        "        self.topic_model.fit(ctm_dataset)\n",
        "\n",
        "        # Get topic assignments (theta)\n",
        "        topics_prob = self.topic_model.get_thetas(ctm_dataset)  # shape (num_entities, num_topics)\n",
        "        topics = np.argmax(topics_prob, axis=1)\n",
        "\n",
        "        # Organize entities per topic\n",
        "        topic_to_entities = defaultdict(list)\n",
        "        for ent, topic in zip(unique_entities, topics):\n",
        "            topic_to_entities[topic].append(ent)\n",
        "\n",
        "        # Collect sentences per topic from chunks based on entities\n",
        "        topic_contexts = defaultdict(list)\n",
        "        for topic, entities in topic_to_entities.items():\n",
        "            for ent in entities:\n",
        "                for chunk_id in entity_to_chunk[ent]:\n",
        "                    for sent in sent_tokenize(self.chunks[chunk_id]):\n",
        "                        if ent in sent.lower():\n",
        "                            topic_contexts[topic].append(sent)\n",
        "\n",
        "        # Remove duplicates in sentences\n",
        "        for topic in topic_contexts:\n",
        "            topic_contexts[topic] = list(set(topic_contexts[topic]))\n",
        "\n",
        "        # Compute topic embeddings by averaging sentence embeddings\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "        for topic_id, sentences in topic_contexts.items():\n",
        "            if not sentences:\n",
        "                continue\n",
        "            sent_embs = self.embedding_model.encode(sentences, normalize_embeddings=True)\n",
        "            mean_emb = np.mean(sent_embs, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": topic_to_entities[topic_id],\n",
        "                \"sentences\": sentences,\n",
        "                \"sentence_embeddings\": sent_embs\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 5)\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=2):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        output = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            sents = meta[\"sentences\"]\n",
        "            sent_embs = meta[\"sentence_embeddings\"]\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            output += [sents[i] for i in top_indices]\n",
        "        return output\n",
        "\n",
        "\n",
        "# === AllergyTopicSearcherModel2 with CTM instead of BERTopic ===\n",
        "class AllergyTopicSearcherModel2:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, model_name=\"emilyalsentzer/Bio_ClinicalBERT\", num_topics=6):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model_name = model_name\n",
        "        self.num_topics = num_topics\n",
        "\n",
        "        self.embedding_model = None\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "        entity_context_pairs = []\n",
        "        entity_to_chunk = defaultdict(list)\n",
        "\n",
        "        for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "            chunk = self.chunks[idx].lower()\n",
        "            sentences = sent_tokenize(chunk)\n",
        "            for ent in ents:\n",
        "                ent_lower = ent.lower()\n",
        "                for sent in sentences:\n",
        "                    if ent_lower in sent:\n",
        "                        entity_context_pairs.append((ent_lower, sent.strip()))\n",
        "                        entity_to_chunk[ent_lower].append(idx)\n",
        "                        break\n",
        "\n",
        "        contextual_texts = [f\"{ent}: {context}\" for ent, context in entity_context_pairs]\n",
        "        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=True)\n",
        "\n",
        "        # Tokenize texts for CTM\n",
        "        text_handler = TextHandler()\n",
        "        tokenized_contexts = [text_handler.tokenize(text) for text in contextual_texts]\n",
        "\n",
        "        ctm_dataset = CTMDataset(tokenized_contexts, contextual_embeddings)\n",
        "\n",
        "        # Train CTM\n",
        "        self.topic_model = CombinedTM(\n",
        "            bow_size=ctm_dataset.get_vocab_size(),\n",
        "            contextual_size=contextual_embeddings.shape[1],\n",
        "            n_components=self.num_topics,\n",
        "            num_epochs=300,\n",
        "            train_embeddings=False,\n",
        "            seed=SEED\n",
        "        )\n",
        "\n",
        "        self.topic_model.fit(ctm_dataset)\n",
        "\n",
        "        # Get topic assignments\n",
        "        topics_prob = self.topic_model.get_thetas(ctm_dataset)\n",
        "        topics = np.argmax(topics_prob, axis=1)\n",
        "\n",
        "        topic_to_contexts = defaultdict(list)\n",
        "        topic_to_entities = defaultdict(set)\n",
        "        topic_to_embeddings = defaultdict(list)\n",
        "\n",
        "        for i, topic in enumerate(topics):\n",
        "            ent, context = entity_context_pairs[i]\n",
        "            topic_to_contexts[topic].append(context)\n",
        "            topic_to_entities[topic].add(ent)\n",
        "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id in topic_to_contexts:\n",
        "            embeddings = topic_to_embeddings[topic_id]\n",
        "            mean_emb = np.mean(embeddings, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": list(topic_to_entities[topic_id]),\n",
        "                \"sentences\": topic_to_contexts[topic_id],\n",
        "                \"sentence_embeddings\": np.array(topic_to_embeddings[topic_id])\n",
        "            })\n",
        "\n",
        "        self.topic_embeddings = np.array(topic_embeddings)\n",
        "\n",
        "        num_clusters = min(len(self.topic_embeddings), 5)\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=2):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        output = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            seen = set()\n",
        "            sents = []\n",
        "            embs = []\n",
        "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
        "                if sent not in seen:\n",
        "                    seen.add(sent)\n",
        "                    sents.append(sent)\n",
        "                    embs.append(emb)\n",
        "            sent_embs = np.array(embs)\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            output += [sents[i] for i in top_indices]\n",
        "        return output\n",
        "\n",
        "\n",
        "# === Evaluation ===\n",
        "chunks = [\n",
        "    \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\",\n",
        "    \"Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\",\n",
        "    \"Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\",\n",
        "    \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\",\n",
        "    \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\",\n",
        "    \"He walks in cold weather but has no allergy symptoms or reactions.\"\n",
        "]\n",
        "\n",
        "manual_entities_per_chunk = [\n",
        "    [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "    [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "    [\"anaphylaxis\", \"allergic reaction\"],\n",
        "    [\"food allergies\", \"milk\", \"eggs\"],\n",
        "    [\"urticaria\", \"hives\", \"eczema\", \"allergies\"],\n",
        "    [\"cold weather\", \"allergy symptoms\", \"reactions\"]\n",
        "]\n",
        "\n",
        "queries = [\n",
        "    \"peanut allergy\",\n",
        "    \"symptoms of anaphylaxis\",\n",
        "    \"hay fever\",\n",
        "    \"eczema treatment\",\n",
        "    \"allergic reaction to milk\",\n",
        "    \"signs of food allergy\",\n",
        "    \"urticaria causes\",\n",
        "    \"pet dander allergies\",\n",
        "    \"cold weather allergy\",\n",
        "    \"hives and swelling\"\n",
        "]\n",
        "\n",
        "print(\"üîß Initializing Model 1 (CTM)...\")\n",
        "m1 = AllergyTopicSearcherModel1(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"üîß Initializing Model 2 (CTM)...\")\n",
        "m2 = AllergyTopicSearcherModel2(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"\\n\\nüîç Starting Evaluation\")\n",
        "for q in queries:\n",
        "    res1 = m1.search(q, top_k_sents=2)\n",
        "    res2 = m2.search(q, top_k_sents=2)\n",
        "\n",
        "    print(f\"\\n\\nüîé Query: {q}\")\n",
        "    print(\"-\" * 90)\n",
        "    print(\"üìò Model 1:\")\n",
        "    for i, r in enumerate(res1, 1):\n",
        "        print(f\"  {i}. {r}\")\n",
        "    print(\"üìô Model 2:\")\n",
        "    for i, r in enumerate(res2, 1):\n",
        "        print(f\"  {i}. {r}\")\n",
        "    print(\"=\" * 90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "zsxCok5PLArP",
        "outputId": "485cc60b-89b9-4047-e7b2-b10000cca316"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'contextualized_topic_models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1894081275.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextualized_topic_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCombinedTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextualized_topic_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCTMDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextualized_topic_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preparation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contextualized_topic_models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKq6MiZCNJWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#topic2vec\n",
        "!pip install top2vec[sentence_encoders]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVRHd_jxNJZ0",
        "outputId": "cdece43e-1efb-469e-f199-2cae50ba766f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting top2vec[sentence_encoders]\n",
            "  Downloading top2vec-1.0.36-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (1.6.1)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (4.3.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (0.5.8)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (0.8.40)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (1.9.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (4.67.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (0.16.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (2.18.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (1.15.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (7.3.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->top2vec[sentence_encoders]) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim>=4.0.0->top2vec[sentence_encoders]) (1.17.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.13)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->top2vec[sentence_encoders]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->top2vec[sentence_encoders]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->top2vec[sentence_encoders]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->top2vec[sentence_encoders]) (1.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (80.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (4.14.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->top2vec[sentence_encoders]) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->top2vec[sentence_encoders]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->top2vec[sentence_encoders]) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->top2vec[sentence_encoders]) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->top2vec[sentence_encoders]) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (0.1.2)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub->top2vec[sentence_encoders]) (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (0.33.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->top2vec[sentence_encoders]) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->top2vec[sentence_encoders]) (1.1.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud->top2vec[sentence_encoders]) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud->top2vec[sentence_encoders]) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (3.2.3)\n",
            "Downloading top2vec-1.0.36-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: top2vec\n",
            "Successfully installed top2vec-1.0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade gensim\n",
        "!pip install --upgrade top2vec[sentence_encoders]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3KgXsvyUN6LO",
        "outputId": "6b8d9d4c-af3d-47b6-b866-d694f7f383e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.1)\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.0.2\n",
            "\u001b[2K    Uninstalling numpy-2.0.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K  Attempting uninstall: scipy\n",
            "\u001b[2K    Found existing installation: scipy 1.16.0\n",
            "\u001b[2K    Uninstalling scipy-1.16.0:\n",
            "\u001b[2K      Successfully uninstalled scipy-1.16.0\n",
            "\u001b[2K  Attempting uninstall: gensim\n",
            "\u001b[2K    Found existing installation: gensim 4.3.1\n",
            "\u001b[2K    Uninstalling gensim-4.3.1:\n",
            "\u001b[2K      Successfully uninstalled gensim-4.3.1\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [gensim]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scann 1.4.0 requires numpy~=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "1d57a2649c7a4bdbbedcf06dfa7ef0bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec[sentence_encoders] in /usr/local/lib/python3.11/dist-packages (1.0.36)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (1.6.1)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (4.3.3)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (0.5.8)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (0.8.40)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (1.9.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (4.67.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (0.16.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (from top2vec[sentence_encoders]) (2.18.1)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (7.3.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->top2vec[sentence_encoders]) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim>=4.0.0->top2vec[sentence_encoders]) (1.17.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.13)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->top2vec[sentence_encoders]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->top2vec[sentence_encoders]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->top2vec[sentence_encoders]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->top2vec[sentence_encoders]) (1.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (80.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (4.14.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->top2vec[sentence_encoders]) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->top2vec[sentence_encoders]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->top2vec[sentence_encoders]) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->top2vec[sentence_encoders]) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->top2vec[sentence_encoders]) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->top2vec[sentence_encoders]) (0.1.2)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub->top2vec[sentence_encoders]) (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (0.33.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->top2vec[sentence_encoders]) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->top2vec[sentence_encoders]) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->top2vec[sentence_encoders]) (1.1.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud->top2vec[sentence_encoders]) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud->top2vec[sentence_encoders]) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (3.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from top2vec import Top2Vec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aceFbUmN9qi",
        "outputId": "5c3988e3-38e1-4400-8c6d-374ac545fec6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
            "\n",
            "stdout:\n",
            "\n",
            "\n",
            "\n",
            "stderr:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 4, in <module>\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 314, in __getattr__\n",
            "    raise CudaSupportError(\"Error at driver init: \\n%s:\" %\n",
            "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: \n",
            "\n",
            "CUDA driver library cannot be found.\n",
            "If you are sure that a CUDA driver is installed,\n",
            "try setting environment variable NUMBA_CUDA_DRIVER\n",
            "with the file path of the CUDA driver shared library.\n",
            ":\n",
            "\n",
            "\n",
            "Not patching Numba\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/cudf/utils/gpu_utils.py:62: UserWarning: Failed to dlopen libcuda.so.1\n",
            "  warnings.warn(str(e))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Full Robust Top2Vec Code with Two Models and Evaluation ===\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from top2vec import Top2Vec\n",
        "import scann\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Download punkt tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# === Model 1 ===\n",
        "class AllergyTopicSearcherTop2VecModel1:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "        self.topic_embeddings = None\n",
        "        self.topic_metadata = []\n",
        "        self.searcher = None\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        entity_to_chunk = defaultdict(list)\n",
        "        all_entities = []\n",
        "        for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "            for ent in ents:\n",
        "                ent_lower = ent.lower()\n",
        "                all_entities.append(ent_lower)\n",
        "                entity_to_chunk[ent_lower].append(idx)\n",
        "        unique_entities = sorted(set(all_entities))\n",
        "\n",
        "        if not unique_entities:\n",
        "            raise ValueError(\"No unique entities found. Check input data.\")\n",
        "\n",
        "        topic_model = Top2Vec(\n",
        "            documents=unique_entities,\n",
        "            embedding_model=\"universal-sentence-encoder\",\n",
        "            min_count=1,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        topic_words, _, topic_nums = topic_model.get_topics()\n",
        "\n",
        "        topic_to_entities = defaultdict(list)\n",
        "        for i, topic in enumerate(topic_nums):\n",
        "            topic_to_entities[topic].append(unique_entities[i])\n",
        "\n",
        "        topic_contexts = defaultdict(list)\n",
        "        for topic, entities in topic_to_entities.items():\n",
        "            for ent in entities:\n",
        "                for chunk_id in entity_to_chunk[ent]:\n",
        "                    for sent in sent_tokenize(self.chunks[chunk_id]):\n",
        "                        if ent in sent.lower():\n",
        "                            topic_contexts[topic].append(sent)\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for topic_id, sents in topic_contexts.items():\n",
        "            sents = list(set(sents))\n",
        "            if not sents:\n",
        "                continue\n",
        "            sent_embs = self.embedding_model.encode(sents, normalize_embeddings=True)\n",
        "            mean_emb = np.mean(sent_embs, axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": topic_id,\n",
        "                \"entities\": topic_to_entities[topic_id],\n",
        "                \"sentences\": sents,\n",
        "                \"sentence_embeddings\": sent_embs\n",
        "            })\n",
        "\n",
        "        if not topic_embeddings:\n",
        "            raise ValueError(\"No topic embeddings found in Model 1.\")\n",
        "\n",
        "        self.topic_embeddings = np.vstack(topic_embeddings)\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=min(5, len(self.topic_embeddings)), num_leaves_to_search=2)\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=2):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, _ = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        output = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            sent_embs = meta[\"sentence_embeddings\"]\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            output += [meta[\"sentences\"][i] for i in top_indices]\n",
        "        return output\n",
        "\n",
        "\n",
        "# === Model 2 ===\n",
        "class AllergyTopicSearcherTop2VecModel2:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "        self.topic_embeddings = None\n",
        "        self.topic_metadata = []\n",
        "        self.searcher = None\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        entity_context_pairs = []\n",
        "        for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "            chunk = self.chunks[idx].lower()\n",
        "            for sent in sent_tokenize(chunk):\n",
        "                for ent in ents:\n",
        "                    ent_lower = ent.lower()\n",
        "                    if ent_lower in sent:\n",
        "                        entity_context_pairs.append((ent_lower, sent.strip()))\n",
        "                        break\n",
        "\n",
        "        contextual_texts = [f\"{ent}: {ctx}\" for ent, ctx in entity_context_pairs]\n",
        "\n",
        "        if not contextual_texts:\n",
        "            raise ValueError(\"No contextual texts found. Check entity-chunk pairings.\")\n",
        "\n",
        "        topic_model = Top2Vec(\n",
        "            documents=contextual_texts,\n",
        "            embedding_model=\"universal-sentence-encoder\",\n",
        "            min_count=1,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        topic_words, _, topic_nums = topic_model.get_topics()\n",
        "\n",
        "        topic_to_data = defaultdict(lambda: {\"contexts\": [], \"ents\": [], \"embs\": []})\n",
        "        for i, topic in enumerate(topic_nums):\n",
        "            ent, ctx = entity_context_pairs[i]\n",
        "            topic_to_data[topic][\"contexts\"].append(ctx)\n",
        "            topic_to_data[topic][\"ents\"].append(ent)\n",
        "            topic_to_data[topic][\"embs\"].append(self.embedding_model.encode(f\"{ent}: {ctx}\", normalize_embeddings=True))\n",
        "\n",
        "        topic_embeddings = []\n",
        "        topic_metadata = []\n",
        "\n",
        "        for tid, data in topic_to_data.items():\n",
        "            if not data[\"embs\"]:\n",
        "                continue\n",
        "            mean_emb = np.mean(data[\"embs\"], axis=0)\n",
        "            mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "            topic_embeddings.append(mean_emb)\n",
        "            topic_metadata.append({\n",
        "                \"topic_id\": tid,\n",
        "                \"entities\": list(set(data[\"ents\"])),\n",
        "                \"sentences\": list(set(data[\"contexts\"])),\n",
        "                \"sentence_embeddings\": np.array(data[\"embs\"])\n",
        "            })\n",
        "\n",
        "        if not topic_embeddings:\n",
        "            raise ValueError(\"No topic embeddings found in Model 2.\")\n",
        "\n",
        "        self.topic_embeddings = np.vstack(topic_embeddings)\n",
        "        self.topic_metadata = topic_metadata\n",
        "\n",
        "        self.searcher = (\n",
        "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "            .tree(num_leaves=min(5, len(self.topic_embeddings)), num_leaves_to_search=2)\n",
        "            .score_brute_force()\n",
        "            .reorder(3)\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=2):\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "        neighbors, _ = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        output = []\n",
        "        for idx in neighbors:\n",
        "            meta = self.topic_metadata[idx]\n",
        "            sent_embs = meta[\"sentence_embeddings\"]\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "            output += [meta[\"sentences\"][i] for i in top_indices]\n",
        "        return output\n",
        "\n",
        "\n",
        "# === Evaluation ===\n",
        "chunks = [\n",
        "    \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\",\n",
        "    \"Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\",\n",
        "    \"Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\",\n",
        "    \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\",\n",
        "    \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\",\n",
        "    \"He walks in cold weather but has no allergy symptoms or reactions.\"\n",
        "]\n",
        "\n",
        "manual_entities_per_chunk = [\n",
        "    [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "    [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "    [\"anaphylaxis\", \"allergic reaction\"],\n",
        "    [\"food allergies\", \"milk\", \"eggs\"],\n",
        "    [\"urticaria\", \"hives\", \"eczema\", \"allergies\"],\n",
        "    [\"cold weather\", \"allergy symptoms\", \"reactions\"]\n",
        "]\n",
        "\n",
        "queries = [\n",
        "    \"peanut allergy\",\n",
        "    \"symptoms of anaphylaxis\",\n",
        "    \"hay fever\",\n",
        "    \"eczema treatment\",\n",
        "    \"allergic reaction to milk\",\n",
        "    \"signs of food allergy\",\n",
        "    \"urticaria causes\",\n",
        "    \"pet dander allergies\",\n",
        "    \"cold weather allergy\",\n",
        "    \"hives and swelling\"\n",
        "]\n",
        "\n",
        "print(\"üîß Initializing Model 1...\")\n",
        "m1 = AllergyTopicSearcherTop2VecModel1(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"üîß Initializing Model 2...\")\n",
        "m2 = AllergyTopicSearcherTop2VecModel2(chunks, manual_entities_per_chunk)\n",
        "\n",
        "print(\"\\\\nüîç Evaluating Queries:\")\n",
        "for q in queries:\n",
        "    print(f\"\\\\nüß™ Query: {q}\")\n",
        "    print(\"Model 1:\", m1.search(q))\n",
        "    print(\"Model 2:\", m2.search(q))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "84-9gShYNteW",
        "outputId": "d69e00f4-3193-45aa-db86-78e5dc9ab307"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Model 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "2025-07-06 15:10:23,258 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
            "INFO:top2vec:Downloading universal-sentence-encoder model\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-656253925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîß Initializing Model 1...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAllergyTopicSearcherTop2VecModel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual_entities_per_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîß Initializing Model 2...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-656253925.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, chunks, manual_entities_per_chunk, embedding_model_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-656253925.py\u001b[0m in \u001b[0;36m_prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No unique entities found. Check input data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         topic_model = Top2Vec(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique_entities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0membedding_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"universal-sentence-encoder\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/top2vec/top2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, contextual_top2vec, c_top2vec_smoothing_window, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, gpu_umap, hdbscan_args, gpu_hdbscan, index_topics, verbose)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics_indexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         self.compute_topics(umap_args=umap_args,\n\u001b[0m\u001b[1;32m    785\u001b[0m                             \u001b[0mhdbscan_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhdbscan_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                             \u001b[0mtopic_merge_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopic_merge_delta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/top2vec/top2vec.py\u001b[0m in \u001b[0;36mcompute_topics\u001b[0;34m(self, umap_args, hdbscan_args, topic_merge_delta, gpu_umap, gpu_hdbscan, index_topics, contextual_top2vec, c_top2vec_smoothing_window)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;31m# create topic vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_topic_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \u001b[0;31m# deduplicate topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/top2vec/top2vec.py\u001b[0m in \u001b[0;36m_create_topic_vectors\u001b[0;34m(self, cluster_labels)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0munique_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         self.topic_vectors = self._l2_normalize(\n\u001b[0;32m-> 1061\u001b[0;31m             np.vstack([self.document_vectors[np.where(cluster_labels == label)[0]]\n\u001b[0m\u001b[1;32m   1062\u001b[0m                       .mean(axis=0) for label in unique_labels]))\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
          ]
        }
      ]
    }
  ]
}