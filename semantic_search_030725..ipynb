{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxIagHqS9XT_",
        "outputId": "23eb8e4e-c24c-44f3-bab7-d010d018dfb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scann in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.8)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install numpy sentence-transformers bertopic hdbscan nltk scann\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import logging\n",
        "\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "import scann\n",
        "\n",
        "# === SET RANDOM SEED FOR DETERMINISM ===\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "\n",
        "# === SETUP LOGGING ===\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# === DOWNLOAD TOKENIZER ===\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "class AllergyTopicSearcher:\n",
        "    def __init__(self, chunks, manual_entities_per_chunk, model_name=\"emilyalsentzer/Bio_ClinicalBERT\"):\n",
        "        self.chunks = chunks\n",
        "        self.manual_entities_per_chunk = manual_entities_per_chunk\n",
        "        self.embedding_model_name = model_name\n",
        "\n",
        "        # Initialize models and data holders\n",
        "        self.embedding_model = None\n",
        "        self.topic_model = None\n",
        "        self.topic_metadata = []\n",
        "        self.topic_embeddings = None\n",
        "        self.searcher = None\n",
        "\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        try:\n",
        "            logger.info(\"Loading embedding model...\")\n",
        "            self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "            # Step 1: Map entities to chunk IDs\n",
        "            entity_to_chunk = defaultdict(list)\n",
        "            all_entities = []\n",
        "            for idx, ents in enumerate(self.manual_entities_per_chunk):\n",
        "                for ent in ents:\n",
        "                    ent_lower = ent.lower()\n",
        "                    all_entities.append(ent_lower)\n",
        "                    entity_to_chunk[ent_lower].append(idx)\n",
        "\n",
        "            unique_entities = sorted(set(all_entities))\n",
        "            logger.info(f\"Unique entities found: {len(unique_entities)}\")\n",
        "\n",
        "            # Step 2: Embed unique entities\n",
        "            entity_embeddings = self.embedding_model.encode(unique_entities, normalize_embeddings=True)\n",
        "\n",
        "            # Step 3: Fit BERTopic on entities (make deterministic)\n",
        "            umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=SEED)\n",
        "            hdbscan_model = HDBSCAN(min_cluster_size=2, min_samples=1, metric='euclidean',\n",
        "                                    prediction_data=True)\n",
        "\n",
        "            self.topic_model = BERTopic(\n",
        "                embedding_model=self.embedding_model,\n",
        "                umap_model=umap_model,\n",
        "                hdbscan_model=hdbscan_model,\n",
        "                representation_model=KeyBERTInspired(),\n",
        "                calculate_probabilities=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            topics, _ = self.topic_model.fit_transform(unique_entities, embeddings=entity_embeddings)\n",
        "\n",
        "            # Step 4: Organize topics and gather sentences\n",
        "            topic_to_entities = defaultdict(list)\n",
        "            for ent, topic in zip(unique_entities, topics):\n",
        "                topic_to_entities[topic].append(ent)\n",
        "\n",
        "            topic_contexts = defaultdict(list)\n",
        "            for topic, entities in topic_to_entities.items():\n",
        "                for ent in entities:\n",
        "                    for chunk_id in entity_to_chunk[ent]:\n",
        "                        for sent in sent_tokenize(self.chunks[chunk_id]):\n",
        "                            if ent in sent.lower():\n",
        "                                topic_contexts[topic].append(sent)\n",
        "\n",
        "            # Remove duplicates\n",
        "            for topic in topic_contexts:\n",
        "                topic_contexts[topic] = list(set(topic_contexts[topic]))\n",
        "\n",
        "            # Step 5: Compute topic embeddings from sentence embeddings\n",
        "            topic_embeddings = []\n",
        "            topic_metadata = []\n",
        "\n",
        "            for topic_id, sentences in topic_contexts.items():\n",
        "                if not sentences:\n",
        "                    continue\n",
        "                sent_embs = self.embedding_model.encode(sentences, normalize_embeddings=True)\n",
        "                mean_emb = np.mean(sent_embs, axis=0)\n",
        "                mean_emb /= np.linalg.norm(mean_emb) + 1e-10\n",
        "                topic_embeddings.append(mean_emb)\n",
        "                topic_metadata.append({\n",
        "                    \"topic_id\": topic_id,\n",
        "                    \"entities\": topic_to_entities[topic_id],\n",
        "                    \"sentences\": sentences,\n",
        "                    \"sentence_embeddings\": sent_embs\n",
        "                })\n",
        "\n",
        "            self.topic_embeddings = np.array(topic_embeddings)\n",
        "\n",
        "            if len(self.topic_embeddings) == 0:\n",
        "                raise ValueError(\"No topic embeddings generated.\")\n",
        "\n",
        "            # Step 6: Build ScaNN search index\n",
        "            num_clusters = min(len(self.topic_embeddings), 5)\n",
        "            self.searcher = (\n",
        "                scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
        "                .tree(num_leaves=num_clusters, num_leaves_to_search=2, training_sample_size=len(self.topic_embeddings))\n",
        "                .score_brute_force()\n",
        "                .reorder(3)\n",
        "                .build()\n",
        "            )\n",
        "\n",
        "            self.topic_metadata = topic_metadata\n",
        "            logger.info(\"Preparation complete. Ready to search.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during preparation: {e}\")\n",
        "            raise\n",
        "\n",
        "    def search(self, query, top_k_topics=1, top_k_sents=3):\n",
        "        if self.searcher is None or not self.topic_metadata:\n",
        "            raise RuntimeError(\"Searcher not initialized. Call _prepare() first.\")\n",
        "\n",
        "        print(f\"\\nðŸ”Ž Query: '{query}'\\n{'=' * 60}\")\n",
        "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "\n",
        "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
        "\n",
        "        for rank, (idx, score) in enumerate(zip(neighbors, scores), 1):\n",
        "            meta = self.topic_metadata[idx]\n",
        "            print(f\"\\nRank {rank} [Score: {score:.4f}]\")\n",
        "            print(f\"  Topic ID   : {meta['topic_id']}\")\n",
        "            print(f\"  Entities   : {meta['entities']}\")\n",
        "\n",
        "            # Sentence-level ranking within topic\n",
        "            sents = meta[\"sentences\"]\n",
        "            sent_embs = meta[\"sentence_embeddings\"]\n",
        "            sent_embs_norm = sent_embs / np.linalg.norm(sent_embs, axis=1, keepdims=True)\n",
        "            sims = np.dot(sent_embs_norm, query_emb)\n",
        "            top_indices = sims.argsort()[::-1][:top_k_sents]\n",
        "\n",
        "            print(f\"  Top {top_k_sents} Relevant Sentences:\")\n",
        "            for i in top_indices:\n",
        "                print(f\"    - {sents[i]} (score: {sims[i]:.4f})\")\n",
        "\n",
        "\n",
        "# === Example usage ===\n",
        "if __name__ == \"__main__\":\n",
        "    chunks = [\n",
        "        \"Peanut allergy is one of the most common causes of severe allergic reactions. Symptoms can include hives, swelling, and anaphylaxis.\",\n",
        "        \"Allergic rhinitis, commonly known as hay fever, is an allergic response to pollen, dust, or pet dander.\",\n",
        "        \"Anaphylaxis is a serious, potentially life-threatening allergic reaction that can occur rapidly.\",\n",
        "        \"Patients with food allergies, such as milk or eggs, need to be careful with their diet.\",\n",
        "        \"Skin reactions like urticaria (hives) and eczema are often signs of allergies.\",\n",
        "        \"He walks in cold weather but has no allergy symptoms or reactions.\"\n",
        "    ]\n",
        "\n",
        "    manual_entities_per_chunk = [\n",
        "        [\"peanut allergy\", \"hives\", \"swelling\", \"anaphylaxis\"],\n",
        "        [\"allergic rhinitis\", \"hay fever\", \"pollen\", \"dust\", \"pet dander\"],\n",
        "        [\"anaphylaxis\", \"allergic reaction\"],\n",
        "        [\"food allergies\", \"milk\", \"eggs\"],\n",
        "        [\"urticaria\", \"hives\", \"eczema\", \"allergies\"],\n",
        "        [\"cold weather\", \"allergy symptoms\", \"reactions\"]\n",
        "    ]\n",
        "\n",
        "    searcher = AllergyTopicSearcher(chunks, manual_entities_per_chunk)\n",
        "\n",
        "    # Run a test query\n",
        "    searcher.search(\"What are skin allergy symptoms?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqsQc7QtXYX9",
        "outputId": "fa8e10b0-27a3-40e7-a7e7-190cab400108"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name emilyalsentzer/Bio_ClinicalBERT. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”Ž Query: 'What are skin allergy symptoms?'\n",
            "============================================================\n",
            "\n",
            "Rank 1 [Score: 0.9274]\n",
            "  Topic ID   : 1\n",
            "  Entities   : ['allergic reaction', 'allergies', 'allergy symptoms', 'food allergies', 'peanut allergy']\n",
            "  Top 3 Relevant Sentences:\n",
            "    - Skin reactions like urticaria (hives) and eczema are often signs of allergies. (score: 0.9047)\n",
            "    - He walks in cold weather but has no allergy symptoms or reactions. (score: 0.9031)\n",
            "    - Peanut allergy is one of the most common causes of severe allergic reactions. (score: 0.8918)\n"
          ]
        }
      ]
    }
  ]
}