{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jQ_l2BOsULLR",
    "outputId": "1fd1d304-bb9b-4bca-dfdf-7551def7239e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Collecting scann\n",
      "  Downloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.9.post2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.43.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
      "Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scann, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bertopic\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bertopic-0.17.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 scann-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
      "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
      "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
      "Requirement already satisfied: scann in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.43.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.16.1\n",
      "    Uninstalling scipy-1.16.1:\n",
      "      Successfully uninstalled scipy-1.16.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scann 1.4.0 requires numpy~=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "8e8bdb5df3c64b45b91a97058a331c72",
       "pip_warning": {
        "packages": [
         "numpy",
         "scipy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install numpy sentence-transformers bertopic hdbscan nltk scann\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "!pip install sentence-transformers bertopic hdbscan umap-learn scann nltk datasets\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P16N5paOAvgg"
   },
   "source": [
    "# Topic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627,
     "referenced_widgets": [
      "e9fdce0cfe134264b8409bef4d78242f",
      "619680cd179f44b78595228463b41a6c",
      "ea9bfdc259ee495db9eddbc0a1e14ee7",
      "f252e9dfb25d45b48e4562c966c90e82",
      "20b8a541892c4f5a9e54925efe48dfb1",
      "3d141dda90954e5a9bf080f3055ab3b1",
      "f09f711cc6344734aec72580f63e089a",
      "5e2f3e08e6904163abcf7cc20cba7973",
      "f00b6363846e449db5373f0aca29aef2",
      "646ae89555414f3cb3901deb480c8196",
      "e550ce5d6cdc488f812be46e93f67683",
      "e7417e46aa16457d9853b5ba8128c493",
      "03e91802960f468a95d145edfa1daef8",
      "fb9ff246137d4084bbc6211ae6f6cba9",
      "f62a74da1c5f4ac88b1fa61639a44796",
      "d63b08cfcbf14a458dd5460dd4b84bba",
      "39b5b918a62e471984f3fba536ad05a7",
      "b264f4f7e75d47669951a3557594ce30",
      "cdabf2fea32146a8b2acaf8f49f4b98a",
      "3a05554327de485f9579b5fa2912f08b",
      "a7160c3be4e94725810bccb2d92fef43",
      "7dbff010da024e8ebdec9856f6aeebca",
      "fd3c50197244430c9cbab55127a0c05a",
      "9a1d94654cb9493c969f926b257aff02",
      "60a74b7db97846aab77500afa2c74bc0",
      "e4ae136d34654666a72c61b102c5b61e",
      "357d67d5b34e494da9484397008d2f36",
      "e9b480fa9b544670af34b63fca009bf1",
      "c27e0a7b0fb04a99b83389ebc3d24d89",
      "aecb9bfde4ec4b8d80c4acf3dd4e0e98",
      "0a6fe888e1ea449faf4a61a90eb021ac",
      "21913a5f2f4d48f5849aa80fa21b5d14",
      "d408666a1b11480f84633a6fed8dc538",
      "2780a8759a5341c6876eb7e7680bb9cf",
      "e4c7aa56197942a080bdf18361c923b1",
      "1c215df37d3244ca884d6556efdf1f08",
      "f980a24ce70543e8b2791c9e9220195a",
      "190c505d3fa6445593a017467f19501c",
      "b4e8a07e3a9346698dec4c552bf771f6",
      "77f041d44e0b45acabb57466271f076d",
      "2a28ef5f41404d45bcb45753978973a9",
      "4441103e4986475bbcf5eb09e963d089",
      "ca3b523d518a4abf8c4685f9efdc7ba4",
      "18f8990685244c3c95d66f9ed2892f6c",
      "6fc656f8019a4c2e9ed1b17fa691120b",
      "bd22aa4ae9b04d1590946abf9038ef17",
      "97ab63f5d2cb4529afc16fcc0b584047",
      "ba2e8cc08ed748bcb40c4a3d25ba39d1",
      "11f29c316ecf4dafad162500a6805559",
      "e14cc1b0b172404f80f0ea212db43050",
      "c5a92a79fc7340979665618b8ed7c31c",
      "af53ebe0e0c34e74978c650ef35d32d1",
      "f421a38630144a23ba096212617591f0",
      "d24e2fc151e34c2d95b84832207fa44a",
      "9fe6bb06d74143cd95f98ce66e042e9e",
      "c0eac39e7411437ebc5f6d11730517be",
      "1279f48e374248b984a422b0195e7568",
      "f0bb793f292d41a3bc6e05ca53ad344b",
      "69feb250386a4155b57ed14916687973",
      "c64f99088eb04274a723e545d7866b16",
      "0ef5c4b3526c4e2fab5f019967cb2993",
      "13bb6fca02ac4fe9aa3c65805dbb3dd2",
      "f2f712f4bed241d19b2b143689f469f8",
      "48cb0133e2c64e9c818a8e7eb9b005b7",
      "21998b8debdd43e2af85b0b400abbe27",
      "04dd691bb0c84d62be46c877d610c103",
      "bb155727e35e4fecbd401cbefd1027e9",
      "7e7927d954d540c98e5ecde0a4b72b19",
      "50e09382a03b4c3eb7ba028c7abf6d1f",
      "16f7bb9a720b4780b7473f604d7a462f",
      "8a0ae85753d242da814956a03db932d6",
      "88d9d8d07320484e9175c99935ac21b3",
      "dc1a0665d4144f32954f16e8681edf2d",
      "ee8eae7096b44c079fa54d31e111c197",
      "fa4b6ff531534b428d99a97ca0d4c2a4",
      "9f5fb7868bb24ce19d776d6dd074a2da",
      "aaa9ebcfca2247c48233d29eddc40d2f",
      "2ed109ebb2ff42a9bf7dcfb1b1a9585b",
      "3d96e8afd3a04b2d95a4ed1acfffe71a",
      "fa30a495890e43789a382117373dfcec",
      "ac8cfcf4a1584997a20b0aa1503bf4a4",
      "05c274e7c2634e5c8709c3a5dc14425f",
      "4edae02c0c764670b2defff49092fc80",
      "13a5c22fedc74949b76662aa81435f17",
      "fdc6e390f4144f45b8048eec99ea242b",
      "fe8ede14eb8d482d85803cf00fca9973",
      "2883063d1a16452bad9938b0b7905761",
      "6364f9c1209748e5bf8bffd91b0c428f",
      "1f4a59312ec44bc18ab237aedd0202a7",
      "6ffe44b86367495398d3b9bb8630006c",
      "110ca80e97d841639366913cf980a7dc",
      "a2ff7ddb1ac848239cf5d75d9aa33a55",
      "89ecc7b304f647899f56b4eb82e8c798",
      "ad47019eab894f11b3e2a4b39ec3055e",
      "ac8b253b836342eebb1b7f305fe8773a",
      "909752cf68be42f29bb3326e9632cbd2",
      "7c4be065ec5041a2b7a85e66239f2d80",
      "1b4e575c485b4f039e3ae1b81fbdc3bf",
      "12f3995bb59c4f17884742c840eb2901",
      "8fdc6a1bdcd0494789d438e969f52b86",
      "a138d37a6fe64383a83406aeb3f4e432",
      "1beadf5aee294588b32c62558d7c49f3",
      "329b263b991c4475b6f19df74bfd1f4a",
      "1819ec1d358d4ad394b5a13417e561c7",
      "f60284055e3b42269221db0ce5603cd1",
      "08f2e56cfd1745be8914fdf1393e141f",
      "a6f025e8262043ffa589e21e59bc26e3",
      "087769cf26b84e6ab93c63763f697f14",
      "fe25dfdef63047799945ec7f61ee9457",
      "00d8afe416f44b03b0bc469527bc7a1b",
      "e5d8205bfccd4d5d8ec14aa6a9427bf7",
      "c2340fbb5f8a4b5d8811baccce849aab",
      "5f295a74ee034bba971fd735f8902223",
      "a4e387e9d8ab458cbe3f60cb73bfda69",
      "c2993a6bad39419ea919b3d11173b23f",
      "c6c733d0d24f4a06847862458aed141c",
      "f7dd7cae3c724853b7edcff87422b0c7",
      "8c0ced2e14d04b2f92772da53945c0e4",
      "0d2083e96a434183832c024e126e3497",
      "981ffec1f28a46d89e945d67b503e0cc",
      "941f98255ddc4c9787270dfa6df13fff",
      "361313ec88284af48875c3cb2156dc68",
      "1a1ee95486d940aba1ab01f4ea49f5c8",
      "ab1746fa387b4ad7bd5af5f46fb18522",
      "ee8ffc2fdcfe41cabf4067f64f359ccb",
      "d51634713c8a498f82baea3113b2e9bc",
      "d2ff1b435aff49b7901fac0bcdd8d29a",
      "35c920dccaa1417191b99a74bad28cf0",
      "eb1069ac8ffc4fe3b34f80534b494da8",
      "73208c79b493468bbc81f9208286a470",
      "6d4336c43d1f469bb89764ece5e5d1cb",
      "4fa782dd3df14175ad1d8c061fedfbb3"
     ]
    },
    "id": "yp3mKY-GDbN6",
    "outputId": "942a8cee-854a-495c-879d-727319b0615c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fdce0cfe134264b8409bef4d78242f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7417e46aa16457d9853b5ba8128c493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3c50197244430c9cbab55127a0c05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2780a8759a5341c6876eb7e7680bb9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc656f8019a4c2e9ed1b17fa691120b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/691 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0eac39e7411437ebc5f6d11730517be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb155727e35e4fecbd401cbefd1027e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed109ebb2ff42a9bf7dcfb1b1a9585b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4a59312ec44bc18ab237aedd0202a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdc6a1bdcd0494789d438e969f52b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d8205bfccd4d5d8ec14aa6a9427bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361313ec88284af48875c3cb2156dc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best Hyperparameters Found ===\n",
      "Best UMAP: {'n_neighbors': 10, 'n_components': 3, 'min_dist': 0.25, 'metric': 'cosine'}\n",
      "Best HDBSCAN: {'min_cluster_size': 2, 'min_samples': 1, 'metric': 'euclidean'}\n",
      "Metrics => Coherence: 0.6393, Diversity: 0.4465, Silhouette: 0.5239\n",
      "Model ready for querying.\n",
      "\n",
      "=== Topic Quality Metrics ===\n",
      "Coherence Score (c_v): 0.6393\n",
      "Topic Diversity: 0.4465\n",
      " Silhouette Score: 0.5239\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS & SETUP ===\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "import scann\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics import silhouette_score, precision_recall_fscore_support\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === SEED FIXING ===\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed_all(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "nltk.download(\"punkt\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# === CLEANING & CONTEXT EXTRACTION (IMPROVED) ===\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):\n",
    "    entity_context_pairs = []\n",
    "    for idx, ents in enumerate(entities_per_chunk):\n",
    "        chunk = clean_text(chunks[idx])\n",
    "        sentences = sent_tokenize(chunk)\n",
    "        for ent in ents:\n",
    "            ent_lower = ent.lower()\n",
    "            matched = False\n",
    "            for i, sent in enumerate(sentences):\n",
    "                if ent_lower in sent:\n",
    "                    context = \" \".join(sentences[max(0, i - 1): i + 2]) if use_multi_sentence else sent.strip()\n",
    "                    enriched = f\"The concept '{ent_lower}' appears in the following context: {context}\"\n",
    "                    entity_context_pairs.append((ent_lower, enriched.strip()))\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                fallback = f\"The concept '{ent_lower}' appears in the following context: {chunk}\"\n",
    "                entity_context_pairs.append((ent_lower, fallback.strip()))\n",
    "    return entity_context_pairs\n",
    "\n",
    "\n",
    "# === TOPIC SEARCHER CLASS (WITH DEDUPLICATION, NOISE FILTERING) ===\n",
    "class AllergyTopicSearcher:\n",
    "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params,\n",
    "                 model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"):\n",
    "        self.chunks = chunks\n",
    "        self.entities_per_chunk = entities_per_chunk\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "        self.umap_params = umap_params\n",
    "        self.hdbscan_params = hdbscan_params\n",
    "\n",
    "        self.topic_model = None\n",
    "        self.topic_metadata = []\n",
    "        self.topic_embeddings = None\n",
    "        self.searcher = None\n",
    "\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        entity_context_pairs = extract_entity_contexts(self.chunks, self.entities_per_chunk)\n",
    "        contextual_texts = [ctx for _, ctx in entity_context_pairs]\n",
    "        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=False)\n",
    "\n",
    "        umap_model = UMAP(**self.umap_params, random_state=SEED)\n",
    "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
    "\n",
    "        self.topic_model = BERTopic(\n",
    "            embedding_model=self.embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            representation_model=KeyBERTInspired(),\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
    "\n",
    "        topic_to_contexts = defaultdict(list)\n",
    "        topic_to_entities = defaultdict(set)\n",
    "        topic_to_embeddings = defaultdict(list)\n",
    "\n",
    "        for i, topic in enumerate(topics):\n",
    "            if topic == -1:\n",
    "                continue  # Skip noisy topics\n",
    "            ent, ctx = entity_context_pairs[i]\n",
    "            topic_to_contexts[topic].append(ctx)\n",
    "            topic_to_entities[topic].add(ent)\n",
    "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
    "\n",
    "        topic_embeddings = []\n",
    "        topic_metadata = []\n",
    "\n",
    "        for topic_id in topic_to_contexts:\n",
    "            emb = topic_to_embeddings[topic_id]\n",
    "            centroid = np.mean(emb, axis=0)\n",
    "            centroid /= np.linalg.norm(centroid) + 1e-10\n",
    "            topic_embeddings.append(centroid)\n",
    "            topic_metadata.append({\n",
    "                \"topic_id\": topic_id,\n",
    "                \"entities\": list(topic_to_entities[topic_id]),\n",
    "                \"sentences\": topic_to_contexts[topic_id],\n",
    "                \"sentence_embeddings\": np.array(emb)\n",
    "            })\n",
    "\n",
    "        # === OPTIONAL: Merge semantically similar topics (cosine sim > 0.95)\n",
    "        deduped_metadata = []\n",
    "        used = set()\n",
    "\n",
    "        for i, emb_i in enumerate(topic_embeddings):\n",
    "            if i in used:\n",
    "                continue\n",
    "            group = [i]\n",
    "            sim_scores = cosine_similarity([emb_i], topic_embeddings)[0]\n",
    "            for j in range(i + 1, len(sim_scores)):\n",
    "                if sim_scores[j] > 0.95:\n",
    "                    group.append(j)\n",
    "                    used.add(j)\n",
    "\n",
    "            merged = {\n",
    "                \"topic_id\": i,\n",
    "                \"sentences\": [],\n",
    "                \"entities\": [],\n",
    "                \"sentence_embeddings\": []\n",
    "            }\n",
    "            for g in group:\n",
    "                merged[\"sentences\"] += topic_metadata[g][\"sentences\"]\n",
    "                merged[\"entities\"] += topic_metadata[g][\"entities\"]\n",
    "                merged[\"sentence_embeddings\"] += list(topic_metadata[g][\"sentence_embeddings\"])\n",
    "\n",
    "            merged[\"sentence_embeddings\"] = np.array(merged[\"sentence_embeddings\"])\n",
    "            merged[\"entities\"] = list(set(merged[\"entities\"]))\n",
    "            deduped_metadata.append(merged)\n",
    "\n",
    "        self.topic_metadata = deduped_metadata\n",
    "        self.topic_embeddings = np.array([\n",
    "            np.mean(m[\"sentence_embeddings\"], axis=0) /\n",
    "            (np.linalg.norm(np.mean(m[\"sentence_embeddings\"], axis=0)) + 1e-10)\n",
    "            for m in deduped_metadata\n",
    "        ])\n",
    "\n",
    "        num_points = len(self.topic_embeddings)\n",
    "        num_leaves = min(10, num_points)\n",
    "        leaves_to_search = min(5, num_leaves)\n",
    "\n",
    "        if num_points < 3:\n",
    "            raise ValueError(\"Not enough topics to build a search index.\")\n",
    "\n",
    "        self.searcher = (\n",
    "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
    "            .tree(num_leaves=num_leaves, num_leaves_to_search=leaves_to_search, training_sample_size=num_points)\n",
    "            .score_brute_force()\n",
    "            .reorder(min(5, num_points))\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "\n",
    "    import re\n",
    "\n",
    "    def search(self, query, top_k_topics=3, top_k_sents=3):\n",
    "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
    "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
    "\n",
    "        results = []\n",
    "        prefix_pattern = r\"^the concept '.*?' appears in (the following )?context:\\s*\"\n",
    "\n",
    "        for i, idx in enumerate(neighbors):\n",
    "            meta = self.topic_metadata[idx]\n",
    "            topic_score = float(scores[i])\n",
    "\n",
    "            # Deduplicate sentences\n",
    "            seen = set()\n",
    "            cleaned_sentences = []\n",
    "            cleaned_embeddings = []\n",
    "\n",
    "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
    "            # Apply regex to remove beginning prefix\n",
    "                cleaned = re.sub(prefix_pattern, \"\", sent, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        # No duplicates\n",
    "                if cleaned not in seen:\n",
    "                    seen.add(cleaned)\n",
    "                    cleaned_sentences.append(cleaned)\n",
    "                    cleaned_embeddings.append(emb)\n",
    "\n",
    "            if not cleaned_sentences:\n",
    "                continue\n",
    "\n",
    "            emb_array = np.array(cleaned_embeddings)\n",
    "            sims = np.dot(emb_array / np.linalg.norm(emb_array, axis=1, keepdims=True), query_emb)\n",
    "            top_ids = sims.argsort()[::-1][:top_k_sents]\n",
    "\n",
    "            top_sents = [(cleaned_sentences[j], float(sims[j])) for j in top_ids]\n",
    "            results.append({\n",
    "            \"topic_id\": meta[\"topic_id\"],\n",
    "            \"topic_score\": topic_score,\n",
    "            \"entities\": meta[\"entities\"],\n",
    "            \"sentences\": top_sents,\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === METRICS ===\n",
    "def compute_bertopic_coherence(topic_model, topic_metadata, topk=15):\n",
    "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
    "    word_lists = [[word for word, _ in topic] for topic in topics]\n",
    "\n",
    "    texts = []\n",
    "    for meta in topic_metadata:\n",
    "        for s in meta[\"sentences\"]:\n",
    "            tokens = clean_text(s).split()\n",
    "            texts.append(tokens)\n",
    "\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "\n",
    "    cm = CoherenceModel(\n",
    "        topics=word_lists,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\"\n",
    "    )\n",
    "    return cm.get_coherence()\n",
    "\n",
    "\n",
    "def compute_topic_diversity(topic_model, topic_metadata, topk=10):\n",
    "    topic_words = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
    "    unique_words = set(word for topic in topic_words for word, _ in topic)\n",
    "    return len(unique_words) / (len(topic_words) * topk)\n",
    "\n",
    "\n",
    "def compute_silhouette_score_custom(topic_metadata):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    for meta in topic_metadata:\n",
    "        emb = meta[\"sentence_embeddings\"]\n",
    "        if len(emb) < 2:  # skip small clusters\n",
    "            continue\n",
    "        all_embeddings.extend(emb)\n",
    "        all_labels.extend([meta[\"topic_id\"]] * len(emb))\n",
    "\n",
    "    if len(all_embeddings) < 3:\n",
    "        return None\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return silhouette_score(all_embeddings, all_labels, metric=\"cosine\")\n",
    "\n",
    "\n",
    "# === DATASET & INITIALIZATION ===\n",
    "allergy_dataset = {\n",
    "  \"chunks\": [\n",
    "  \"Mr. James H., a 79-year-old male with a long-standing history of cardiovascular and metabolic diseases, was brought to the emergency department due to acute confusion and generalized weakness.\",\n",
    "  \"According to his daughter, he had wandered outside disoriented and was unable to identify family members or recall events from the previous day.\",\n",
    "  \"He has known medical conditions including hypertension, heart failure with reduced ejection fraction, insulin-dependent diabetes mellitus, stage 4 chronic kidney disease, and major depressive disorder.\",\n",
    "  \"His medication regimen includes daily doses of lisinopril, furosemide, carvedilol, insulin glargine, sertraline, and donepezil.\",\n",
    "  \"In the past 24 hours, the patient experienced decreased appetite, an episode of vomiting, and two instances of urinary incontinence.\",\n",
    "  \"Vital signs upon arrival included a blood pressure of 98/56 mmHg, heart rate of 112 beats per minute (irregularly irregular), respiratory rate of 20, oxygen saturation of 93% on room air, and a temperature of 37.6°C.\",\n",
    "  \"Physical examination revealed dry mucous membranes, poor skin turgor, moderate lower limb pitting edema, and delayed capillary refill.\",\n",
    "  \"Auscultation of the lungs revealed bilateral basal crackles, and cardiac exam confirmed an irregularly irregular heartbeat without murmurs.\",\n",
    "  \"Neurological examination showed fluctuating attention span but no signs of focal deficits or lateralizing neurological signs.\",\n",
    "  \"Initial lab studies demonstrated an elevated blood glucose of 421 mg/dL, serum sodium of 129 mmol/L, potassium at 5.7 mmol/L, and creatinine at 2.9 mg/dL.\",\n",
    "  \"Serum BUN was elevated at 59 mg/dL and the patient’s anion gap was calculated to be 19, consistent with an anion-gap metabolic acidosis.\",\n",
    "  \"Urinalysis revealed glucosuria and ketonuria without signs of infection, and serum ketones were modestly elevated.\",\n",
    "  \"His HbA1c on record from two months ago was 8.1%, confirming chronic poor glycemic control.\",\n",
    "  \"An ECG showed atrial fibrillation with rapid ventricular response but no acute ischemic changes.\",\n",
    "  \"Chest radiograph revealed cardiomegaly and pulmonary vascular congestion with mild bilateral pleural effusions.\",\n",
    "  \"CT head without contrast was negative for acute infarct, hemorrhage, or mass effect, but showed chronic microvascular changes.\",\n",
    "  \"Given the presentation, he was admitted to the medical ward for acute hyperosmolar hyperglycemic state (HHS) and acute on chronic kidney injury.\",\n",
    "  \"A diagnosis of acute delirium, likely secondary to metabolic derangements, volume depletion, and possible infection, was made.\",\n",
    "  \"He was started on intravenous normal saline, correctional insulin, and telemetry monitoring.\",\n",
    "  \"Furosemide was temporarily held due to volume depletion, and electrolytes were repleted cautiously under nephrology guidance.\",\n",
    "  \"Blood cultures, urine cultures, and chest x-ray were obtained to rule out infection as a potential delirium trigger.\",\n",
    "  \"Empiric antibiotics (ceftriaxone and azithromycin) were initiated pending culture data due to concern for possible aspiration pneumonia.\",\n",
    "  \"On day two, the patient’s mental status began to improve with the resolution of hyperglycemia and normalization of serum osmolarity.\",\n",
    "  \"Repeat labs showed trending down of BUN and creatinine, with sodium rising to 134 and potassium corrected to 4.5 mmol/L.\",\n",
    "  \"He remained in atrial fibrillation and required continuation of beta-blocker therapy to manage ventricular rate.\",\n",
    "  \"Apixaban was continued upon nephrology clearance given acceptable bleeding risk and stable renal function.\",\n",
    "  \"He was evaluated by geriatrics for worsening cognitive decline and safety evaluation related to home discharge.\",\n",
    "  \"PT/OT performed a bedside mobility assessment showing weakness, unsteadiness, and need for moderate assistance with transfers.\",\n",
    "  \"Case management consulted social work regarding home safety, fall prevention, and caregiver support.\",\n",
    "  \"His hospital stay was complicated by mild hypoglycemia on hospital day 3, prompting insulin dose adjustments.\",\n",
    "  \"Nutritional support was consulted to optimize diabetic-friendly, renal-adjusted diet appropriate for age and mobility.\",\n",
    "  \"His depression management was reviewed with psychiatry, and sertraline was continued at 100 mg/day with no suggestion for dose change.\",\n",
    "  \"A Montreal Cognitive Assessment (MoCA) was done revealing a score of 19/30, indicating significant mild cognitive impairment.\",\n",
    "  \"Audiology was recommended due to hearing difficulty interfering with care discussions.\",\n",
    "  \"Oral exam noted poor dentition; dental evaluation was recommended for follow-up to address suspected pain and poor appetite.\",\n",
    "  \"After 6 days, the patient was clinically improved, mentally oriented, and ambulatory with the help of physical therapy.\",\n",
    "  \"Cardiac and renal parameters stabilized sufficiently to permit safe discharge planning.\",\n",
    "  \"The final hospital diagnosis included hyperosmolar hyperglycemic state, volume depletion, acute-on-chronic kidney injury, atrial fibrillation with RVR, and acute delirium.\",\n",
    "  \"He was discharged on a simplified diabetic regimen including basal insulin and correctional sliding scale doses only.\",\n",
    "  \"Apixaban, carvedilol, donepezil, and sertraline were continued with no changes.\",\n",
    "  \"Discharge medication reconciliation included temporary hold of furosemide with plan for outpatient reassessment after fluid status recovery.\",\n",
    "  \"Caregiver role was assumed by daughter who had durable power of attorney and assisted with all home-based needs.\",\n",
    "  \"Written instructions and red flags for hyperglycemia, dizziness, and recurrent confusion were provided.\",\n",
    "  \"A follow-up with his primary care physician, nephrologist, and endocrinologist were scheduled within one and two weeks respectively.\",\n",
    "  \"Home health nursing was arranged to provide medication support and glucose monitoring.\",\n",
    "  \"Nutritionist and physical therapy were ordered for continued improvement in diet and mobility.\",\n",
    "  \"Advanced care planning was briefly discussed including code status, proxy, and end-of-life preferences.\",\n",
    "  \"He is currently listed as full code but family is open to further discussion at next provider visit.\",\n",
    "  \"Patient was grateful for hospital care and expressed motivation to remain active and well at home.\",\n",
    "  \"The overall prognosis remains guarded due to progressive cognitive decline and limited renal reserve.\",\n",
    "  \"Close monitoring for new signs of decompensation or medication nonadherence was advised.\",\n",
    "  \"Pulmonology follow-up was discussed due to prior mild restrictive spirometry suggestive of early interstitial lung disease.\",\n",
    "  \"Family history reveals mother died of complications from dementia and father from ischemic stroke.\",\n",
    "  \"No reported use of tobacco, alcohol, or recreational drugs throughout his life.\",\n",
    "  \"Lives in a single-story home with grab bars and minimal clutter, although risks for falls still persist.\",\n",
    "  \"Wears eyeglasses but rarely uses his hearing aids, sometimes leading to miscommunication or withdrawal.\",\n",
    "  \"History of previous admission 1 year ago for pneumonia requiring IV antibiotics and 6-day hospitalization.\",\n",
    "  \"Documentation from that admission revealed transient delirium and impaired oral intake similar to current episode.\",\n",
    "  \"Goals-of-care conversations were initiated during this admission but deferred for primary care setting follow-up.\",\n",
    "  \"Social isolation remains a concern, especially since his wife passed away 3 years ago.\",\n",
    "  \"Patient receives Meals on Wheels but misses many meal deliveries due to lack of reliable caregiver at times.\",\n",
    "  \"Transportation to medical appointments is provided by his daughter, who balances full-time work responsibilities.\",\n",
    "  \"No current enrollment in adult day health programs; options discussed with case management on discharge.\",\n",
    "  \"Insurance covers home nursing and outpatient labs but does not cover custodial care.\",\n",
    "  \"Patient was educated about Medicare Advantage benefits and encouraged to review covered services with the plan coordinator.\",\n",
    "  \"He was also reminded of the importance of daily glucose checks and hydration in summer months.\",\n",
    "  \"Foot exam demonstrated mild calluses and intact sensation; he denies new ulcers or foot injuries.\",\n",
    "  \"Vaccination status confirmed: received influenza and COVID vaccines last fall, but is due for pneumococcal booster.\",\n",
    "  \"Dentition issues may be contributing to decreased intake; dental clinic referral was sent through EHR.\",\n",
    "  \"Assistive device for walking was provided (four-point cane) after physical therapy evaluation.\",\n",
    "  \"Contact dermatitis on legs due to prolonged pressure and incontinence was treated with barrier cream.\",\n",
    "  \"Skin care and bathing guidance were reviewed with family nursing staff prior to discharge.\",\n",
    "  \"Patient verbalized understanding of all discharge instructions with support from daughter.\",\n",
    "  \"Hospital team closed chart after discussing active problems list, response to therapy, and continued plan.\",\n",
    "  \"Patient left the hospital in a wheelchair, accompanied by family, and appeared in good spirits.\",\n",
    "  \"The full discharge plan was documented and faxed to his primary provider for continuity of care.\",\n",
    "  \"Medication reconciliation showed no potential drug interactions or allergy mismatches.\",\n",
    "  \"He was warned against use of NSAIDs due to underlying CKD and risk of acute worsening.\",\n",
    "  \"Hydration goals of at least 1.5 liters per day were set; urination logs and symptom review were encouraged.\",\n",
    "  \"Emergency instructions included what to do in case of unresponsiveness, low blood glucose, sudden confusion, or chest pain.\",\n",
    "  \"Digital blood glucose monitor was reviewed at bedside; daughter demonstrated appropriate calibration and use.\",\n",
    "  \"All prescriptions were sent electronically to their local pharmacy located eight blocks from their home.\",\n",
    "  \"Patient prefers morning appointments due to increased alertness and energy early in the day.\",\n",
    "  \"A follow-up MoCA test was recommended in 3–6 months to assess cognitive trajectory.\",\n",
    "  \"Updated advance directives were placed in the chart and a copy was given to the daughter.\",\n",
    "  \"Fall prevention strategies were emphasized including appropriate lighting, footwear, and scheduled ambulation.\",\n",
    "  \"Use of automatic pill organizers was encouraged to improve adherence across complex medication schedules.\",\n",
    "  \"Daily weights will be tracked at home to monitor for unexpected fluid retention or heart failure.\",\n",
    "  \"Serum creatinine will be rechecked in one week given borderline rise during admission.\",\n",
    "  \"A nephrology note was sent to alert about potential need for long-term planning if GFR continues to decline.\",\n",
    "  \"Patient qualifies for shared savings Medicare model and was assigned a care coordinator temporarily.\",\n",
    "  \"Patient support group information was handed out, including resources for caregivers.\",\n",
    "  \"He is open to exploring telehealth check-ins for medication titration and early symptom triage.\",\n",
    "  \"Daughter confirmed she has portal access to review labs and visit summaries on his behalf.\",\n",
    "  \"Patient and daughter expressed appreciation for the hospital care coordination team.\",\n",
    "  \"Case closed with summary of diagnosis, medications, specialists involved, and plan for 30-day transitional care.\",\n",
    "  \"Status post discharge: stable, safe for home, alert and oriented with supervision.\"\n",
    "]\n",
    "\n",
    ",\n",
    "\"entities\":[\n",
    "  [\"confusion\", \"weakness\", \"cardiovascular\", \"metabolic\"],\n",
    "  [\"disorientation\", \"memory\"],\n",
    "  [\"hypertension\", \"failure\", \"diabetes\", \"kidney\", \"depression\"],\n",
    "  [\"lisinopril\", \"furosemide\", \"carvedilol\", \"insulin\", \"sertraline\", \"donepezil\"],\n",
    "  [\"appetite\", \"vomiting\", \"incontinence\"],\n",
    "  [\"pressure\", \"rate\", \"rhythm\", \"respiration\", \"saturation\", \"temperature\"],\n",
    "  [\"mucosa\", \"turgor\", \"edema\", \"refill\"],\n",
    "  [\"crackles\", \"heartbeat\", \"murmurs\"],\n",
    "  [\"attention\", \"deficits\"],\n",
    "  [\"glucose\", \"sodium\", \"potassium\", \"creatinine\"],\n",
    "  [\"bun\", \"acidosis\"],\n",
    "  [\"glucosuria\", \"ketonuria\", \"ketones\"],\n",
    "  [\"hba1c\", \"control\"],\n",
    "  [\"ecg\", \"fibrillation\", \"response\", \"ischemia\"],\n",
    "  [\"cardiomegaly\", \"congestion\", \"effusions\"],\n",
    "  [\"infarct\", \"hemorrhage\", \"microvascular\"],\n",
    "  [\"hyperglycemia\", \"injury\"],\n",
    "  [\"delirium\", \"derangements\", \"infection\"],\n",
    "  [\"saline\", \"insulin\", \"telemetry\"],\n",
    "  [\"furosemide\", \"depletion\", \"electrolytes\"],\n",
    "  [\"cultures\", \"infection\"],\n",
    "  [\"antibiotics\", \"ceftriaxone\", \"azithromycin\", \"pneumonia\"],\n",
    "  [\"status\", \"hyperglycemia\", \"osmolarity\"],\n",
    "  [\"bun\", \"creatinine\", \"sodium\", \"potassium\"],\n",
    "  [\"fibrillation\", \"rate\", \"blocker\"],\n",
    "  [\"apixaban\", \"function\", \"bleeding\"],\n",
    "  [\"geriatrics\", \"cognition\"],\n",
    "  [\"pt\", \"ot\", \"mobility\", \"weakness\", \"transfers\"],\n",
    "  [\"safety\", \"falls\"],\n",
    "  [\"hypoglycemia\", \"insulin\"],\n",
    "  [\"nutrition\", \"diet\"],\n",
    "  [\"depression\", \"psychiatry\", \"sertraline\"],\n",
    "  [\"moca\", \"impairment\"],\n",
    "  [\"audiology\", \"hearing\"],\n",
    "  [\"dentition\", \"pain\"],\n",
    "  [\"therapy\", \"ambulation\"],\n",
    "  [\"parameters\"],\n",
    "  [\"hyperglycemia\", \"depletion\", \"injury\", \"fibrillation\", \"delirium\"],\n",
    "  [\"regimen\", \"insulin\"],\n",
    "  [\"apixaban\", \"carvedilol\", \"donepezil\", \"sertraline\"],\n",
    "  [\"reconciliation\", \"furosemide\"],\n",
    "  [\"power\"],\n",
    "  [\"hyperglycemia\", \"dizziness\", \"confusion\"],\n",
    "  [\"nephrologist\", \"endocrinologist\"],\n",
    "  [\"nursing\", \"glucose\"],\n",
    "  [\"nutritionist\", \"therapy\"],\n",
    "  [\"planning\", \"status\", \"proxy\"],\n",
    "  [\"code\"],\n",
    "  [\"prognosis\", \"cognition\", \"reserve\"],\n",
    "  [\"monitoring\", \"decompensation\", \"adherence\"],\n",
    "  [\"pulmonology\", \"spirometry\", \"disease\"],\n",
    "  [\"dementia\", \"stroke\"],\n",
    "  [\"tobacco\", \"alcohol\", \"drugs\"],\n",
    "  [\"falls\"],\n",
    "  [\"hearing\"],\n",
    "  [\"pneumonia\", \"antibiotics\"],\n",
    "  [\"delirium\"],\n",
    "  [\"conversations\"],\n",
    "  [\"isolation\"],\n",
    "  [\"meals\"],\n",
    "  [\"transportation\"],\n",
    "  [\"enrollment\"],\n",
    "  [\"insurance\", \"nursing\", \"labs\"],\n",
    "  [\"medicare\"],\n",
    "  [\"glucose\", \"hydration\"],\n",
    "  [\"exam\", \"calluses\", \"ulcers\"],\n",
    "  [\"vaccination\", \"influenza\", \"covid\", \"booster\"],\n",
    "  [\"dentition\", \"referral\"],\n",
    "  [\"cane\"],\n",
    "  [\"dermatitis\", \"cream\"],\n",
    "  [\"skin\"],\n",
    "  [\"instructions\"],\n",
    "  [\"problems\", \"therapy\", \"plan\"],\n",
    "  [\"wheelchair\"],\n",
    "  [\"continuity\"],\n",
    "  [\"reconciliation\", \"interactions\", \"allergies\"],\n",
    "  [\"nsaids\"],\n",
    "  [\"hydration\", \"urination\", \"symptoms\"],\n",
    "  [\"instructions\", \"glucose\", \"confusion\", \"pain\"],\n",
    "  [\"monitor\", \"calibration\"],\n",
    "  [\"prescriptions\", \"pharmacy\"],\n",
    "  [\"appointments\", \"alertness\", \"energy\"],\n",
    "  [\"moca\"],\n",
    "  [\"directives\"],\n",
    "  [\"prevention\", \"lighting\", \"footwear\", \"ambulation\"],\n",
    "  [\"organizer\", \"adherence\"],\n",
    "  [\"weight\", \"retention\", \"failure\"],\n",
    "  [\"creatinine\"],\n",
    "  [\"nephrology\", \"gfr\"],\n",
    "  [\"medicare\", \"coordinator\"],\n",
    "  [\"group\", \"caregivers\"],\n",
    "  [\"telehealth\", \"titration\", \"triage\"],\n",
    "  [\"portal\", \"labs\", \"summaries\"],\n",
    "  [\"coordination\"],\n",
    "  [\"diagnosis\", \"medications\", \"specialists\", \"care\"],\n",
    "  [\"discharge\", \"supervision\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# === HYPERPARAMETER GRID SEARCH ===\n",
    "def grid_search_best_params(chunks, entities_per_chunk, umap_param_grid, hdbscan_param_grid):\n",
    "    best_score = -np.inf\n",
    "    best_umap = None\n",
    "    best_hdbscan = None\n",
    "    best_metrics = None\n",
    "\n",
    "    combinations = list(product(umap_param_grid[\"n_neighbors\"],\n",
    "                                umap_param_grid[\"n_components\"],\n",
    "                                umap_param_grid[\"min_dist\"],\n",
    "                                hdbscan_param_grid[\"min_cluster_size\"],\n",
    "                                hdbscan_param_grid[\"min_samples\"]))\n",
    "\n",
    "    logger.info(f\"Starting grid search over {len(combinations)} combinations...\")\n",
    "\n",
    "    for (n_neighbors, n_components, min_dist, min_cluster_size, min_samples) in combinations:\n",
    "        umap_params = {\n",
    "            \"n_neighbors\": n_neighbors,\n",
    "            \"n_components\": n_components,\n",
    "            \"min_dist\": min_dist,\n",
    "            \"metric\": \"cosine\"\n",
    "        }\n",
    "        hdbscan_params = {\n",
    "            \"min_cluster_size\": min_cluster_size,\n",
    "            \"min_samples\": min_samples,\n",
    "            \"metric\": \"euclidean\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            model = AllergyTopicSearcher(chunks, entities_per_chunk, umap_params, hdbscan_params)\n",
    "            coherence = compute_bertopic_coherence(model.topic_model, model.topic_metadata)\n",
    "            diversity = compute_topic_diversity(model.topic_model, model.topic_metadata)\n",
    "            silhouette = compute_silhouette_score_custom(model.topic_metadata) or 0.0\n",
    "\n",
    "            # Composite scoring (tweak weights if desired)\n",
    "            score = coherence + 0.5 * diversity + 0.5 * silhouette\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            logger.info(\n",
    "                f\" Tried UMAP: {umap_params}, HDBSCAN: {hdbscan_params} \"\n",
    "                f\"=> Score: {score:.4f} (Coherence: {coherence:.4f}, Diversity: {diversity:.4f}, Silhouette: {silhouette:.4f}) - Time: {elapsed:.2f}s\"\n",
    "            )\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_umap = umap_params\n",
    "                best_hdbscan = hdbscan_params\n",
    "                best_metrics = (coherence, diversity, silhouette)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping combination due to error: {e}\")\n",
    "\n",
    "    return best_umap, best_hdbscan, best_metrics\n",
    "# Define parameter grids\n",
    "umap_grid = {\n",
    "    \"n_neighbors\": [5, 10],\n",
    "    \"n_components\": [3, 5],\n",
    "    \"min_dist\": [0.1, 0.25]\n",
    "}\n",
    "hdbscan_grid = {\n",
    "    \"min_cluster_size\": [2, 5],\n",
    "    \"min_samples\": [1, 3]\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "best_umap, best_hdbscan, best_metrics = grid_search_best_params(\n",
    "    allergy_dataset[\"chunks\"],\n",
    "    allergy_dataset[\"entities\"],\n",
    "    umap_param_grid=umap_grid,\n",
    "    hdbscan_param_grid=hdbscan_grid\n",
    ")\n",
    "\n",
    "print(\"\\n=== Best Hyperparameters Found ===\")\n",
    "print(f\"Best UMAP: {best_umap}\")\n",
    "print(f\"Best HDBSCAN: {best_hdbscan}\")\n",
    "print(f\"Metrics => Coherence: {best_metrics[0]:.4f}, Diversity: {best_metrics[1]:.4f}, Silhouette: {best_metrics[2]:.4f}\")\n",
    "\n",
    "\n",
    "searcher = AllergyTopicSearcher(\n",
    "    chunks=allergy_dataset[\"chunks\"],\n",
    "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
    "    umap_params=best_umap,\n",
    "    hdbscan_params=best_hdbscan,\n",
    "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    ")\n",
    "print(\"Model ready for querying.\")\n",
    "# print(\"\\n=== Generated Topics and Entities ===\")\n",
    "# for meta in searcher.topic_metadata:\n",
    "#     topic_id = meta[\"topic_id\"]\n",
    "#     entities = \", \".join(meta[\"entities\"])\n",
    "#     print(f\"Topic ID: {topic_id} — Entities: {entities}\")\n",
    "\n",
    "# === METRICS ===\n",
    "coherence = compute_bertopic_coherence(searcher.topic_model, searcher.topic_metadata, topk=15)\n",
    "diversity = compute_topic_diversity(searcher.topic_model, searcher.topic_metadata, topk=10)\n",
    "sil_score = compute_silhouette_score_custom(searcher.topic_metadata)\n",
    "\n",
    "print(\"\\n=== Topic Quality Metrics ===\")\n",
    "print(f\"Coherence Score (c_v): {coherence:.4f}\")\n",
    "print(f\"Topic Diversity: {diversity:.4f}\")\n",
    "if sil_score is not None:\n",
    "    print(f\" Silhouette Score: {sil_score:.4f}\")\n",
    "else:\n",
    "    print(\"Silhouette Score: Not applicable.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "DN-Pq9qEKiet",
    "outputId": "453a1284-0050-4e1c-cc4e-e47d3f4571fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best Hyperparameters Found ===\n",
      "Best UMAP: {'n_neighbors': 5, 'n_components': 5, 'min_dist': 0.1, 'metric': 'cosine'}\n",
      "Best HDBSCAN: {'min_cluster_size': 2, 'min_samples': 1, 'metric': 'euclidean'}\n",
      "Metrics => Coherence: 0.7154, Diversity: 0.4153, Silhouette: 0.7765\n",
      "Model ready for querying.\n",
      "\n",
      "=== Topic Quality Metrics ===\n",
      "Coherence Score (c_v): 0.7154\n",
      "Topic Diversity: 0.4153\n",
      " Silhouette Score: 0.7765\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS & SETUP ===\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "import scann\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics import silhouette_score, precision_recall_fscore_support\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === SEED FIXING ===\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed_all(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "nltk.download(\"punkt\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# === CLEANING & CONTEXT EXTRACTION (IMPROVED) ===\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):\n",
    "    entity_context_pairs = []\n",
    "    for idx, ents in enumerate(entities_per_chunk):\n",
    "        chunk = clean_text(chunks[idx])\n",
    "        sentences = sent_tokenize(chunk)\n",
    "        for ent in ents:\n",
    "            ent_lower = ent.lower()\n",
    "            matched = False\n",
    "            for i, sent in enumerate(sentences):\n",
    "                if ent_lower in sent:\n",
    "                    context = \" \".join(sentences[max(0, i - 1): i + 2]) if use_multi_sentence else sent.strip()\n",
    "                    enriched = f\"The concept '{ent_lower}' appears in the following context: {context}\"\n",
    "                    entity_context_pairs.append((ent_lower, enriched.strip()))\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                fallback = f\"The concept '{ent_lower}' appears in the following context: {chunk}\"\n",
    "                entity_context_pairs.append((ent_lower, fallback.strip()))\n",
    "    return entity_context_pairs\n",
    "\n",
    "\n",
    "# === TOPIC SEARCHER CLASS (WITH DEDUPLICATION, NOISE FILTERING) ===\n",
    "class AllergyTopicSearcher:\n",
    "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params,\n",
    "                 model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"):\n",
    "        self.chunks = chunks\n",
    "        self.entities_per_chunk = entities_per_chunk\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "        self.umap_params = umap_params\n",
    "        self.hdbscan_params = hdbscan_params\n",
    "\n",
    "        self.topic_model = None\n",
    "        self.topic_metadata = []\n",
    "        self.topic_embeddings = None\n",
    "        self.searcher = None\n",
    "\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        entity_context_pairs = extract_entity_contexts(self.chunks, self.entities_per_chunk)\n",
    "        contextual_texts = [ctx for _, ctx in entity_context_pairs]\n",
    "        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=False)\n",
    "\n",
    "        umap_model = UMAP(**self.umap_params, random_state=SEED)\n",
    "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
    "\n",
    "        self.topic_model = BERTopic(\n",
    "            embedding_model=self.embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            representation_model=KeyBERTInspired(),\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
    "\n",
    "        topic_to_contexts = defaultdict(list)\n",
    "        topic_to_entities = defaultdict(set)\n",
    "        topic_to_embeddings = defaultdict(list)\n",
    "\n",
    "        for i, topic in enumerate(topics):\n",
    "            if topic == -1:\n",
    "                continue  # Skip noisy topics\n",
    "            ent, ctx = entity_context_pairs[i]\n",
    "            topic_to_contexts[topic].append(ctx)\n",
    "            topic_to_entities[topic].add(ent)\n",
    "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
    "\n",
    "        topic_embeddings = []\n",
    "        topic_metadata = []\n",
    "\n",
    "        for topic_id in topic_to_contexts:\n",
    "            emb = topic_to_embeddings[topic_id]\n",
    "            centroid = np.mean(emb, axis=0)\n",
    "            centroid /= np.linalg.norm(centroid) + 1e-10\n",
    "            topic_embeddings.append(centroid)\n",
    "            topic_metadata.append({\n",
    "                \"topic_id\": topic_id,\n",
    "                \"entities\": list(topic_to_entities[topic_id]),\n",
    "                \"sentences\": topic_to_contexts[topic_id],\n",
    "                \"sentence_embeddings\": np.array(emb)\n",
    "            })\n",
    "\n",
    "        # === OPTIONAL: Merge semantically similar topics (cosine sim > 0.95)\n",
    "        deduped_metadata = []\n",
    "        used = set()\n",
    "\n",
    "        for i, emb_i in enumerate(topic_embeddings):\n",
    "            if i in used:\n",
    "                continue\n",
    "            group = [i]\n",
    "            sim_scores = cosine_similarity([emb_i], topic_embeddings)[0]\n",
    "            for j in range(i + 1, len(sim_scores)):\n",
    "                if sim_scores[j] > 0.95:\n",
    "                    group.append(j)\n",
    "                    used.add(j)\n",
    "\n",
    "            merged = {\n",
    "                \"topic_id\": i,\n",
    "                \"sentences\": [],\n",
    "                \"entities\": [],\n",
    "                \"sentence_embeddings\": []\n",
    "            }\n",
    "            for g in group:\n",
    "                merged[\"sentences\"] += topic_metadata[g][\"sentences\"]\n",
    "                merged[\"entities\"] += topic_metadata[g][\"entities\"]\n",
    "                merged[\"sentence_embeddings\"] += list(topic_metadata[g][\"sentence_embeddings\"])\n",
    "\n",
    "            merged[\"sentence_embeddings\"] = np.array(merged[\"sentence_embeddings\"])\n",
    "            merged[\"entities\"] = list(set(merged[\"entities\"]))\n",
    "            deduped_metadata.append(merged)\n",
    "\n",
    "        self.topic_metadata = deduped_metadata\n",
    "        self.topic_embeddings = np.array([\n",
    "            np.mean(m[\"sentence_embeddings\"], axis=0) /\n",
    "            (np.linalg.norm(np.mean(m[\"sentence_embeddings\"], axis=0)) + 1e-10)\n",
    "            for m in deduped_metadata\n",
    "        ])\n",
    "\n",
    "        num_points = len(self.topic_embeddings)\n",
    "        num_leaves = min(10, num_points)\n",
    "        leaves_to_search = min(5, num_leaves)\n",
    "\n",
    "        if num_points < 3:\n",
    "            raise ValueError(\"Not enough topics to build a search index.\")\n",
    "\n",
    "        self.searcher = (\n",
    "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
    "            .tree(num_leaves=num_leaves, num_leaves_to_search=leaves_to_search, training_sample_size=num_points)\n",
    "            .score_brute_force()\n",
    "            .reorder(min(5, num_points))\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "\n",
    "    import re\n",
    "\n",
    "    def search(self, query, top_k_topics=3, top_k_sents=3):\n",
    "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
    "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
    "\n",
    "        results = []\n",
    "        prefix_pattern = r\"^the concept '.*?' appears in (the following )?context:\\s*\"\n",
    "\n",
    "        for i, idx in enumerate(neighbors):\n",
    "            meta = self.topic_metadata[idx]\n",
    "            topic_score = float(scores[i])\n",
    "\n",
    "            # Deduplicate sentences\n",
    "            seen = set()\n",
    "            cleaned_sentences = []\n",
    "            cleaned_embeddings = []\n",
    "\n",
    "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
    "            # Apply regex to remove beginning prefix\n",
    "                cleaned = re.sub(prefix_pattern, \"\", sent, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        # No duplicates\n",
    "                if cleaned not in seen:\n",
    "                    seen.add(cleaned)\n",
    "                    cleaned_sentences.append(cleaned)\n",
    "                    cleaned_embeddings.append(emb)\n",
    "\n",
    "            if not cleaned_sentences:\n",
    "                continue\n",
    "\n",
    "            emb_array = np.array(cleaned_embeddings)\n",
    "            sims = np.dot(emb_array / np.linalg.norm(emb_array, axis=1, keepdims=True), query_emb)\n",
    "            top_ids = sims.argsort()[::-1][:top_k_sents]\n",
    "\n",
    "            top_sents = [(cleaned_sentences[j], float(sims[j])) for j in top_ids]\n",
    "            results.append({\n",
    "            \"topic_id\": meta[\"topic_id\"],\n",
    "            \"topic_score\": topic_score,\n",
    "            \"entities\": meta[\"entities\"],\n",
    "            \"sentences\": top_sents,\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === METRICS ===\n",
    "def compute_bertopic_coherence(topic_model, topic_metadata, topk=15):\n",
    "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
    "    word_lists = [[word for word, _ in topic] for topic in topics]\n",
    "\n",
    "    texts = []\n",
    "    for meta in topic_metadata:\n",
    "        for s in meta[\"sentences\"]:\n",
    "            tokens = clean_text(s).split()\n",
    "            texts.append(tokens)\n",
    "\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "\n",
    "    cm = CoherenceModel(\n",
    "        topics=word_lists,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\"\n",
    "    )\n",
    "    return cm.get_coherence()\n",
    "\n",
    "\n",
    "def compute_topic_diversity(topic_model, topic_metadata, topk=10):\n",
    "    topic_words = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
    "    unique_words = set(word for topic in topic_words for word, _ in topic)\n",
    "    return len(unique_words) / (len(topic_words) * topk)\n",
    "\n",
    "\n",
    "def compute_silhouette_score_custom(topic_metadata):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    for meta in topic_metadata:\n",
    "        emb = meta[\"sentence_embeddings\"]\n",
    "        if len(emb) < 2:  # skip small clusters\n",
    "            continue\n",
    "        all_embeddings.extend(emb)\n",
    "        all_labels.extend([meta[\"topic_id\"]] * len(emb))\n",
    "\n",
    "    if len(all_embeddings) < 3:\n",
    "        return None\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return silhouette_score(all_embeddings, all_labels, metric=\"cosine\")\n",
    "\n",
    "\n",
    "# === DATASET & INITIALIZATION ===\n",
    "allergy_dataset = {\n",
    "  \"chunks\": [\"A 58-year-old female with a history of invasive ductal carcinoma of the breast presented with unintentional weight loss and malaise over two months.\",\n",
    "  \"She reported new onset of bone pain localized to the lower back and hips, progressively worsening in intensity.\",\n",
    "  \"There was a history of intermittent fever and night sweats but no reported cough or hemoptysis.\",\n",
    "  \"Her past medical history includes Stage II breast cancer treated with lumpectomy and adjuvant chemotherapy three years ago.\",\n",
    "  \"She completed radiation therapy 18 months prior and was on hormonal therapy with tamoxifen.\",\n",
    "  \"Family history revealed a mother diagnosed with ovarian cancer at age 62.\",\n",
    "  \"Physical examination showed pallor, mild lymphadenopathy in the supraclavicular region, and tenderness over the lumbar spine.\",\n",
    "  \"Vital signs were stable with a low-grade fever of 37.8°C documented.\",\n",
    "  \"Neurological examination was non-focal with intact motor and sensory function.\",\n",
    "  \"Skin inspection revealed scattered petechiae over the lower extremities.\",\n",
    "  \"Initial laboratory tests showed anemia with a hemoglobin of 9.6 g/dL and elevated alkaline phosphatase.\",\n",
    "  \"Serum calcium was elevated at 11.2 mg/dL with normal renal function.\",\n",
    "  \"Liver enzymes revealed mildly elevated transaminases.\",\n",
    "  \"Tumor markers demonstrated elevated CA 15-3 and carcinoembryonic antigen (CEA).\",\n",
    "  \"Bone marrow biopsy was scheduled given pancytopenia and bone pain.\",\n",
    "  \"Plain radiographs of the lumbar spine displayed lytic lesions involving vertebral bodies L3 to L5.\",\n",
    "  \"A whole-body bone scan indicated multiple areas of increased uptake consistent with skeletal metastases.\",\n",
    "  \"Contrast-enhanced CT scan of the chest, abdomen, and pelvis showed multiple hypodense liver lesions.\",\n",
    "  \"No pulmonary nodules or mediastinal lymphadenopathy were identified.\",\n",
    "  \"Ultrasound-guided liver biopsy confirmed metastatic adenocarcinoma consistent with a breast primary.\",\n",
    "  \"Immunohistochemistry revealed estrogen and progesterone receptor positivity with HER2 negativity.\",\n",
    "  \"The oncology team initiated treatment with a combination of bisphosphonates and aromatase inhibitors.\",\n",
    "  \"Supportive care included analgesics for bone pain and intravenous hydration for hypercalcemia.\",\n",
    "  \"She was referred to pain management and physiotherapy services early in the treatment course.\",\n",
    "  \"The patient was counseled on prognosis, treatment goals, and potential side effects, emphasizing quality of life.\",\n",
    "  \"Follow-up imaging after three cycles of systemic therapy showed partial response with reduction in liver lesions.\",\n",
    "  \"Her hemoglobin levels improved gradually, and bone pain was better controlled after bisphosphonate therapy.\",\n",
    "  \"Repeat tumor markers demonstrated a downward trend correlating with clinical improvement.\",\n",
    "  \"She experienced mild nausea and fatigue as common adverse effects of aromatase inhibitors.\",\n",
    "  \"Endocrinology was consulted to monitor bone mineral density and calcium homeostasis.\",\n",
    "  \"Psychological support was provided as the patient reported anxiety and mood changes linked to diagnosis.\",\n",
    "  \"Genetic counseling was offered due to family history and implications for targeted therapies and relatives.\",\n",
    "  \"The patient was screened for vitamin D deficiency and started supplementation accordingly.\",\n",
    "  \"Regular liver function tests and hematologic parameters were closely monitored throughout treatment.\",\n",
    "  \"No evidence of brain metastases was found on screening MRI performed due to headaches.\",\n",
    "  \"The multidisciplinary tumor board reviewed her case regularly to optimize management plans.\",\n",
    "  \"Physical rehabilitation focused on maintaining mobility and preventing skeletal complications.\",\n",
    "  \"She was advised on diet modifications to address nutritional deficits and maintain strength.\",\n",
    "  \"Symptoms of potential pathological fractures were carefully monitored, with orthopedics in consultation.\",\n",
    "  \"The patient's preferences strongly influenced decision-making, with goals centered on symptom control.\",\n",
    "  \"Anemia was managed conservatively with erythropoiesis-stimulating agents as hemoglobin fell below threshold.\",\n",
    "  \"The endocrinology team adjusted hormonal therapy due to emerging side effects including arthralgia.\",\n",
    "  \"Bone biopsies excluded secondary malignancies and confirmed disease progression status.\",\n",
    "  \"A plan for cyclic imaging surveillance was established to assess disease stability.\",\n",
    "  \"The patient was enrolled in a clinical trial investigating novel agents for metastatic breast cancer.\",\n",
    "  \"Treatment adherence was supported via nursing follow-ups and patient education materials.\",\n",
    "  \"The role of palliative care was discussed early with emphasis on holistic support.\",\n",
    "  \"Complications such as thrombocytopenia developed transiently, managed with dose adjustments.\",\n",
    "  \"Cardiotoxicity surveillance was instituted due to prior chemotherapy exposure, with echocardiograms scheduled.\",\n",
    "  \"The patient reported improved mood and coping strategies following psychiatric intervention.\",\n",
    "  \"End-of-life care planning was briefly addressed with patient and family input incorporated.\",\n",
    "  \"The patient participated actively in support groups facilitated by the oncology center.\",\n",
    "  \"Pain control was optimized using a multimodal analgesic regimen including opioids and NSAIDs.\",\n",
    "  \"Bone-modifying agents helped reduce skeletal related events and improve quality of life.\",\n",
    "  \"Serum biomarkers were periodically assessed as part of response monitoring.\",\n",
    "  \"The impact of hormonal therapy on bone health was managed with calcium and vitamin D supplements.\",\n",
    "  \"Anemia-related fatigue was addressed with physical activity modulation and energy conservation techniques.\",\n",
    "  \"The patient was closely observed for signs of disease progression or treatment resistance.\",\n",
    "  \"Interdisciplinary communication between oncology, nursing, rehabilitation, and social workers was continuous.\",\n",
    "  \"Access to financial and social support services was coordinated to assist with treatment costs.\",\n",
    "  \"The patient’s diary was used to track symptoms and side effects to inform ongoing care.\",\n",
    "  \"Reassessment of goals of care was planned regularly to align with changing clinical status.\",\n",
    "  \"International guidelines for metastatic breast cancer management were applied in treatment decisions.\",\n",
    "  \"The case exemplified challenges in balancing disease control with minimizing treatment toxicity.\",\n",
    "  \"The patient was encouraged to engage in meaningful activities and social interactions.\",\n",
    "  \"Long-term follow-up focused on symptom management, surveillance, and survivorship issues.\",\n",
    "  \"The clinical summary contributed to educational efforts for trainees in complex cancer care.\",\n",
    "  \"Transitions between inpatient, outpatient, and supportive community care were smoothly implemented.\",\n",
    "  \"Family meetings facilitated shared decision-making and education about prognosis and care options.\",\n",
    "  \"The clinical documentation included detailed medication reconciliation and allergy assessments.\",\n",
    "  \"Data captured support future research and quality improvement initiatives in metastatic breast cancer.\",\n",
    "  \"Final diagnoses included metastatic estrogen receptor positive breast cancer with skeletal and hepatic involvement.\",\n",
    "  \"The patient’s overall prognosis was communicated with sensitivity and clarity aligning care expectations.\"\n",
    "]\n",
    "\n",
    ",\n",
    "\"entities\":[\n",
    "  [\"female\", \"carcinoma\", \"breast\", \"weight\", \"malaise\"],\n",
    "  [\"bone\", \"pain\", \"back\", \"hips\"],\n",
    "  [\"fever\", \"sweats\", \"cough\", \"hemoptysis\"],\n",
    "  [\"cancer\", \"lumpectomy\", \"chemotherapy\"],\n",
    "  [\"radiation\", \"therapy\", \"tamoxifen\"],\n",
    "  [\"family\", \"ovarian\", \"cancer\"],\n",
    "  [\"pallor\", \"lymphadenopathy\", \"supraclavicular\", \"tenderness\", \"lumbar\", \"spine\"],\n",
    "  [\"vital\", \"fever\"],\n",
    "  [\"neurological\", \"motor\", \"sensory\"],\n",
    "  [\"skin\", \"petechiae\", \"extremities\"],\n",
    "  [\"anemia\", \"hemoglobin\", \"alkaline\", \"phosphatase\"],\n",
    "  [\"calcium\", \"renal\", \"function\"],\n",
    "  [\"liver\", \"enzymes\", \"transaminases\"],\n",
    "  [\"tumor\", \"markers\", \"ca15-3\", \"cea\"],\n",
    "  [\"bone\", \"marrow\", \"biopsy\", \"pancytopenia\"],\n",
    "  [\"radiographs\", \"vertebral\", \"lytic\", \"lesions\"],\n",
    "  [\"bone\", \"scan\", \"metastases\"],\n",
    "  [\"ct\", \"chest\", \"abdomen\", \"pelvis\", \"lesions\"],\n",
    "  [\"pulmonary\", \"nodules\", \"lymphadenopathy\"],\n",
    "  [\"biopsy\", \"adenocarcinoma\", \"breast\", \"primary\"],\n",
    "  [\"immunohistochemistry\", \"estrogen\", \"progesterone\", \"her2\"],\n",
    "  [\"bisphosphonates\", \"aromatase\", \"inhibitors\"],\n",
    "  [\"analgesics\", \"hypercalcemia\", \"hydration\"],\n",
    "  [\"pain\", \"management\", \"physiotherapy\"],\n",
    "  [\"counseling\", \"prognosis\", \"quality\", \"life\"],\n",
    "  [\"imaging\", \"therapy\", \"response\"],\n",
    "  [\"hemoglobin\", \"bisphosphonate\", \"therapy\"],\n",
    "  [\"tumor\", \"markers\", \"trend\"],\n",
    "  [\"nausea\", \"fatigue\", \"aromatase\"],\n",
    "  [\"endocrinology\", \"bone\", \"density\", \"calcium\"],\n",
    "  [\"psychological\", \"anxiety\", \"mood\"],\n",
    "  [\"genetic\", \"counseling\", \"therapy\"],\n",
    "  [\"vitamin\", \"d\", \"supplementation\"],\n",
    "  [\"liver\", \"function\", \"hematologic\"],\n",
    "  [\"brain\", \"metastases\", \"mri\", \"headaches\"],\n",
    "  [\"tumor\", \"board\", \"management\"],\n",
    "  [\"rehabilitation\", \"mobility\", \"complications\"],\n",
    "  [\"diet\", \"nutritional\", \"deficits\"],\n",
    "  [\"fractures\", \"orthopedics\"],\n",
    "  [\"patient\", \"goals\", \"symptom\", \"control\"],\n",
    "  [\"anemia\", \"erythropoiesis\"],\n",
    "  [\"side-effects\", \"arthralgia\", \"hormonal\"],\n",
    "  [\"bone\", \"biopsies\", \"progression\"],\n",
    "  [\"imaging\", \"surveillance\"],\n",
    "  [\"clinical\", \"trial\", \"breast\", \"cancer\"],\n",
    "  [\"adherence\", \"nursing\", \"education\"],\n",
    "  [\"palliative\", \"care\", \"support\"],\n",
    "  [\"thrombocytopenia\", \"dose\", \"adjustments\"],\n",
    "  [\"cardiotoxicity\", \"echocardiogram\", \"chemotherapy\"],\n",
    "  [\"psychiatric\", \"intervention\", \"mood\"],\n",
    "  [\"care\", \"planning\", \"family\"],\n",
    "  [\"support\", \"groups\", \"oncology\"],\n",
    "  [\"analgesic\", \"opioids\", \"nsaids\"],\n",
    "  [\"bone-modifying\", \"skeletal\", \"quality\"],\n",
    "  [\"biomarkers\", \"monitoring\"],\n",
    "  [\"hormonal\", \"therapy\", \"calcium\", \"vitamin\"],\n",
    "  [\"fatigue\", \"activity\", \"conservation\"],\n",
    "  [\"disease\", \"progression\", \"resistance\"],\n",
    "  [\"communication\", \"oncology\", \"rehabilitation\", \"social\"],\n",
    "  [\"financial\", \"support\", \"treatment\"],\n",
    "  [\"symptoms\", \"side-effects\", \"diary\"],\n",
    "  [\"goals\", \"care\", \"status\"],\n",
    "  [\"guidelines\", \"management\"],\n",
    "  [\"balance\", \"control\", \"toxicity\"],\n",
    "  [\"activities\", \"social\", \"interaction\"],\n",
    "  [\"survivorship\", \"monitoring\"],\n",
    "  [\"education\", \"trainees\", \"cancer\"],\n",
    "  [\"care\", \"transitions\", \"community\"],\n",
    "  [\"decision-making\", \"family\", \"prognosis\"],\n",
    "  [\"documentation\", \"medication\", \"allergy\"],\n",
    "  [\"research\", \"quality\", \"improvement\"],\n",
    "  [\"diagnosis\", \"breast\", \"cancer\", \"metastatic\", \"skeletal\", \"hepatic\"],\n",
    "  [\"prognosis\", \"communication\", \"care\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# === HYPERPARAMETER GRID SEARCH ===\n",
    "def grid_search_best_params(chunks, entities_per_chunk, umap_param_grid, hdbscan_param_grid):\n",
    "    best_score = -np.inf\n",
    "    best_umap = None\n",
    "    best_hdbscan = None\n",
    "    best_metrics = None\n",
    "\n",
    "    combinations = list(product(umap_param_grid[\"n_neighbors\"],\n",
    "                                umap_param_grid[\"n_components\"],\n",
    "                                umap_param_grid[\"min_dist\"],\n",
    "                                hdbscan_param_grid[\"min_cluster_size\"],\n",
    "                                hdbscan_param_grid[\"min_samples\"]))\n",
    "\n",
    "    logger.info(f\"Starting grid search over {len(combinations)} combinations...\")\n",
    "\n",
    "    for (n_neighbors, n_components, min_dist, min_cluster_size, min_samples) in combinations:\n",
    "        umap_params = {\n",
    "            \"n_neighbors\": n_neighbors,\n",
    "            \"n_components\": n_components,\n",
    "            \"min_dist\": min_dist,\n",
    "            \"metric\": \"cosine\"\n",
    "        }\n",
    "        hdbscan_params = {\n",
    "            \"min_cluster_size\": min_cluster_size,\n",
    "            \"min_samples\": min_samples,\n",
    "            \"metric\": \"euclidean\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            model = AllergyTopicSearcher(chunks, entities_per_chunk, umap_params, hdbscan_params)\n",
    "            coherence = compute_bertopic_coherence(model.topic_model, model.topic_metadata)\n",
    "            diversity = compute_topic_diversity(model.topic_model, model.topic_metadata)\n",
    "            silhouette = compute_silhouette_score_custom(model.topic_metadata) or 0.0\n",
    "\n",
    "            # Composite scoring (tweak weights if desired)\n",
    "            score = coherence + 0.5 * diversity + 0.5 * silhouette\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            logger.info(\n",
    "                f\" Tried UMAP: {umap_params}, HDBSCAN: {hdbscan_params} \"\n",
    "                f\"=> Score: {score:.4f} (Coherence: {coherence:.4f}, Diversity: {diversity:.4f}, Silhouette: {silhouette:.4f}) - Time: {elapsed:.2f}s\"\n",
    "            )\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_umap = umap_params\n",
    "                best_hdbscan = hdbscan_params\n",
    "                best_metrics = (coherence, diversity, silhouette)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping combination due to error: {e}\")\n",
    "\n",
    "    return best_umap, best_hdbscan, best_metrics\n",
    "# Define parameter grids\n",
    "umap_grid = {\n",
    "    \"n_neighbors\": [3, 5, 8],\n",
    "    \"n_components\": [3, 5],\n",
    "    \"min_dist\": [0.0, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "hdbscan_grid = {\n",
    "    \"min_cluster_size\": [2, 5],\n",
    "    \"min_samples\": [1, 3]\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "best_umap, best_hdbscan, best_metrics = grid_search_best_params(\n",
    "    allergy_dataset[\"chunks\"],\n",
    "    allergy_dataset[\"entities\"],\n",
    "    umap_param_grid=umap_grid,\n",
    "    hdbscan_param_grid=hdbscan_grid\n",
    ")\n",
    "\n",
    "print(\"\\n=== Best Hyperparameters Found ===\")\n",
    "print(f\"Best UMAP: {best_umap}\")\n",
    "print(f\"Best HDBSCAN: {best_hdbscan}\")\n",
    "print(f\"Metrics => Coherence: {best_metrics[0]:.4f}, Diversity: {best_metrics[1]:.4f}, Silhouette: {best_metrics[2]:.4f}\")\n",
    "\n",
    "\n",
    "searcher = AllergyTopicSearcher(\n",
    "    chunks=allergy_dataset[\"chunks\"],\n",
    "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
    "    umap_params=best_umap,\n",
    "    hdbscan_params=best_hdbscan,\n",
    "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    ")\n",
    "print(\"Model ready for querying.\")\n",
    "# print(\"\\n=== Generated Topics and Entities ===\")\n",
    "# for meta in searcher.topic_metadata:\n",
    "#     topic_id = meta[\"topic_id\"]\n",
    "#     entities = \", \".join(meta[\"entities\"])\n",
    "#     print(f\"Topic ID: {topic_id} — Entities: {entities}\")\n",
    "\n",
    "# === METRICS ===\n",
    "coherence = compute_bertopic_coherence(searcher.topic_model, searcher.topic_metadata, topk=15)\n",
    "diversity = compute_topic_diversity(searcher.topic_model, searcher.topic_metadata, topk=10)\n",
    "sil_score = compute_silhouette_score_custom(searcher.topic_metadata)\n",
    "\n",
    "print(\"\\n=== Topic Quality Metrics ===\")\n",
    "print(f\"Coherence Score (c_v): {coherence:.4f}\")\n",
    "print(f\"Topic Diversity: {diversity:.4f}\")\n",
    "if sil_score is not None:\n",
    "    print(f\" Silhouette Score: {sil_score:.4f}\")\n",
    "else:\n",
    "    print(\"Silhouette Score: Not applicable.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogY6IH87UT2T",
    "outputId": "68730907-e727-42a8-c5dd-bbf10a18ee19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best Hyperparameters Found ===\n",
      "Best UMAP: {'n_neighbors': 3, 'n_components': 5, 'min_dist': 0.3, 'metric': 'cosine'}\n",
      "Best HDBSCAN: {'min_cluster_size': 5, 'min_samples': 1, 'metric': 'euclidean'}\n",
      "Metrics => Coherence: 0.7843, Diversity: 0.7818, Silhouette: 0.6811\n",
      "Model ready for querying.\n",
      "\n",
      "=== Topic Quality Metrics ===\n",
      "Coherence Score (c_v): 0.7843\n",
      "Topic Diversity: 0.7818\n",
      " Silhouette Score: 0.6811\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS & SETUP ===\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "import scann\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics import silhouette_score, precision_recall_fscore_support\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === SEED FIXING ===\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed_all(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "nltk.download(\"punkt\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# === CLEANING & CONTEXT EXTRACTION (IMPROVED) ===\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def extract_entity_contexts(chunks, entities_per_chunk, use_multi_sentence=True):\n",
    "    entity_context_pairs = []\n",
    "    for idx, ents in enumerate(entities_per_chunk):\n",
    "        chunk = clean_text(chunks[idx])\n",
    "        sentences = sent_tokenize(chunk)\n",
    "        for ent in ents:\n",
    "            ent_lower = ent.lower()\n",
    "            matched = False\n",
    "            for i, sent in enumerate(sentences):\n",
    "                if ent_lower in sent:\n",
    "                    context = \" \".join(sentences[max(0, i - 1): i + 2]) if use_multi_sentence else sent.strip()\n",
    "                    enriched = f\"The concept '{ent_lower}' appears in the following context: {context}\"\n",
    "                    entity_context_pairs.append((ent_lower, enriched.strip()))\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                fallback = f\"The concept '{ent_lower}' appears in the following context: {chunk}\"\n",
    "                entity_context_pairs.append((ent_lower, fallback.strip()))\n",
    "    return entity_context_pairs\n",
    "\n",
    "\n",
    "# === TOPIC SEARCHER CLASS (WITH DEDUPLICATION, NOISE FILTERING) ===\n",
    "class AllergyTopicSearcher:\n",
    "    def __init__(self, chunks, entities_per_chunk, umap_params, hdbscan_params,\n",
    "                 model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"):\n",
    "        self.chunks = chunks\n",
    "        self.entities_per_chunk = entities_per_chunk\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "        self.umap_params = umap_params\n",
    "        self.hdbscan_params = hdbscan_params\n",
    "\n",
    "        self.topic_model = None\n",
    "        self.topic_metadata = []\n",
    "        self.topic_embeddings = None\n",
    "        self.searcher = None\n",
    "\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        entity_context_pairs = extract_entity_contexts(self.chunks, self.entities_per_chunk)\n",
    "        contextual_texts = [ctx for _, ctx in entity_context_pairs]\n",
    "        contextual_embeddings = self.embedding_model.encode(contextual_texts, normalize_embeddings=False)\n",
    "\n",
    "        umap_model = UMAP(**self.umap_params, random_state=SEED)\n",
    "        hdbscan_model = HDBSCAN(**self.hdbscan_params, prediction_data=True)\n",
    "\n",
    "        self.topic_model = BERTopic(\n",
    "            embedding_model=self.embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            representation_model=KeyBERTInspired(),\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        topics, _ = self.topic_model.fit_transform(contextual_texts, embeddings=contextual_embeddings)\n",
    "\n",
    "        topic_to_contexts = defaultdict(list)\n",
    "        topic_to_entities = defaultdict(set)\n",
    "        topic_to_embeddings = defaultdict(list)\n",
    "\n",
    "        for i, topic in enumerate(topics):\n",
    "            if topic == -1:\n",
    "                continue  # Skip noisy topics\n",
    "            ent, ctx = entity_context_pairs[i]\n",
    "            topic_to_contexts[topic].append(ctx)\n",
    "            topic_to_entities[topic].add(ent)\n",
    "            topic_to_embeddings[topic].append(contextual_embeddings[i])\n",
    "\n",
    "        topic_embeddings = []\n",
    "        topic_metadata = []\n",
    "\n",
    "        for topic_id in topic_to_contexts:\n",
    "            emb = topic_to_embeddings[topic_id]\n",
    "            centroid = np.mean(emb, axis=0)\n",
    "            centroid /= np.linalg.norm(centroid) + 1e-10\n",
    "            topic_embeddings.append(centroid)\n",
    "            topic_metadata.append({\n",
    "                \"topic_id\": topic_id,\n",
    "                \"entities\": list(topic_to_entities[topic_id]),\n",
    "                \"sentences\": topic_to_contexts[topic_id],\n",
    "                \"sentence_embeddings\": np.array(emb)\n",
    "            })\n",
    "\n",
    "        # === OPTIONAL: Merge semantically similar topics (cosine sim > 0.95)\n",
    "        deduped_metadata = []\n",
    "        used = set()\n",
    "\n",
    "        for i, emb_i in enumerate(topic_embeddings):\n",
    "            if i in used:\n",
    "                continue\n",
    "            group = [i]\n",
    "            sim_scores = cosine_similarity([emb_i], topic_embeddings)[0]\n",
    "            for j in range(i + 1, len(sim_scores)):\n",
    "                if sim_scores[j] > 0.95:\n",
    "                    group.append(j)\n",
    "                    used.add(j)\n",
    "\n",
    "            merged = {\n",
    "                \"topic_id\": i,\n",
    "                \"sentences\": [],\n",
    "                \"entities\": [],\n",
    "                \"sentence_embeddings\": []\n",
    "            }\n",
    "            for g in group:\n",
    "                merged[\"sentences\"] += topic_metadata[g][\"sentences\"]\n",
    "                merged[\"entities\"] += topic_metadata[g][\"entities\"]\n",
    "                merged[\"sentence_embeddings\"] += list(topic_metadata[g][\"sentence_embeddings\"])\n",
    "\n",
    "            merged[\"sentence_embeddings\"] = np.array(merged[\"sentence_embeddings\"])\n",
    "            merged[\"entities\"] = list(set(merged[\"entities\"]))\n",
    "            deduped_metadata.append(merged)\n",
    "\n",
    "        self.topic_metadata = deduped_metadata\n",
    "        self.topic_embeddings = np.array([\n",
    "            np.mean(m[\"sentence_embeddings\"], axis=0) /\n",
    "            (np.linalg.norm(np.mean(m[\"sentence_embeddings\"], axis=0)) + 1e-10)\n",
    "            for m in deduped_metadata\n",
    "        ])\n",
    "\n",
    "        num_points = len(self.topic_embeddings)\n",
    "        num_leaves = min(10, num_points)\n",
    "        leaves_to_search = min(5, num_leaves)\n",
    "\n",
    "        if num_points < 3:\n",
    "            raise ValueError(\"Not enough topics to build a search index.\")\n",
    "\n",
    "        self.searcher = (\n",
    "            scann.scann_ops_pybind.builder(self.topic_embeddings, 3, \"dot_product\")\n",
    "            .tree(num_leaves=num_leaves, num_leaves_to_search=leaves_to_search, training_sample_size=num_points)\n",
    "            .score_brute_force()\n",
    "            .reorder(min(5, num_points))\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "\n",
    "    import re\n",
    "\n",
    "    def search(self, query, top_k_topics=3, top_k_sents=3):\n",
    "        query_emb = self.embedding_model.encode([query], normalize_embeddings=True)[0]\n",
    "        neighbors, scores = self.searcher.search(query_emb, final_num_neighbors=top_k_topics)\n",
    "\n",
    "        results = []\n",
    "        prefix_pattern = r\"^the concept '.*?' appears in (the following )?context:\\s*\"\n",
    "\n",
    "        for i, idx in enumerate(neighbors):\n",
    "            meta = self.topic_metadata[idx]\n",
    "            topic_score = float(scores[i])\n",
    "\n",
    "            # Deduplicate sentences\n",
    "            seen = set()\n",
    "            cleaned_sentences = []\n",
    "            cleaned_embeddings = []\n",
    "\n",
    "            for sent, emb in zip(meta[\"sentences\"], meta[\"sentence_embeddings\"]):\n",
    "            # Apply regex to remove beginning prefix\n",
    "                cleaned = re.sub(prefix_pattern, \"\", sent, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        # No duplicates\n",
    "                if cleaned not in seen:\n",
    "                    seen.add(cleaned)\n",
    "                    cleaned_sentences.append(cleaned)\n",
    "                    cleaned_embeddings.append(emb)\n",
    "\n",
    "            if not cleaned_sentences:\n",
    "                continue\n",
    "\n",
    "            emb_array = np.array(cleaned_embeddings)\n",
    "            sims = np.dot(emb_array / np.linalg.norm(emb_array, axis=1, keepdims=True), query_emb)\n",
    "            top_ids = sims.argsort()[::-1][:top_k_sents]\n",
    "\n",
    "            top_sents = [(cleaned_sentences[j], float(sims[j])) for j in top_ids]\n",
    "            results.append({\n",
    "            \"topic_id\": meta[\"topic_id\"],\n",
    "            \"topic_score\": topic_score,\n",
    "            \"entities\": meta[\"entities\"],\n",
    "            \"sentences\": top_sents,\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === METRICS ===\n",
    "def compute_bertopic_coherence(topic_model, topic_metadata, topk=15):\n",
    "    topics = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
    "    word_lists = [[word for word, _ in topic] for topic in topics]\n",
    "\n",
    "    texts = []\n",
    "    for meta in topic_metadata:\n",
    "        for s in meta[\"sentences\"]:\n",
    "            tokens = clean_text(s).split()\n",
    "            texts.append(tokens)\n",
    "\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "\n",
    "    cm = CoherenceModel(\n",
    "        topics=word_lists,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\"\n",
    "    )\n",
    "    return cm.get_coherence()\n",
    "\n",
    "\n",
    "def compute_topic_diversity(topic_model, topic_metadata, topk=10):\n",
    "    topic_words = [topic_model.get_topic(meta[\"topic_id\"])[:topk] for meta in topic_metadata]\n",
    "    unique_words = set(word for topic in topic_words for word, _ in topic)\n",
    "    return len(unique_words) / (len(topic_words) * topk)\n",
    "\n",
    "\n",
    "def compute_silhouette_score_custom(topic_metadata):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    for meta in topic_metadata:\n",
    "        emb = meta[\"sentence_embeddings\"]\n",
    "        if len(emb) < 2:  # skip small clusters\n",
    "            continue\n",
    "        all_embeddings.extend(emb)\n",
    "        all_labels.extend([meta[\"topic_id\"]] * len(emb))\n",
    "\n",
    "    if len(all_embeddings) < 3:\n",
    "        return None\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return silhouette_score(all_embeddings, all_labels, metric=\"cosine\")\n",
    "\n",
    "\n",
    "# === DATASET & INITIALIZATION ===\n",
    "allergy_dataset = {\n",
    "  \"chunks\":  [\n",
    "  \"A 75-year-old male with ischemic cardiomyopathy (LVEF 28%) and persistent atrial fibrillation on warfarin presented with worsening dyspnea and orthopnea over 2 weeks.\",\n",
    "  \"He has a history of COPD, stage 4 chronic kidney disease, type 2 diabetes mellitus (HbA1c 9.1%), and prior CABG in 2008.\",\n",
    "  \"On admission, his vitals were BP 155/95, HR 112 irregular, RR 28, and SpO₂ 86% on room air, improving with oxygen via nasal cannula.\",\n",
    "  \"Physical exam revealed elevated JVP, bibasilar crackles, 3+ pitting edema, and an S3 gallop.\",\n",
    "  \"Labs showed Cr 2.8, eGFR 25, BNP 2,400 pg/mL, and INR 2.3. ABG showed pH 7.36, PaCO₂ 50, PaO₂ 70. CXR indicated cardiomegaly and pleural effusions.\",\n",
    "  \"ECG confirmed atrial fibrillation with QTc 480 ms. Pacemaker function was intact.\",\n",
    "  \"He was treated with IV furosemide, oxygen therapy, and continued on carvedilol and lisinopril. Metformin was held due to renal impairment.\",\n",
    "  \"Diabetes was managed with basal-bolus insulin and sliding scale. A low-sodium renal diet was started.\",\n",
    "  \"Education included fluid restriction, daily weight tracking, and signs of heart failure. Warfarin was continued with INR monitoring.\",\n",
    "  \"He showed improvement with a 3 kg net negative fluid balance and SpO₂ rising to 94%.\",\n",
    "  \"An echocardiogram showed LVEF 28%, moderate mitral regurgitation, and left atrial enlargement.\",\n",
    "  \"Discharge medications included furosemide 80 mg, carvedilol 12.5 mg BID, lisinopril 5 mg, spironolactone 25 mg, basal insulin, and warfarin.\",\n",
    "  \"Follow-up appointments were scheduled with cardiology, nephrology, and the diabetes clinic. Anticoagulation clinic and home health visits arranged.\",\n",
    "  \"Patient is a widower living alone with limited mobility and uses a walker. Transportation issues were noted.\",\n",
    "  \"Social work coordinated support services including meal delivery, medication management, and transport assistance.\"\n",
    "]\n",
    ",\n",
    "\"entities\":[\n",
    "  [\"male\", \"cardiomyopathy\", \"lvef\", \"fibrillation\", \"warfarin\", \"dyspnea\", \"orthopnea\"],\n",
    "  [\"copd\", \"kidney\", \"disease\", \"diabetes\", \"hba1c\", \"cabg\"],\n",
    "  [\"bp\", \"hr\", \"irregularity\", \"rr\", \"spo2\", \"oxygen\", \"nasal\", \"cannula\"],\n",
    "  [\"jvp\", \"crackles\", \"edema\", \"gallop\"],\n",
    "  [\"cr\", \"egfr\", \"bnp\", \"inr\", \"abg\", \"ph\", \"paco2\", \"pao2\", \"cxr\", \"cardiomegaly\", \"effusions\"],\n",
    "  [\"ecg\", \"fibrillation\", \"qtc\", \"pacemaker\"],\n",
    "  [\"furosemide\", \"oxygen\", \"therapy\", \"carvedilol\", \"lisinopril\", \"metformin\", \"renal\", \"impairment\"],\n",
    "  [\"diabetes\", \"insulin\", \"scale\", \"diet\"],\n",
    "  [\"fluid\", \"weight\", \"tracking\", \"signs\", \"failure\", \"warfarin\", \"inr\", \"monitoring\"],\n",
    "  [\"improvement\", \"fluid\", \"balance\", \"spo2\"],\n",
    "  [\"echocardiogram\", \"lvef\", \"regurgitation\", \"atrium\", \"enlargement\"],\n",
    "  [\"furosemide\", \"carvedilol\", \"lisinopril\", \"spironolactone\", \"insulin\", \"warfarin\"],\n",
    "  [\"follow-up\", \"cardiology\", \"nephrology\", \"clinic\", \"anticoagulation\", \"health\", \"visits\"],\n",
    "  [\"widower\", \"mobility\", \"walker\"],\n",
    "  [\"social\", \"support\", \"meal\", \"delivery\", \"medication\", \"management\", \"transport\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# === HYPERPARAMETER GRID SEARCH ===\n",
    "def grid_search_best_params(chunks, entities_per_chunk, umap_param_grid, hdbscan_param_grid):\n",
    "    best_score = -np.inf\n",
    "    best_umap = None\n",
    "    best_hdbscan = None\n",
    "    best_metrics = None\n",
    "\n",
    "    combinations = list(product(umap_param_grid[\"n_neighbors\"],\n",
    "                                umap_param_grid[\"n_components\"],\n",
    "                                umap_param_grid[\"min_dist\"],\n",
    "                                hdbscan_param_grid[\"min_cluster_size\"],\n",
    "                                hdbscan_param_grid[\"min_samples\"]))\n",
    "\n",
    "    logger.info(f\"Starting grid search over {len(combinations)} combinations...\")\n",
    "\n",
    "    for (n_neighbors, n_components, min_dist, min_cluster_size, min_samples) in combinations:\n",
    "        umap_params = {\n",
    "            \"n_neighbors\": n_neighbors,\n",
    "            \"n_components\": n_components,\n",
    "            \"min_dist\": min_dist,\n",
    "            \"metric\": \"cosine\"\n",
    "        }\n",
    "        hdbscan_params = {\n",
    "            \"min_cluster_size\": min_cluster_size,\n",
    "            \"min_samples\": min_samples,\n",
    "            \"metric\": \"euclidean\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            model = AllergyTopicSearcher(chunks, entities_per_chunk, umap_params, hdbscan_params)\n",
    "            coherence = compute_bertopic_coherence(model.topic_model, model.topic_metadata)\n",
    "            diversity = compute_topic_diversity(model.topic_model, model.topic_metadata)\n",
    "            silhouette = compute_silhouette_score_custom(model.topic_metadata) or 0.0\n",
    "\n",
    "            # Composite scoring (tweak weights if desired)\n",
    "            score = coherence + 0.5 * diversity + 0.5 * silhouette\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            logger.info(\n",
    "                f\" Tried UMAP: {umap_params}, HDBSCAN: {hdbscan_params} \"\n",
    "                f\"=> Score: {score:.4f} (Coherence: {coherence:.4f}, Diversity: {diversity:.4f}, Silhouette: {silhouette:.4f}) - Time: {elapsed:.2f}s\"\n",
    "            )\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_umap = umap_params\n",
    "                best_hdbscan = hdbscan_params\n",
    "                best_metrics = (coherence, diversity, silhouette)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping combination due to error: {e}\")\n",
    "\n",
    "    return best_umap, best_hdbscan, best_metrics\n",
    "# Define parameter grids\n",
    "umap_grid = {\n",
    "    \"n_neighbors\": [3, 5, 8],\n",
    "    \"n_components\": [3, 5],\n",
    "    \"min_dist\": [0.0, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "hdbscan_grid = {\n",
    "    \"min_cluster_size\": [2, 5],\n",
    "    \"min_samples\": [1, 3]\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "best_umap, best_hdbscan, best_metrics = grid_search_best_params(\n",
    "    allergy_dataset[\"chunks\"],\n",
    "    allergy_dataset[\"entities\"],\n",
    "    umap_param_grid=umap_grid,\n",
    "    hdbscan_param_grid=hdbscan_grid\n",
    ")\n",
    "\n",
    "print(\"\\n=== Best Hyperparameters Found ===\")\n",
    "print(f\"Best UMAP: {best_umap}\")\n",
    "print(f\"Best HDBSCAN: {best_hdbscan}\")\n",
    "print(f\"Metrics => Coherence: {best_metrics[0]:.4f}, Diversity: {best_metrics[1]:.4f}, Silhouette: {best_metrics[2]:.4f}\")\n",
    "\n",
    "\n",
    "searcher = AllergyTopicSearcher(\n",
    "    chunks=allergy_dataset[\"chunks\"],\n",
    "    entities_per_chunk=allergy_dataset[\"entities\"],\n",
    "    umap_params=best_umap,\n",
    "    hdbscan_params=best_hdbscan,\n",
    "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    ")\n",
    "print(\"Model ready for querying.\")\n",
    "# print(\"\\n=== Generated Topics and Entities ===\")\n",
    "# for meta in searcher.topic_metadata:\n",
    "#     topic_id = meta[\"topic_id\"]\n",
    "#     entities = \", \".join(meta[\"entities\"])\n",
    "#     print(f\"Topic ID: {topic_id} — Entities: {entities}\")\n",
    "\n",
    "# === METRICS ===\n",
    "coherence = compute_bertopic_coherence(searcher.topic_model, searcher.topic_metadata, topk=15)\n",
    "diversity = compute_topic_diversity(searcher.topic_model, searcher.topic_metadata, topk=10)\n",
    "sil_score = compute_silhouette_score_custom(searcher.topic_metadata)\n",
    "\n",
    "print(\"\\n=== Topic Quality Metrics ===\")\n",
    "print(f\"Coherence Score (c_v): {coherence:.4f}\")\n",
    "print(f\"Topic Diversity: {diversity:.4f}\")\n",
    "if sil_score is not None:\n",
    "    print(f\" Silhouette Score: {sil_score:.4f}\")\n",
    "else:\n",
    "    print(\"Silhouette Score: Not applicable.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
